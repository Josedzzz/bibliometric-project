@book{10.1145/3596711,
editor = {Whitton, Mary C.},
title = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {Volume 2},
abstract = {When we began planning how to celebrate 50 years of SIGGRAPH Conferences, there was unanimous agreement that one of the projects should be publishing a second volume of Seminal Graphics Papers. The first volume was published in 1998 as part of the celebration of the 25th SIGGRAPH conference. Seminal Graphics Papers Volume 2, perhaps more than any other activity undertaken in this milestone year, celebrates ACM SIGGRAPH's origins and continued success as a Technical and Professional Society. This collection of papers typifies the ground-breaking research that has been the conference's hallmark since 1974. A quick scan of the chapter and the paper titles shows just how far SIGGRAPH research has pushed the boundaries of our discipline and contributed to its evolution.The ACM Digital Library team has been supportive of this Seminal Graphics Papers project from the beginning. I am pleased to let you know that both Volumes 1 and 2 of Seminal Graphics Papers are freely available from the ACM Digital Library at these URLs: Volume 1: https://dl.acm.org/doi/book/10.1145/280811Volume 2: https://dl.acm.org/doi/book/10.1145/3596711}
}

@book{10.1145/280811,
editor = {Wolfe, Rosalee},
title = {Seminal graphics: pioneering efforts that shaped the field, Volume 1},
year = {1998},
isbn = {158113052X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {Volume 1}
}

@article{10.1145/3065386,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}

@proceedings{10.1145/3025453,
title = {CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHI 2017 is the premier international conference for the field of Human-Computer Interaction (HCI). This year it was held in Denver, bordering the beautiful Rocky Mountain region in the U.S., and reflected in our logo. The CHI 2017 conference began with two days of workshops and symposia, followed by four days of a technical program with 17 parallel sessions of provocative papers, panels, case studies, SIGs (Special Interest Groups), courses, and the popular student research, design, and game competitions. The growing alt.chi forum, now in its twelfth year, presented stimulating new ideas in HCI. The Interactivity forum showcased cutting-edge technology. For its second year, the CHI Art Exhibit merged art and technology in fascinating ways.This innovative work can be found in the Proceedings and Extended Abstracts, archived in the ACM Digital Library. For papers, the conference received 2400 submissions which were rigorously reviewed, resulting in 600 accepted papers. To ensure a better fit with reviewers, new subcommittees were created. Across all tracks, CHI received nearly 5000 submissions and accepted over 1000.Our conference theme this year, Explore, Innovate, Inspire, informed our planning process. A new venue held this year was CHI Stories. We generally know little of the personalities that drive the research presented at CHI. CHI Stories is a chance for CHI community members to share personal stories of inspiration, challenge, successes and failures, and grit. We also focused on inclusion this year. Hundreds of CHI attendees volunteered their skills in a Day of Service partnering with non-profit organizations. Inclusion was also manifest throughout the conference, for example, in the Diversity and Inclusion Lunch and by using telepresence robots to enable people with disabilities to participate remotely in the conference. Our keynote speakers were chosen to reflect our conference theme. The speakers were Neri Oxman, who combines computational design, digital fabrication, materials science and synthetic biology; Ben Shneiderman a founder of the CHI conference who, along with some key CHI personalities, gave a perspective on CHI's history and future; Wael Ghonim, credited with starting the Arab Spring and nominated for the Nobel Peace Prize; and best-selling author Nicholas Carr who challenges us to examine the unforeseen impacts of technology, particularly with automation.The world has experienced a dramatic change this past year. We live in extraordinary times and this calls for extraordinary thinking, something that the CHI community excels at. One of the challenges the community faced this year was responding to a U.S. executive order to ban citizens of certain countries from entering the country to attend CHI. As CHI is committed to inclusion, we decided to hold events at the conference to discuss and plan how we can continue our commitment to inclusion. The conference held a panel to discuss impacts of current political events on science, and hastily organized a panel to promote a conversation of civil liberties in science and a SIG on how the CHI community can participate in change. We are proud to have expanded telepresence options through the use of robots to enable people to participate in the conference remotely if they were physically unable to enter the country. We had a keynote speaker who inspired us on the topic of Internet activism. Our art exhibit, I'll Be Watching You, examined the contemporary issue of surveillance.},
location = {Denver, Colorado, USA}
}

@proceedings{10.1145/1390156,
title = {ICML '08: Proceedings of the 25th international conference on Machine learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the papers accepted to the 25th International Conference on Machine Learning (ICML 2008). ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at http://www.machinelearning.org.This year, ICML was held July 5..9 at the University of Helsinki, in Helsinki, Finland, and was co-located with COLT-2008, the 21st Annual Conference on Computational Learning Theory, and UAI-2008, the 24th Conference on Uncertainty in Artificial Intelligence. No less than 583 papers were submitted to ICML 2008. There was a very thorough review process, in which each paper was reviewed double-blind by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. There were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Some papers were checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 158 papers were accepted to ICML this year, including a small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 27\%.ICML authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included one or two invited talks by a prominent researcher. We were very fortunate to be able to host Michael Collins, of the Massachusetts Institute of Technology; Andrew Ng, of Stanford University; and Luc De Raedt, of the Katholieke Universiteit Leuven, and John Winn of Microsoft Research Cambridge. In addition to the technical talks, ICML- 2008 also included nine tutorials held before the main conference, presented by Alex Smola, Arthur Gretton, and Kenji Fukumizu; Bert Kappen and Marc Toussaint; Neil Lawrence; MartinWainwright; Ralf Herbrich and Thore Graepel; Andreas Krause and Carlos Guestrin; Shai Shalev-Shwartz and Yoram Singer; Rob Fergus; and Matthias Seeger. This year our workshops were organized jointly with COLT and UAI as part of a special "overlap day," consisting of eleven workshops selected and arranged collaboratively by the respective workshop chairs of the three conferences. This day provided a rich opportunity for interaction among the attendees of the conferences.This year, ICML enlarged its award offerings to match several other well-established conferences. We hope these will help build our community, celebrate our advances, and encourage applications and long-term thinking. In addition to our previously traditional "Best Paper" and "Best Student Paper" awards, we also gave awards for "Best Application Paper" and "10-year Best Paper" (for the best paper of ICML 1998, optionally given in conjunction with a co-located conference). We thank the Machine Learning Journal for sponsoring some of our paper awards.},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/775047,
title = {KDD '02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The KDD 2002 conference, held from 23rd to 26th July 2002, was the eighth in the series. It represented a return to the country in which the series was launched: the first was held in Montreal, Canada, and this, the eighth, was held in Edmonton, Canada. In the years between the first conference in the series and this present one, data mining has be, come a well-established discipline. It has continued to strengthen its links to other data analytic disciplines, including statistics, machine learning, pattern recognition, visualization, and database technology, but has now clearly carved out a niche of its own. Over the period in which this series has been running, hardware technology has continued to advance in great leaps, with the result that large databases have continued to grow in both number and size. The implication is that the challenge of data mining is even more important, that the problems requiring data mining solutions are ever more ubiquitous, and that new tools and methods for tackling are even more necessary.KDD 2002 received a record number of submitted papers - 307 in total, 37 of which were considered for the industral/applicafion track. Among the 270 research submissions, 32 were selected (12\%) for full papers; and among the 37 industrial/application submissions, 12 (32\%) were selected for full papers. An additional 44 submissions were chosen to be presented as posters, a vast majority of which were research submissions. This low rate of acceptance reflects a conscious effort to maintain the very high standards of quality and relevance, which have been achieved by previous conferences in the series. It means that the papers and posters in the proceedings represent the cutting edge of data mining problemsl solutions, and technology. On the other hand, this policy inevitably meant that many excellent contributions did not make it to the final program. The choice had to be informed by balance as well as quality - KDD 2002 had to showcase research in data mining across the entire frontier of the discipline. This breadth was reflected in the choice of invited speakers, both well known in the data mining; community, but from different backgrounds: Daryl Pregibon and Geoff Hinton. The program also includes 6 workshops in such diverse areas as 'Data Mining in Bioinformatics', 'Web Mining', 'Multimedia Data Mining', 'Multi-Relational Data Mining', 'Temporal Data Mining', and 'Fractals in Data Mining' as well as 6 tutorials on 'Text Mining for Bioinformatics', 'Querying and Mining Data Streams', 'Link Analysis', 'Multivariate Density Estimation', 'Common Reasons Data Mining Projects Fail', and 'Visual Data Mining'.},
location = {Edmonton, Alberta, Canada}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@proceedings{10.1145/2339530,
title = {KDD '12: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The KDD conference has seen remarkable growth since its origins as an IJCAI workshop in Detroit in 1989, evolving into a full-fledged research conference in 1995, underscoring the important role data mining as a field has played in extracting knowledge and actionable insights from vast troves of data that is being generated in the digital world around us. This year we received a record 755 submissions to the research program, from which 133 papers were accepted, for an aggregate acceptance rate of 17.6\% (quite similar to recent years).Among the academic conferences, the KDD conference has typically more of an emphasis on research motivated by real-world applications. It is important to keep in mind that it is this synergy of research in areas like algorithms, computational geometry, database, graph theory, machine learning, natural language processing, statistics, visualization and many others when applied to problems arising in diverse fields such as web, medicine, climatology, marketing that drives our field forward, makes it vibrant and fun - who would know that ideas in computational geometry can be adapted to construct fast algorithms to improve online advertising and movie recommendations?The breadth of topics covered in this year's research program is truly comprehensive, including social networks, privacy, text mining, predictive modeling, time-series forecasting, spatial data analysis, geometry, and more. We are very fortunate to have 4 world-class keynote speakers this year spanning industry and academia, providing inspirational talks on cutting-edge techniques and issues in web mining, information networks, statistical inference for big data, and social computing.The process of whittling down the initial 734 submissions to the final set of 133 accepted papers required the coordination and time of a large number of willing volunteers. The program committee (PC) consisted of over 350 reviewers (PC members) and 50 senior PC members. In the first phase each submitted paper was automatically assigned to 3 reviewers (after a bidding process). Once the reviews from each of the 3 reviewers were completed, the program chairs rejected papers that did not receive much support from any of the reviewers. We rejected 259 papers at this stage. Special care was taken to minimize the error of rejecting a potentially good paper at this stage. The papers that survived the first phase were assigned to the senior PC members based on their bids, they had the option of initiating a discussion for any of their papers, e.g., if there was significant divergence in scores among reviewers, or if a paper was on the borderline of being accepted. Following the discussion phase, the senior PC members provided a recommendation score and a detailed meta-review for each paper. In the final phase, we (the program chairs) analyzed all of this information, starting with the obvious accept and reject decisions, and then gradually focusing in more detail on the papers near the borderline, seeking additional reviews and input from the PC and senior PC members where appropriate. We also initiated a shepherding phase with 15 papers having the opportunity of fixing mild issues we thought would be possible to address before they can be accepted. 13 of them were accepted after thorough revisions. Finally, it is quite likely that in hindsight some worthy papers may have been rejected as part of this process - these errors are an unfortunate reality of modern computer science conferences, and hard to avoid when a very large number of decisions have to be made over a short time span based on a subjective reviewing process. Nevertheless, we, the PC chairs, are responsible for those unfortunate errors and welcome suggestions on the matter.},
location = {Beijing, China}
}

@proceedings{10.1145/2487575,
title = {KDD '13: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 19th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). The annual ACM SIGKDD conference is the premier international forum for data mining, knowledge discovery and big data. It brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD-2013 features plenary presentations, paper presentations, poster sessions, workshops, tutorials, exhibits, demonstrations, and the KDD Cup competition.Today, you hear a lot about big data, data science and data intensive computing. The core of this work is extracting knowledge and useful information from data, which for science leads to beautiful insights, and for applications leads to actions, alerts and decisions. The KDD community has always been at the center of this activity and it is clear from this conference that it will continue to drive this broader field of big data.This year there were 726 submissions to the KDD Research Track, and 125 papers were accepted. There were 136 submissions to the KDD Industry and Government Track, and 34 papers were accepted.KDD also has a history of inviting talks that are of broad interest to the KDD community. This year we chose to have 4 plenary talks. A program committee also selected 8 talks to present at the Industry Practice Exposition.A strength of the KDD conference is the number of workshops and tutorials that are co-located with it. This year there were 10 full-day workshops, 5 half-day workshops, and 6 tutorials.We thank all sponsors, who are a very important part of the conference, and the members of the Organizing Committee and our other colleagues who volunteered their time during the past year to make this conference a success. Special thanks goes to the Research Track Co-Chairs and the Industry and Government Track Co-Chairs. Also special thanks are due to the Local Arrangements Chair, the Treasurer, the Proceedings Co-Chairs, and the KDD Cup Committee.We are grateful to the several program committees that provided the advice necessary to put together a quality program - the Research Track Program Committee, the Research Track Senior Program Committee, the Industry and Government Track Program Committee, the Industry Practice Expo Program Committee, the Workshop Program Committee, the Tutorial Program Committee, and the Demo Program Committee.We know that you will find this year's exhibits and demonstrations exciting and remind you that some of the most interesting discussions can be found there.Please join us for KDD-2013 to gain new knowledge and to exchange exciting new research results, leading practices, and high impact applications in big data, knowledge discovery and data mining. We hope that you will find this program interesting and thought-provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
location = {Chicago, Illinois, USA}
}

@proceedings{10.1145/1102351,
title = {ICML '05: Proceedings of the 22nd international conference on Machine learning},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume, which is also available online from
http://www.machinelearning.org, contains the papers accepted for
presentation at ICML-2005, the 22nd lnternational Conference on
Machine Learning, which was held at the University of Bonn in
Germany from August 7 to August 11, 2005. ICML is the annual
conference of the lnternational Machine Learning Society (IMLS),
and forms an international forum for the discussion and
presentation of the latest results in the field of machine
learning. This year, ICML was co-located with the 15th
lnternational Conference on Inductive Logic Programming (ILP-2005),
the proceedings of which are published by Springer Verlag in a
separate volume.The papers in this volume were selected on the basis of a
thorough review process. In the first round of reviewing, three
program committee members produced individual reviews for a paper.
Authors then had the opportunity to view those reviews and submit
an author's reply to the reviewers. Led by the responsible area
chair, the reviewers then engaged in a discussion about the paper,
ultimately leading to the decision by the program chairs. In sum,
of the 491 papers that were initially submitted, 62 were accepted
immediately, and a further 81 were conditionally accepted and
reconsidered after resubmission in a second round of reviewing. Of
those 81 conditionally accepted papers, 72 were finally accepted,
leading to a total of 134 accepted papers, which translates into an
acceptance rate of 27.3 \%. The author reply was a new feature of
ICML this year, while the option of working with conditional
accepts has already become a tradition.In addition to the presentations of the accepted papers, the
ICML program included several other features. On the first and last
day of the conference, 11 workshops and 6 tutorials on current
topics of machine learning were held. For many of these,
proceedings and/or presentation materials are available online from
the ICML website. The other days of the conference each featured an
invited talk by a prominent researcher as a program highlight. We
were delighted that Johannes Gehrke of Cornell University, Michael
Jordan of the University of California at Berkeley, and Gerhard
Widmer of the University of Linz in Austria, agreed to deliver an
invited talk. The abstracts of their talks are also published as
part of these proceedings.Continuing a long standing tradition at ICML, all papers
presented in a talk at the conference were also exhibited at
evening poster sessions, giving everyone ample time to discuss the
results in depth. In order to emphasize the co-location with
ILP-2005, the program contained joint elements in both invited
speakers, paper sessions, poster sessions, and tutorials. As usual,
the scientific program was complemented by a social program, this
time featuring an excursion to the scenic surroundings of the city
of Bonn.During the conference best paper and best student paper awards
were presented, the former being sponsored by NICTA, the later by
the Machine Learning Journal.},
location = {Bonn, Germany}
}

@article{10.1145/359340.359342,
author = {Rivest, R. L. and Shamir, A. and Adleman, L.},
title = {A method for obtaining digital signatures and public-key cryptosystems},
year = {1978},
issue_date = {Feb. 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/359340.359342},
doi = {10.1145/359340.359342},
abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.},
journal = {Commun. ACM},
month = feb,
pages = {120–126},
numpages = {7},
keywords = {authentication, cryptography, digital signatures, electronic funds transfer, electronic mail, factorization, message-passing, prime number, privacy, public-key cryptosystems, security}
}

@inproceedings{10.1145/170035.170072,
author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
title = {Mining association rules between sets of items in large databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170072},
doi = {10.1145/170035.170072},
abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {207–216},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}

@article{10.1145/170036.170072,
author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
title = {Mining association rules between sets of items in large databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170072},
doi = {10.1145/170036.170072},
abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {207–216},
numpages = {10}
}

@article{10.1145/1327452.1327492,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: simplified data processing on large clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327492},
doi = {10.1145/1327452.1327492},
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
journal = {Commun. ACM},
month = jan,
pages = {107–113},
numpages = {7}
}

@article{10.1145/331499.331504,
author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
title = {Data clustering: a review},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/331499.331504},
doi = {10.1145/331499.331504},
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview
of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify
 cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
journal = {ACM Comput. Surv.},
month = sep,
pages = {264–323},
numpages = {60},
keywords = {cluster analysis, clustering applications, exploratory data analysis, incremental clustering, similarity indices, unsupervised learning}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@article{10.1145/3422622,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative adversarial networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422622},
doi = {10.1145/3422622},
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
journal = {Commun. ACM},
month = oct,
pages = {139–144},
numpages = {6}
}

@proceedings{10.1145/2723372,
title = {SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, "The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, "From Data to Insights @ Bare Metal Speed," explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, "Three Favorite Results," on Tuesday. Christopher R\'{e} will lead a panel on "Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?," with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5\%, and we believe that the revision processhas improved the quality of the technical program.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/2939672.2939778,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{10.5555/944919.944937,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent dirichlet allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}

@proceedings{10.1145/1081870,
title = {KDD '05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -- KDD'05. The KDD conferences provide a forum for novel research results and interesting applications in the areas of data mining and knowledge discovery. The conference series gives researchers and practitioners a unique opportunity to share their perspectives with others and to present new research ideas, applications, solutions, tools, systems, and research directions broadly related to knowledge discovery and data mining.The call for papers attracted 465 research submissions and 73 industrial submissions from around the world. The program committee accepted 40 research papers, 36 research posters, 14 industrial papers and 11 industrial posters. In addition, this year's conference includes three plenary talks and two panels.Putting together KDD'05 was a team effort. First, we would like to thank the authors for providing the content of the program and the program committee and external reviewers, who worked very hard in reviewing papers and providing suggestions for their improvements. Second, we would like to thank the Organizing Committee, who also worked very hard, and, as volunteers, didn't get much for it. I strongly encourage you to thank them, and even, perhaps, to let them get out of the elevator ahead of you. Finally, we would like to thank our sponsor, ACM SIGKDD, for their continued support.Daniel Hudson Burnham (1846-1912) was a partner in the Chicago based architecture firm Burnham and Root. Burnham and Root created the foundation for the modern skyscraper by using a floating foundation of cement that provided a stable foundation even when, as in many Chicago locations, it was not possible to reach bedrock. Burnham and Root was also the lead architect for, and in charge of, construction for the World Columbian Exposition (1893), which celebrated the 400th anniversary of the arrival of Columbus to North America. The World Columbian Exposition was the largest World's Fair to that date and drew 27.5 million attendees at a time when the US population was about 65 million. To achieve this, many logistic obstacles had to be overcome. Burnham's style is nicely captured by a quote associated with him: "Make no little plans. They have no magic to strike man's blood and probably will themselves not be realized.This is the theme we have chosen for this year's KDD Conference. The field of data mining and knowledge discovery is over ten years old, and is now mature enough to begin to make some big plans and to tackle some very difficult problems and challenges. We will begin discussions about these challenges at this year's conference and continue them throughout the year. Over the coming year, please look to the SIGKDD Explorations for further information.We hope that you will find this program interesting and thought provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
location = {Chicago, Illinois, USA}
}

@article{10.1145/1541880.1541882,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
title = {Anomaly detection: A survey},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1541880.1541882},
doi = {10.1145/1541880.1541882},
abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {15},
numpages = {58},
keywords = {Anomaly detection, outlier detection}
}

@article{10.5555/1953048.2078195,
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
title = {Scikit-learn: Machine Learning in Python},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {2825–2830},
numpages = {6}
}

@proceedings{10.1145/564691,
title = {SIGMOD '02: Proceedings of the 2002 ACM SIGMOD international conference on Management of data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 2002 ACM SIGMOD International Conference on Management of Data, was held June 4-6, 2002 at the spectacular Frank Lloyd Wright-designed Monona Terrace conference center in Madison, Wisconsin. The SIGMOD conference has long held its status a leading forum for database researchers, practitioners, developers, and users to explore cutting-edge ideas and to exchange techniques, tools, and experiences. This year we are pleased to have a particularly strong program, including significant representation from both industry and academia and contributions from around the world. The SIGMOD conference is sponsored by the Association for Computing Machinery (ACM) and its Special Interest Group on Management of Data (SIGMOD). As has become customary, two days of the conference were overlapped with the Symposium on Principles of Database Systems (PODS). In addition, there were a number of important events co-located with SIGMOD this year, including the 5th International Workshop on the Web and Databases (WebDB), the Workshop on Research Issues in Data Mining and Knowledge Discovery (DMKD), and the New Database Faculty Symposium.As with previous years, acceptance into the conference proceedings was extremely competitive. From the 240 research program submissions, the program committee selected 42 papers for presentation and inclusion in the proceedings. These papers span the range of traditional database topics as well as issues of emerging interest such as data streaming, middle-tier data management, and integration with web services and the WWW. The: program committee worked hard to select these papers through a detailed review process and active discussion both electronically and at the program committee meeting held in January at Berkeley. We are deeply indebted to Surajit Chaudhuri and Jonathan Simon for their efforts in developing and hosting the Conference Management Tool at Microsoft. We used the CMT to run the entire submissions and review process for research papers, including the program committee meeting. We would also like to thank Umesh Dayal, who took on extra work to help manage the review process.In addition to the research track, we had 41 submissions to the Demonstrations track, of which 21 projects were invited to present. The Demonstrations track has become a key venue for the early dissemination of cutting-edge prototype and systems development experience. Paul Aoki chaired the demos program committee, which put together a diverse and exciting program. The Industrial program committee, chaired by Mike Carey took an active role in soliciting quality submissions and have put together a program that is a key component of this year's conference. Likewise, Donald Kossmann and Peter Scheuermann and the tutorial committee organized an excellent slate of Panels and Tutorials to round out the program.},
location = {Madison, Wisconsin}
}

@inproceedings{10.1145/2939672.2939754,
author = {Grover, Aditya and Leskovec, Jure},
title = {node2vec: Scalable Feature Learning for Networks},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939754},
doi = {10.1145/2939672.2939754},
abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {855–864},
numpages = {10},
keywords = {feature learning, graph representations, information networks, node embeddings},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@inproceedings{10.1145/2623330.2623732,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: online learning of social representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {deep learning, latent representations, learning with partial labels, network classification, online learning, social networks},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/130385.130401,
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
title = {A training algorithm for optimal margin classifiers},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130385.130401},
doi = {10.1145/130385.130401},
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {144–152},
numpages = {9},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI \&amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/383059.383071,
author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
title = {Chord: A scalable peer-to-peer lookup service for internet applications},
year = {2001},
isbn = {1581134118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383059.383071},
doi = {10.1145/383059.383071},
abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
booktitle = {Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications},
pages = {149–160},
numpages = {12},
location = {San Diego, California, USA},
series = {SIGCOMM '01}
}

@article{10.1145/964723.383071,
author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
title = {Chord: A scalable peer-to-peer lookup service for internet applications},
year = {2001},
issue_date = {October 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/964723.383071},
doi = {10.1145/964723.383071},
abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {149–160},
numpages = {12}
}

@article{10.1145/359545.359563,
author = {Lamport, Leslie},
title = {Time, clocks, and the ordering of events in a distributed system},
year = {1978},
issue_date = {July 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/359545.359563},
doi = {10.1145/359545.359563},
abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
journal = {Commun. ACM},
month = jul,
pages = {558–565},
numpages = {8},
keywords = {clock synchronization, computer networks, distributed systems, multiprocess systems}
}

@article{10.1145/1721654.1721672,
author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
title = {A view of cloud computing},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1721654.1721672},
doi = {10.1145/1721654.1721672},
abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
journal = {Commun. ACM},
month = apr,
pages = {50–58},
numpages = {9}
}

@inproceedings{10.1145/37401.37406,
author = {Reynolds, Craig W.},
title = {Flocks, herds and schools: A distributed behavioral model},
year = {1987},
isbn = {0897912276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/37401.37406},
doi = {10.1145/37401.37406},
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
booktitle = {Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {25–34},
numpages = {10},
series = {SIGGRAPH '87}
}

@article{10.1145/37402.37406,
author = {Reynolds, Craig W.},
title = {Flocks, herds and schools: A distributed behavioral model},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {0097-8930},
url = {https://doi.org/10.1145/37402.37406},
doi = {10.1145/37402.37406},
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
journal = {SIGGRAPH Comput. Graph.},
month = aug,
pages = {25–34},
numpages = {10}
}

@inbook{10.1145/280811.281008,
author = {Reynolds, Craig W.},
title = {Flocks, herds, and schools: a distributed behavioral model},
year = {1998},
isbn = {158113052X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/280811.281008},
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
booktitle = {Seminal Graphics: Pioneering Efforts That Shaped the Field, Volume 1},
pages = {273–282},
numpages = {10}
}

@inproceedings{10.1145/371920.372071,
author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
title = {Item-based collaborative filtering recommendation algorithms},
year = {2001},
isbn = {1581133480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/371920.372071},
doi = {10.1145/371920.372071},
booktitle = {Proceedings of the 10th International Conference on World Wide Web},
pages = {285–295},
numpages = {11},
location = {Hong Kong, Hong Kong},
series = {WWW '01}
}

@proceedings{10.1145/509907,
title = {STOC '02: Proceedings of the thiry-fourth annual ACM symposium on Theory of computing},
year = {2002},
isbn = {1581134959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The papers in this volume were presented at the Thirty-Fourth Annual ACM Symposium on Theory of Computing (STOC2002), held in Montreal, Quebec, Canada, May 19-21, 2002. The Symposium was sponsored by the ACM Special Interest Group on Algorithms and Computation Theory (SIGACT).In response to a call for papers, 287 paper submissions were received. All were submitted electronically. The program committee conducted its deliberations electronically, via an on-line meeting that ran from January 10 to January 19. The committee selected 91 papers from among the submissions. The submissions were not refereed, and many of these papers represented reports of continuing research. It is expected that most of them will appear in a more polished and complete form in scientific journals.The papers encompassed in wide variety of areas of theoretical computer science. The topics included algorithms and computational complexity bounds for classical problems in algebra, geometry, topology, graph theory, game theory, logic and machine learning, as well as theoretical aspects of security, databases, information retrieval, and networks, the web, computational biology, and alternative models of computation including quantum computation and self-assembly.},
location = {Montreal, Quebec, Canada}
}

@article{10.1145/324133.324140,
author = {Kleinberg, Jon M.},
title = {Authoritative sources in a hyperlinked environment},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/324133.324140},
doi = {10.1145/324133.324140},
abstract = {The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.},
journal = {J. ACM},
month = sep,
pages = {604–632},
numpages = {29},
keywords = {World Wide Web, graph algorithms, hypertext structure, link analysis}
}

@inproceedings{10.1145/342009.335388,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: identifying density-based local outliers},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335388},
doi = {10.1145/342009.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {93–104},
numpages = {12},
keywords = {database mining, outlier detection},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@article{10.1145/335191.335388,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: identifying density-based local outliers},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335388},
doi = {10.1145/335191.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
journal = {SIGMOD Rec.},
month = may,
pages = {93–104},
numpages = {12},
keywords = {database mining, outlier detection}
}

@proceedings{10.1145/2517349,
title = {SOSP '13: Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
year = {2013},
isbn = {9781450323888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP 2013), held at the Nemacolin Woodlands Resort, Farmington, Pennsylvania, USA. This year's program includes 30 papers, and touches on a wide range of computer systems topics, from kernels to big data, from responsiveness to correctness, and from devices to data centers. The program committee made every effort to identify and include some of the most creative and thought-provoking ideas in computer systems today. Each accepted paper was shepherded by a program committee member to make sure the papers are as readable and complete as possible. We hope you will enjoy the program as much as we did in selecting it.},
location = {Farminton, Pennsylvania}
}

@article{10.5555/2627435.2670313,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, neural networks, regularization}
}

@inproceedings{10.1145/956750.956769,
author = {Kempe, David and Kleinberg, Jon and Tardos, \'{E}va},
title = {Maximizing the spread of influence through a social network},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956769},
doi = {10.1145/956750.956769},
abstract = {Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of "word of mouth" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63\% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {137–146},
numpages = {10},
keywords = {approximation algorithms, diffusion of innovations, social networks, viral marketing},
location = {Washington, D.C.},
series = {KDD '03}
}

@article{10.1145/182.358434,
author = {Allen, James F.},
title = {Maintaining knowledge about temporal intervals},
year = {1983},
issue_date = {Nov. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/182.358434},
doi = {10.1145/182.358434},
journal = {Commun. ACM},
month = nov,
pages = {832–843},
numpages = {12},
keywords = {interval reasoning, interval representation, temporal interval}
}

@proceedings{10.1145/1082473,
title = {AAMAS '05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems},
year = {2005},
isbn = {1595930930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Autonomous Agents and Multi-Agent Systems are, in the judgment
of many observers, the most significant new paradigm for software
modelling and development to emerge from Computer Science in the
last two decades. Autonomous Agents are computer programs that are
able to decide between different methods of achieving their
programmed goals. This is in contrast to the scripted, predefined
behaviour that a traditional program, for example a Unix Daemon,
would exhibit, and means that Autonomous Agents can cope with
dynamic environments, can be developed as a quick solution to
complex problems, and can tailor their behaviour to individual
users in a way that traditional software cannot.Multi-Agent Systems are computational environments in which
different programs, possibly developed in isolation, can
co-ordinate their behaviours to achieve their goals. The technology
of Multi-Agent Systems is, therefore, particularly applicable to
modern operational environments like e-business, ubiquitous
computing, and, of course, the Internet.These two topics of research are tightly coupled. For a
Multi-Agent System to be fully implemented, it is often necessary
to utilize the technology of Autonomous Agents. For an Agent to act
with true autonomy in a realistic, modern setting, it must often be
able to reason about other Agents and co-ordinate its behaviour
with them. It is this insight that has led to the development of
the AAMAS community and the spectacularly successful series of
conferences that have been running for the last for years.The AAMAS community has long recognised [Wooldridge &amp;
Jennings 1995] that the significance of the abstractions,
techniques, tools, and technologies developed by the research
community worldwide will only be recognised if they have practical
applications in the real world of science, technology, education,
healthcare, business, and commerce. The paradigm of AAMAS and
available agent technologies has reached a significant level of
maturity, and they are now widely regarded as ready for wider
adoption. The purpose of the First Industry Track of the Autonomous
Agent and Multi-Agent Systems conference is to provide a forum that
will bring real world applications that are being developed by
teams internationally to the attention of the AAMAS community and
the wider world.Why is this important? We have identified several
motivations:•By disseminating the news of successful application of
AAMAS technology we hope to provide researchers with clear evidence
of the success and usefulness of particular techniques.•Feedback from attempts to use AAMAS technology is
valuable in the formation of the AAMAS research agenda.•A forum for discussion of industrial and
application-orientated concerns has been lacking in the community
to date; by providing one, we hope that practitioners will be able
to share their experiences and, therefore, accelerate the uptake of
AAMAS technology.•Finally, we expect that, by providing an industry
track, we will encourage more participation in AAMAS by
practitioners, and that this will lead to a wider understanding of
the importance of AAMAS technology in commercial organisations.Fifteen papers that discuss the application of AAMAS technology
are presented in this collection. They were selected by a
refereeing process that aimed to find work that made use of the
particular properties of AAMAS systems and had actually been used
"in anger." The ratio of submitted to accepted papers was higher
than 2:1; that is, more papers were excluded from presentation than
were accepted.The papers that have been selected by the referees and programme
committee for the track allow the identification of some trends in
the development and application of AAMAS technology. Four out of
fifteen papers are in the field of transport, traffic, and
logistics. Natural problem decomposition, geographical
distribution, and requirements for autonomous decision making make
this domain particularly suitable for AAMAS technology.A further four papers are focused on Aerospace applications. In
this domain, the technological adventurousness of a highly
competitive and demanding industry may explain the work presented.
However, the use of AAMAS technology to provide autonomy for
systems that cannot be easily or cheaply supervised by human
intervention seems to be another clear driver.Three papers focus on using AAMAS technology for manufacturing
applications. Here, the drivers seem to have been to use Agent
Autonomy or co-ordination techniques to provide cost savings,
realised by removing large numbers of repetitive management and
control tasks.The track also features papers which describe the use of AAMAS
technology for electricity network management and to provide a
training system for naval personnel. Both of these papers point to
cost savings realised by automating decision making processes.Just as we can see trends in the papers that have been selected
for publication, it's also interesting to point to missing areas.
There are no papers from promising domains for AAMAS adoption
including telecommunications, internet, pharmaceuticals,
eGovernment, or healthcare. Similarl,y the submission rate from
small companies and start-ups was expected to be somewhat higher.
These are areas that have been explored extensively in the past
[Luck et-al 2003] so it is important for the AAMAS community to
investigate why the research that has been performed has not
matured into published case studies at this time.Although the collection is diverse, all the papers share two
common traits. Every paper is the result of the efforts of a team
that has been prepared to take the risk of adopting a new
technology in a high pressure environment, and every paper
summarises what happens when talented people take a risk and that
risk pays off. We salute all these pioneers, and look forward to
the presentation of the technical program and the discussions that
are bound to result.},
location = {The Netherlands}
}

@article{10.1145/505282.505283,
author = {Sebastiani, Fabrizio},
title = {Machine learning in automated text categorization},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/505282.505283},
doi = {10.1145/505282.505283},
abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.},
journal = {ACM Comput. Surv.},
month = mar,
pages = {1–47},
numpages = {47},
keywords = {Machine learning, text categorization, text classification}
}

@inproceedings{10.1145/342009.335372,
author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
title = {Mining frequent patterns without candidate generation},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335372},
doi = {10.1145/342009.335372},
abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}

@article{10.1145/335191.335372,
author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
title = {Mining frequent patterns without candidate generation},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335372},
doi = {10.1145/335191.335372},
abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
journal = {SIGMOD Rec.},
month = may,
pages = {1–12},
numpages = {12}
}

@article{10.1145/361237.361242,
author = {Duda, Richard O. and Hart, Peter E.},
title = {Use of the Hough transformation to detect lines and curves in pictures},
year = {1972},
issue_date = {Jan. 1972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/361237.361242},
doi = {10.1145/361237.361242},
abstract = {Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.},
journal = {Commun. ACM},
month = jan,
pages = {11–15},
numpages = {5},
keywords = {Hough transformation, colinear points, curve detection, line detection, pattern recognition, picture processing, point-line transformation}
}

@article{10.1145/361219.361220,
author = {Salton, G. and Wong, A. and Yang, C. S.},
title = {A vector space model for automatic indexing},
year = {1975},
issue_date = {Nov. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/361219.361220},
doi = {10.1145/361219.361220},
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
journal = {Commun. ACM},
month = nov,
pages = {613–620},
numpages = {8},
keywords = {automatic indexing, automatic information retrieval, content analysis, document space}
}

@proceedings{10.1145/100216,
title = {STOC '90: Proceedings of the twenty-second annual ACM symposium on Theory of Computing},
year = {1990},
isbn = {0897913612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Baltimore, Maryland, USA}
}

@inproceedings{10.1145/190314.190336,
author = {Perkins, Charles E. and Bhagwat, Pravin},
title = {Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers},
year = {1994},
isbn = {0897916824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/190314.190336},
doi = {10.1145/190314.190336},
abstract = {An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.},
booktitle = {Proceedings of the Conference on Communications Architectures, Protocols and Applications},
pages = {234–244},
numpages = {11},
location = {London, United Kingdom},
series = {SIGCOMM '94}
}

@article{10.1145/190809.190336,
author = {Perkins, Charles E. and Bhagwat, Pravin},
title = {Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/190809.190336},
doi = {10.1145/190809.190336},
abstract = {An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = oct,
pages = {234–244},
numpages = {11}
}

@article{10.1145/1118178.1118215,
author = {Wing, Jeannette M.},
title = {Computational thinking},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/1118178.1118215},
doi = {10.1145/1118178.1118215},
abstract = {It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.},
journal = {Commun. ACM},
month = mar,
pages = {33–35},
numpages = {3}
}

@proceedings{10.1145/3027063,
title = {CHI EA '17: Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHI 2017 is the premier international conference for the field of Human-Computer Interaction (HCI). This year it was held in Denver, bordering the beautiful Rocky Mountain region in the U.S., and reflected in our logo. The CHI 2017 conference began with two days of workshops and symposia, followed by four days of a technical program with 17 parallel sessions of provocative papers, panels, case studies, SIGs (Special Interest Groups), courses, and the popular student research, design, and game competitions. The growing alt.chi forum, now in its twelfth year, presented stimulating new ideas in HCI. The Interactivity forum showcased cutting-edge technology. For its second year, the CHI Art Exhibit merged art and technology in fascinating ways.This innovative work can be found in the Proceedings and Extended Abstracts, archived in the ACM Digital Library. For papers, the conference received 2400 submissions which were rigorously reviewed, resulting in 600 accepted papers. To ensure a better fit with reviewers, new subcommittees were created. Across all tracks, CHI received nearly 5000 submissions and accepted over 1000.Our conference theme this year, Explore, Innovate, Inspire, informed our planning process. A new venue held this year was CHI Stories. We generally know little of the personalities that drive the research presented at CHI. CHI Stories is a chance for CHI community members to share personal stories of inspiration, challenge, successes and failures, and grit. We also focused on inclusion this year. Hundreds of CHI attendees volunteered their skills in a Day of Service partnering with non-profit organizations. Inclusion was also manifest throughout the conference, for example, in the Diversity and Inclusion Lunch and by using telepresence robots to enable people with disabilities to participate remotely in the conference. Our keynote speakers were chosen to reflect our conference theme. The speakers were Neri Oxman, who combines computational design, digital fabrication, materials science and synthetic biology; Ben Shneiderman a founder of the CHI conference who, along with some key CHI personalities, gave a perspective on CHI's history and future; Wael Ghonim, credited with starting the Arab Spring and nominated for the Nobel Peace Prize; and best-selling author Nicholas Carr who challenges us to examine the unforeseen impacts of technology, particularly with automation.The world has experienced a dramatic change this past year. We live in extraordinary times and this calls for extraordinary thinking, something that the CHI community excels at. One of the challenges the community faced this year was responding to a U.S. executive order to ban citizens of certain countries from entering the country to attend CHI. As CHI is committed to inclusion, we decided to hold events at the conference to discuss and plan how we can continue our commitment to inclusion. The conference held a panel to discuss impacts of current political events on science, and hastily organized a panel to promote a conversation of civil liberties in science and a SIG on how the CHI community can participate in change. We are proud to have expanded telepresence options through the use of robots to enable people to participate in the conference remotely if they were physically unable to enter the country. We had a keynote speaker who inspired us on the topic of Internet activism. Our art exhibit, I'll Be Watching You, examined the contemporary issue of surveillance.},
location = {Denver, Colorado, USA}
}

@article{10.1145/1970392.1970395,
author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
title = {Robust principal component analysis?},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/1970392.1970395},
doi = {10.1145/1970392.1970395},
abstract = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
journal = {J. ACM},
month = jun,
articleno = {11},
numpages = {37},
keywords = {ℓ1-norm minimization, Principal components, duality, low-rank matrices, nuclear-norm minimization, robustness vis-a-vis outliers, sparsity, video surveillance}
}

