@ARTICLE{7906512,
  author={L’Heureux, Alexandra and Grolinger, Katarina and Elyamany, Hany F. and Capretz, Miriam A. M.},
  journal={IEEE Access}, 
  title={Machine Learning With Big Data: Challenges and Approaches}, 
  year={2017},
  volume={5},
  number={},
  pages={7776-7797},
  abstract={The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.},
  keywords={Big Data;Machine learning algorithms;Data mining;Algorithm design and analysis;Data analysis;Support vector machines;Classification algorithms;Big Data;Big Data Vs;data analysis;data analytics;deep learning;distributed computing;machine learning;neural networks},
  doi={10.1109/ACCESS.2017.2696365},
  ISSN={2169-3536},
  month={},}@ARTICLE{8732419,
  author={Rappaport, Theodore S. and Xing, Yunchou and Kanhere, Ojas and Ju, Shihao and Madanayake, Arjuna and Mandal, Soumyajit and Alkhateeb, Ahmed and Trichopoulos, Georgios C.},
  journal={IEEE Access}, 
  title={Wireless Communications and Applications Above 100 GHz: Opportunities and Challenges for 6G and Beyond}, 
  year={2019},
  volume={7},
  number={},
  pages={78729-78757},
  abstract={Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.},
  keywords={Wireless communication;Wireless sensor networks;Antenna arrays;Bandwidth;Communication system security;Cognition;Imaging;mmWave;millimeter wave;5G;D-band;6G;channel sounder;propagation measurements;Terahertz (THz);array processing;imaging;scattering theory;cone of silence;digital phased arrays;digital beamformer;signal processing for THz;position location;channel modeling;THz applications;wireless cognition;network offloading},
  doi={10.1109/ACCESS.2019.2921522},
  ISSN={2169-3536},
  month={},}@ARTICLE{10105236,
  author={Shoufan, Abdulhadi},
  journal={IEEE Access}, 
  title={Exploring Students’ Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey}, 
  year={2023},
  volume={11},
  number={},
  pages={38805-38818},
  abstract={ChatGPT has sparked both excitement and skepticism in education. To analyze its impact on teaching and learning it is crucial to understand how students perceive ChatGPT and assess its potential and challenges. Toward this, we conducted a two-stage study with senior students in a computer engineering program ( $n=56$ ). In the first stage, we asked the students to evaluate ChatGPT using their own words after they used it to complete one learning activity. The returned responses (3136 words) were analyzed by coding and theme building (36 codes and 15 themes). In the second stage, we used the derived codes and themes to create a 27-item questionnaire. The students responded to this questionnaire three weeks later after completing other activities with the help of ChatGPT. The results show that the students admire the capabilities of ChatGPT and find it interesting, motivating, and helpful for study and work. They find it easy to use and appreciate its human-like interface that provides well-structured responses and good explanations. However, many students feel that ChatGPT’s answers are not always accurate and most of them believe that it requires good background knowledge to work with since it does not replace human intelligence. So, most students think that ChatGPT needs to be improved but are optimistic that this will happen soon. When it comes to the negative impact of ChatGPT on learning, academic integrity, jobs, and life, the students are divided. We conclude that ChatGPT can and should be used for learning. However, students should be aware of its limitations. Educators should try using ChatGPT and guide students on effective prompting techniques and how to assess generated responses. The developers should improve their models to enhance the accuracy of given answers. The study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.},
  keywords={Chatbots;Education;Codes;Encoding;Performance evaluation;Oral communication;ChatGPT;students’ perceptions;education},
  doi={10.1109/ACCESS.2023.3268224},
  ISSN={2169-3536},
  month={},}@ARTICLE{8643084,
  author={Wang, Shuai and Ouyang, Liwei and Yuan, Yong and Ni, Xiaochun and Han, Xuan and Wang, Fei-Yue},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Blockchain-Enabled Smart Contracts: Architecture, Applications, and Future Trends}, 
  year={2019},
  volume={49},
  number={11},
  pages={2266-2277},
  abstract={In recent years, the rapid development of cryptocurrencies and their underlying blockchain technology has revived Szabo’s original idea of smart contracts, i.e., computer protocols that are designed to automatically facilitate, verify, and enforce the negotiation and implementation of digital contracts without central authorities. Smart contracts can find a wide spectrum of potential application scenarios in the digital economy and intelligent industries, including financial services, management, healthcare, and Internet of Things, among others, and also have been integrated into the mainstream blockchain-based development platforms, such as Ethereum and Hyperledger. However, smart contracts are still far from mature, and major technical challenges such as security and privacy issues are still awaiting further research efforts. For instance, the most notorious case might be “The DAO Attack” in June 2016, which led to more than $50 million Ether transferred into an adversary’s account. In this paper, we strive to present a systematic and comprehensive overview of blockchain-enabled smart contracts, aiming at stimulating further research toward this emerging research area. We first introduced the operating mechanism and mainstream platforms of blockchain-enabled smart contracts, and proposed a research framework for smart contracts based on a novel six-layer architecture. Second, both the technical and legal challenges, as well as the recent research progresses, are listed. Third, we presented several typical application scenarios. Toward the end, we discussed the future development trends of smart contracts. This paper is aimed at providing helpful guidance and reference for future research efforts.},
  keywords={Smart contracts;Blockchain;Bitcoin;Market research;Proposals;Blockchain;parallel blockchain;six-layer architecture;smart contracts},
  doi={10.1109/TSMC.2019.2895123},
  ISSN={2168-2232},
  month={Nov},}@ARTICLE{8764449,
  author={Poria, Soujanya and Majumder, Navonil and Mihalcea, Rada and Hovy, Eduard},
  journal={IEEE Access}, 
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances}, 
  year={2019},
  volume={7},
  number={},
  pages={100943-100953},
  abstract={Emotion is intrinsic to humans and consequently, emotion understanding is a key part of human-like artificial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasingly popular as a new research frontier in natural language processing (NLP) due to its ability to mine opinions from the plethora of publicly available conversational data on platforms such as Facebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychological analysis), education (understanding student frustration), and more. In Addition, ERC is also extremely important for generating emotion-aware dialogues that require an understanding of the user’s emotions. Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms. However, it is a difficult problem to solve because of several research challenges. In this paper, we discuss these challenges and shed light on recent research in this field. We also describe the drawbacks of these approaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC.},
  keywords={Emotion recognition;Task analysis;Context modeling;Taxonomy;Natural language processing;Pragmatics;Emotion recognition;sentiment analysis;dialogue systems;natural language processing},
  doi={10.1109/ACCESS.2019.2929050},
  ISSN={2169-3536},
  month={},}@ARTICLE{8746184,
  author={Miklosik, Andrej and Kuchta, Martin and Evans, Nina and Zak, Stefan},
  journal={IEEE Access}, 
  title={Towards the Adoption of Machine Learning-Based Analytical Tools in Digital Marketing}, 
  year={2019},
  volume={7},
  number={},
  pages={85705-85718},
  abstract={Exponential technological expansion creates opportunities for competitive advantage by applying new data-oriented approaches to digital marketing practices. Machine learning (ML) can predict future developments and support decision-making by extracting insights from large amounts of generated data. This functionality greatly impacts and streamlines the strategic decision-making process of organizations. The research gap analysis revealed that a little is known about marketers' attitude toward, and knowledge about, ML tools and their adoption and utilization to support strategic and operational management. The research presented here focuses on the selection and adoption of the ML-driven analytical tools by three distinct groups: marketing agencies, media companies, and advertisers. Qualitative and quantitative research was conducted on a sample of these organizations operating in Slovakia. The findings highlight: 1) the important role of intelligent analytical tools in the creation and deployment of marketing strategies; 2) the lack of knowledge about emerging technologies, such as ML and artificial intelligence (AI); 3) the potential application of the ML tools in marketing, and; 4) the low level of adoption and utilization of the ML-driven analytical tools in marketing management. A framework consisting of enablers and a process map was developed to help organizations identify the opportunities and successfully execute projects that are oriented toward the deployment and adoption of the analytical ML tools in digital marketing.},
  keywords={Tools;Decision making;Strategic planning;Companies;Internet;Big data;data-driven analytical tools;digital marketing;machine learning (ML);marketing agencies;marketing analysis},
  doi={10.1109/ACCESS.2019.2924425},
  ISSN={2169-3536},
  month={},}@ARTICLE{10190626,
  author={AlZubi, Ahmad Ali and Galyna, Kalda},
  journal={IEEE Access}, 
  title={Artificial Intelligence and Internet of Things for Sustainable Farming and Smart Agriculture}, 
  year={2023},
  volume={11},
  number={},
  pages={78686-78692},
  abstract={Technologies like AI and IoT have been employed in farming for some time now, along with other forms of cutting-edge computer science. There has been a shift in recent years toward thinking about how to put this new technology to use. Agriculture has provided a large portion of humanity’s sustenance for thousands of years, with its most notable contribution being the widespread use of effective agricultural practices for several crop types. The advent of cutting-edge IoT know-how with the ability to monitor agricultural ecosystems and guarantee high-quality production is underway. Smart Sustainable Agriculture continues to face formidable hurdles due to the widespread dispersion of agricultural procedures, such as the deployment and administration of IoT and AI devices, the sharing of data and administration, interoperability, and the analysis and storage of enormous data quantities. This work initially analyses existing Internet-of-Things technologies used in Smart Sustainable Agriculture (SSA) to discover architectural components that might facilitate the development of SSA platforms. This paper examines the state of research and development in SSA, pays attention to the current form of information, and proposes an Internet of Things (IoT) and artificial intelligence (AI) framework as a starting point for SSA.},
  keywords={Smart agriculture;Crops;Artificial intelligence;Internet of Things;Farming;Monitoring;Soil;Internet of Things;Sustainable development;Smart agriculture;Internet of Things (IoT);artificial intelligence (AI);smart sustainable agriculture (SSA);smart farming},
  doi={10.1109/ACCESS.2023.3298215},
  ISSN={2169-3536},
  month={},}@ARTICLE{5733835,
  author={},
  journal={ISO/IEC/IEEE 24765:2010(E)}, 
  title={ISO/IEC/IEEE International Standard - Systems and software engineering -- Vocabulary}, 
  year={2010},
  volume={},
  number={},
  pages={1-418},
  abstract={The systems and software engineering disciplines are continuing to mature while information technology advances. This International Standard was prepared to collect and standardize terminology. Its purpose is to identify terms currently in use in the field and standard definitions for these terms. It is intended to serve as a useful reference for those in the Information Technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute (PMI). This International Standard replaces IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, which was contributed by the IEEE as a source document. The approach and lexical exactitude of IEEE Std 610.12-1990 served as a model for this International Standard. Nevertheless, approximately two thirds of the definitions in this International Standard are new since IEEE Std 610.12 was last updated in 1990, a reflection of the continued evolution in the field.},
  keywords={IEEE standards;ISO standards;IEC standards;Software engineering;Dictionaries;computer;dictionary;information technology;software engineering;systems engineering;terminology;vocabulary},
  doi={10.1109/IEEESTD.2010.5733835},
  ISSN={},
  month={Dec},}@ARTICLE{9490241,
  author={Tedre, Matti and Toivonen, Tapani and Kahila, Juho and Vartiainen, Henriikka and Valtonen, Teemu and Jormanainen, Ilkka and Pears, Arnold},
  journal={IEEE Access}, 
  title={Teaching Machine Learning in K–12 Classroom: Pedagogical and Technological Trajectories for Artificial Intelligence Education}, 
  year={2021},
  volume={9},
  number={},
  pages={110558-110572},
  abstract={Over the past decades, numerous practical applications of machine learning techniques have shown the potential of AI-driven and data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning and AI in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based “traditional” programming is a central aspect and building block in developing next generation computational thinking.},
  keywords={Education;Machine learning;Programming profession;Automation;Task analysis;Terminology;Technological innovation;Machine learning;artificial intelligence;K-12;school;computing education;computational thinking;pedagogy},
  doi={10.1109/ACCESS.2021.3097962},
  ISSN={2169-3536},
  month={},}@ARTICLE{9405644,
  author={Kashevnik, Alexey and Shchedrin, Roman and Kaiser, Christian and Stocker, Alexander},
  journal={IEEE Access}, 
  title={Driver Distraction Detection Methods: A Literature Review and Framework}, 
  year={2021},
  volume={9},
  number={},
  pages={60063-60076},
  abstract={Driver inattention and distraction are the main causes of road accidents, many of which result in fatalities. To reduce road accidents, the development of information systems to detect driver inattention and distraction is essential. Currently, distraction detection systems for road vehicles are not yet widely available or are limited to specific causes of driver inattention such as driver fatigue. Despite the increasing automation of driving due to the availability of increasingly sophisticated assistance systems, the human driver will continue to play a longer role as supervisor of vehicle automation. With this in mind, we review the published scientific literature on driver distraction detection methods and integrate the identified approaches into a holistic framework that is the main contribution of the paper. Based on published scientific work, our driver distraction detection framework contains a structured summary of reviewed approaches for detecting the three main distraction detection approaches: manual distraction, visual distraction, and cognitive distraction. Our framework visualizes the whole detection information chain from used sensors, measured data, computed data, computed events, inferred behavior, and inferred distraction type. Besides providing a sound summary for researchers interested in distracted driving, we discuss several practical implications for the development of driver distraction detection systems that can also combine different approaches for higher detection quality. We think our research can be useful despite - or even because of - the great developments in automated driving.},
  keywords={Vehicles;Automation;Task analysis;Monitoring;Visualization;Taxonomy;Psychology;Automotive applications;automated vehicles;data systems;distraction detection;driver distraction;driver monitoring;driving distraction;intelligent transportation;vehicle driving},
  doi={10.1109/ACCESS.2021.3073599},
  ISSN={2169-3536},
  month={},}@ARTICLE{9409047,
  author={Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
  journal={IEEE Access}, 
  title={From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction}, 
  year={2021},
  volume={9},
  number={},
  pages={60447-60458},
  abstract={In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
  keywords={Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
  doi={10.1109/ACCESS.2021.3074559},
  ISSN={2169-3536},
  month={},}@ARTICLE{8915693,
  author={Estevez, Julian and Garate, Gorka and Graña, Manuel},
  journal={IEEE Access}, 
  title={Gentle Introduction to Artificial Intelligence for High-School Students Using Scratch}, 
  year={2019},
  volume={7},
  number={},
  pages={179027-179036},
  abstract={The importance of educating the next generations in the understanding of the fundamentals of the upcoming scientific and technological innovations that will force a broad social and economical paradigm change can not be overstressed. One such breakthrough technologies is Artificial Intelligence (AI), specifically machine learning algorithms. Nowadays, the public has little understanding of the workings and implications of AI techniques that are already entering their lives in many ways. We aim to achieve widespread public understanding of these issues in an experiential learning framework. Following a design based research approach, we propose to implement program coding scaffoldings to teach and experiment some basic mechanisms of AI systems. Such experiments would be shedding new light into AI potentials and limitations. In this paper we focus on innovative ways to introduce high school students to the fundamentals and operation of two of the most popular AI algorithms. We describe the elements of a workshop where we provide an academic use-create-modify scaffolding where students work on the Scratch partial coding of the algorithms so they can explore the behavior of the algorithm, gaining understanding of the underlying computational thinking of AI processes. The extent of the impact on the students of this experience is measured through questionnaires filled before and after participation in the workshop. Preliminary experiments offer encouraging results, showing that the workshop has differential impact on the way students understand AI.},
  keywords={Artificial intelligence;Conferences;Tools;Programming profession;Education;Scratch programming;teaching AI fundamentals;public AI awareness},
  doi={10.1109/ACCESS.2019.2956136},
  ISSN={2169-3536},
  month={},}@ARTICLE{954607,
  author={Picard, R.W. and Vyzas, E. and Healey, J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Toward machine emotional intelligence: analysis of affective physiological state}, 
  year={2001},
  volume={23},
  number={10},
  pages={1175-1191},
  abstract={The ability to recognize emotion is one of the hallmarks of emotional intelligence, an aspect of human intelligence that has been argued to be even more important than mathematical and verbal intelligences. This paper proposes that machine intelligence needs to include emotional intelligence and demonstrates results toward this goal: developing a machine's ability to recognize the human affective state given four physiological signals. We describe difficult issues unique to obtaining reliable affective data and collect a large set of data from a subject trying to elicit and experience each of eight emotional states, daily, over multiple weeks. This paper presents and compares multiple algorithms for feature-based recognition of emotional state from this data. We analyze four physiological signals that exhibit problematic day-to-day variations: The features of different emotions on the same day tend to cluster more tightly than do the features of the same emotion on different days. To handle the daily variations, we propose new features and algorithms and compare their performance. We find that the technique of seeding a Fisher Projection with the results of sequential floating forward search improves the performance of the Fisher Projection and provides the highest recognition rates reported to date for classification of affect from physiology: 81 percent recognition accuracy on eight classes of emotion, including neutral.},
  keywords={Machine intelligence;Intelligent agent;Emotion recognition;Humans;Clustering algorithms;Pattern recognition;Neuroscience;Data analysis;Signal analysis;Physiology},
  doi={10.1109/34.954607},
  ISSN={1939-3539},
  month={Oct},}@ARTICLE{10223039,
  author={Guo, Ziyue and Zhu, Zongyang and Li, Yizhi and Cao, Shidong and Chen, Hangyue and Wang, Gaoang},
  journal={IEEE Access}, 
  title={AI Assisted Fashion Design: A Review}, 
  year={2023},
  volume={11},
  number={},
  pages={88403-88415},
  abstract={This review explores the integration of enhanced personalization and seamless multimodal interfaces in the field of fashion design and recommendation. We examine the increasing demand for personalized fashion experiences and the potential of multimodal interfaces in facilitating effective communication between designers and users. By leveraging user preferences, body measurements, and style choices, artificial intelligence (AI) systems can deliver highly personalized fashion recommendations. The integration of various input modalities, including text, images and sketches, enables designers and users to communicate their design ideas with ease. The primary results highlight the transformative potential of enhanced personalization and seamless multimodal interfaces, empowering designers and consumers to co-create unique and personalized designs. This paradigm shift fosters a deeper level of engagement and creativity within the fashion industry. Embracing this advancement unlocks unprecedented opportunities for designers, brands, and consumers, ushering in a new era of innovation and creativity in fashion design.},
  keywords={Artificial intelligence;Task analysis;Feature extraction;Surveys;Context modeling;Adaptation models;Deep learning;Clothing industry;Artificial intelligence;deep learning;fashion design},
  doi={10.1109/ACCESS.2023.3306235},
  ISSN={2169-3536},
  month={},}@ARTICLE{9812604,
  author={Ahmad, Naqash and Ghadi, Yazeed and Adnan, Muhammad and Ali, Mansoor},
  journal={IEEE Access}, 
  title={Load Forecasting Techniques for Power System: Research Challenges and Survey}, 
  year={2022},
  volume={10},
  number={},
  pages={71054-71090},
  abstract={The main and pivot part of electric companies is the load forecasting. Decision-makers and think tank of power sectors should forecast the future need of electricity with large accuracy and small error to give uninterrupted and free of load shedding power to consumers. The demand of electricity can be forecasted amicably by many Machine Learning (ML), Deep Learning (DL) and Artificial Intelligence (AI) techniques among which hybrid methods are most popular. The present technologies of load forecasting and present work regarding combination of various ML, DL and AI algorithms are reviewed in this paper. The comprehensive review of single and hybrid forecasting models with functions; advantages and disadvantages are discussed in this paper. The comparison between the performance of the models in terms of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) values are compared and discussed with literature of different models to support the researchers to select the best model for load prediction. This comparison validates the fact that the hybrid forecasting models will provide a more optimal solution.},
  keywords={Load forecasting;Load modeling;Forecasting;Predictive models;Biological system modeling;Companies;Meteorology;Load forecasting;machine learning;load shedding;root mean squared error;mean absolute percentage error},
  doi={10.1109/ACCESS.2022.3187839},
  ISSN={2169-3536},
  month={},}@ARTICLE{9319727,
  author={Chen, Xing and Liu, Guizhong},
  journal={IEEE Internet of Things Journal}, 
  title={Energy-Efficient Task Offloading and Resource Allocation via Deep Reinforcement Learning for Augmented Reality in Mobile Edge Networks}, 
  year={2021},
  volume={8},
  number={13},
  pages={10843-10856},
  abstract={The augmented reality (AR) applications have been widely used in the field of Internet of Things (IoT) because of good immersion experience for users, but their ultralow delay demand and high energy consumption bring a huge challenge to the current communication system and terminal power. The emergence of mobile-edge computing (MEC) provides a good thinking to solve this challenge. In this article, we study an energy-efficient task offloading and resource allocation scheme for AR in both the single-MEC and multi-MEC systems. First, a more specific and detailed AR application model is established as a directed acyclic graph according to its internal functionality. Second, based on this AR model, a joint optimization problem of task offloading and resource allocation is formulated to minimize the energy consumption of each user subject to the latency requirement and the limited resources. The problem is a mixed multiuser competition and cooperation problem, which involves the task offloading decision, uplink/downlink transmission resources allocation, and computing resources allocation of users and MEC server. Since it is an NP-hard problem and the communication environment is dynamic, it is difficult for genetic algorithms or heuristic algorithms to solve. Therefore, we propose an intelligent and efficient resource allocation and task offloading algorithm based on the deep reinforcement learning framework of multiagent deep deterministic policy gradient (MADDPG) in a dynamic communication environment. Finally, simulation results show that the proposed algorithm can greatly reduce the energy consumption of each user terminal.},
  keywords={Task analysis;Servers;Optimization;Resource management;Energy consumption;Computational modeling;Heuristic algorithms;Augmented reality (AR);deep reinforcement learning;Internet of Things (IoT);mobile-edge computing (MEC);multiagent deep deterministic policy gradient (MADDPG);resource allocation;task offloading},
  doi={10.1109/JIOT.2021.3050804},
  ISSN={2327-4662},
  month={July},}@INPROCEEDINGS{6168399,
  author={Patidar, Shyam and Rane, Dheeraj and Jain, Pritesh},
  booktitle={2012 Second International Conference on Advanced Computing & Communication Technologies}, 
  title={A Survey Paper on Cloud Computing}, 
  year={2012},
  volume={},
  number={},
  pages={394-398},
  abstract={Cloud computing is the biggest buzz in the computer world these days -- maybe too big of a buzz. Cloud computing means different things to different people. Cloud computing is not a small, undeveloped branch of IT. Research firm IDC thinks that cloud computing will reach $42 billion in 2012. You can do everything on cloud from running applications to storing data off-site. You can run entire operating systems on the cloud. This paper is for anyone who may have recently heard the term "cloud computing" for the first time and needs to know what it is and how it helps them.},
  keywords={Cloud computing;Hardware;Companies;Investments;Google;Computational modeling;Cloud Computing;Utility Computing;Data Center;On-Demand Computing},
  doi={10.1109/ACCT.2012.15},
  ISSN={2327-0659},
  month={Jan},}@ARTICLE{9669005,
  author={Sivapalan, Gawsalyan and Nundy, Koushik Kumar and Dev, Soumyabrata and Cardiff, Barry and John, Deepu},
  journal={IEEE Transactions on Biomedical Circuits and Systems}, 
  title={ANNet: A Lightweight Neural Network for ECG Anomaly Detection in IoT Edge Sensors}, 
  year={2022},
  volume={16},
  number={1},
  pages={24-35},
  abstract={In this paper, we propose a lightweight neural network for real-time electrocardiogram (ECG) anomaly detection and system level power reduction of wearable Internet of Things (IoT) Edge sensors. The proposed network utilizes a novel hybrid architecture consisting of Long Short Term Memory (LSTM) cells and Multi-Layer Perceptrons (MLP). The LSTM block takes a sequence of coefficients representing the morphology of ECG beats while the MLP input layer is fed with features derived from instantaneous heart rate. Simultaneous training of the blocks pushes the overall network to learn distinct features complementing each other for making decisions. The network was evaluated in terms of accuracy, computational complexity, and power consumption using data from the MIT-BIH arrhythmia database. To address the class imbalance in the dataset, we augmented the dataset using SMOTE algorithm for network training. The network achieved an average classification accuracy of 97% across several records in the database. Further, the network was mapped to a fixed point model, retrained in a bit accurate fixed-point environment to compensate for the quantization error, and ported to an ARM Cortex M4 based embedded platform. In laboratory testing, the overall system was successfully demonstrated, and a significant saving of $\simeq \!\! 50\%$ power was achieved by gating the wireless transmission using the classifier. Wireless transmission was enabled only to transmit the beats deemed anomalous by the classifier. The proposed technique compares favourably with current methods in terms of computational complexity and has the advantage of stand-alone operation in the edge node, without the need for always-on wireless connectivity making it ideal for IoT wearable devices.},
  keywords={Electrocardiography;Feature extraction;Wireless sensor networks;Training;Databases;Wireless communication;Power demand;Anomaly detection;edge computing;IoT sensors;LSTM;MLP;neural networks;power reduction},
  doi={10.1109/TBCAS.2021.3137646},
  ISSN={1940-9990},
  month={Feb},}@ARTICLE{9354557,
  author={Abernathey, Ryan P. and Augspurger, Tom and Banihirwe, Anderson and Blackmon-Luca, Charles C. and Crone, Timothy J. and Gentemann, Chelle L. and Hamman, Joseph J. and Henderson, Naomi and Lepore, Chiara and McCaie, Theo A. and Robinson, Niall H. and Signell, Richard P.},
  journal={Computing in Science & Engineering}, 
  title={Cloud-Native Repositories for Big Scientific Data}, 
  year={2021},
  volume={23},
  number={2},
  pages={26-35},
  abstract={Scientific data have traditionally been distributed via downloads from data server to local computer. This way of working suffers from limitations as scientific datasets grow toward the petabyte scale. A “cloud-native data repository,” as defined in this article, offers several advantages over traditional data repositories—performance, reliability, cost-effectiveness, collaboration, reproducibility, creativity, downstream impacts, and access and inclusion. These objectives motivate a set of best practices for cloud-native data repositories: analysis-ready data, cloud-optimized (ARCO) formats, and loose coupling with data-proximate computing. The Pangeo Project has developed a prototype implementation of these principles by using open-source scientific Python tools. By providing an ARCO data catalog together with on-demand, scalable distributed computing, Pangeo enables users to process big data at rates exceeding 10 GB/s. Several challenges must be resolved in order to realize cloud computing’s full potential for scientific research, such as organizing funding, training users, and enforcing data privacy requirements.},
  keywords={Cloud computing;Training data;Computational modeling;Reproducibility of results;Collaboration;Reliability;Distributed databases},
  doi={10.1109/MCSE.2021.3059437},
  ISSN={1558-366X},
  month={March},}@ARTICLE{9387490,
  author={Granger, Brian E. and Pérez, Fernando},
  journal={Computing in Science & Engineering}, 
  title={Jupyter: Thinking and Storytelling With Code and Data}, 
  year={2021},
  volume={23},
  number={2},
  pages={7-14},
  abstract={Project Jupyter is an open-source project for interactive computing widely used in data science, machine learning, and scientific computing. We argue that even though Jupyter helps users perform complex, technical work, Jupyter itself solves problems that are fundamentally human in nature. Namely, Jupyter helps humans to think and tell stories with code and data. We illustrate this by describing three dimensions of Jupyter: 1) interactive computing; 2) computational narratives; and 3) the idea that Jupyter is more than software. We illustrate the impact of these dimensions on a community of practice in earth and climate science.},
  keywords={Open source software;Scientific computing;Machine learning;Data science;Open source software;Meteorology},
  doi={10.1109/MCSE.2021.3059263},
  ISSN={1558-366X},
  month={March},}@ARTICLE{9006805,
  author={Hussein, Mohamed K. and Mousa, Mohamed H.},
  journal={IEEE Access}, 
  title={Efficient Task Offloading for IoT-Based Applications in Fog Computing Using Ant Colony Optimization}, 
  year={2020},
  volume={8},
  number={},
  pages={37191-37201},
  abstract={The current thinking concerning computations required by Internet of Things (IoT) applications is shifting toward fog computing instead of cloud computing, thereby achieving most of the required computations at the network edge of the IoT devices. Fog computing can thus improve the quality of service of delay-sensitive applications by allowing such applications to take advantage of the low latency provided by fog computing rather than the high latency of the cloud. Therefore, tasks in various IoT applications must be effectively distributed over the fog nodes to improve the quality of service, specifically the task response time. In this paper, two nature-inspired meta-heuristic schedulers, namely ant colony optimization (ACO) and particle swarm optimization (PSO), are used to propose two different scheduling algorithms to effectively load balance IoT tasks over the fog nodes under communication cost and response time considerations. The experimental results of the proposed algorithms are compared with those of the round robin (RR) algorithm. The evaluations show that the proposed ACO-based scheduler achieves an improvement in the response times of IoT applications compared to the proposed PSO-based and RR algorithms and effectively load balances the fog nodes.},
  keywords={Task analysis;Cloud computing;Edge computing;Time factors;Quality of service;Delays;Computer architecture;Fog computing;Internet of Things;quality of service;task offloading and scheduling},
  doi={10.1109/ACCESS.2020.2975741},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8253363,
  author={Ahmad, Adang Suwandi},
  booktitle={2017 International Symposium on Electronics and Smart Devices (ISESD)}, 
  title={Brain inspired cognitive artificial intelligence for knowledge extraction and intelligent instrumentation system}, 
  year={2017},
  volume={},
  number={},
  pages={352-356},
  abstract={Artificial intelligence evolves with the development of computers even rely on computational development. The ways and processes of human thinking developed by Psychologists and welcomed by computational experts produce the science of Artificial Intelligence. This continues with the development of cognitive science that encourages the development of Artificial Intelligence to Cognitive Thinking Intelligence, a new pathway to the science of Artificial Intelligence that can emulate human cognitive abilities even if not 100%. Emulation of human cognitive abilities is developed based on the modeling of system interaction with the environment and information fusion, which can be used to conduct Inferencing, so when this occurs repeatedly it will produce knowledge that grows. This process is called Knowledge Growing System which is Brain Inspired Cognitive Artificial Intelligence and can be used for information extraction and when applied to instrumentation system will realize Intelligent Instrumentation System.},
  keywords={Artificial intelligence;Heart;Data mining;Software;Instruments;Electrocardiography;Smart devices;Artificial Intelligence;Cognitive Artificial Intelligece;Knowledge Extraction},
  doi={10.1109/ISESD.2017.8253363},
  ISSN={},
  month={Oct},}@ARTICLE{10050860,
  author={Han, Binghui and Zahraoui, Younes and Mubin, Marizan and Mekhilef, Saad and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
  journal={IEEE Access}, 
  title={Home Energy Management Systems: A Review of the Concept, Architecture, and Scheduling Strategies}, 
  year={2023},
  volume={11},
  number={},
  pages={19999-20025},
  abstract={Growing electricity demand, the deployment of renewable energy sources and the widespread use of smart home appliances provide new opportunities for home energy management systems (HEMSs), which can be defined as systems that improve the overall energy production and consumption of residential buildings by controlling and scheduling the use of household equipment. By saving energy, reducing residential electricity costs, optimizing the utilization rate and reliability of utility companies’ power systems, and reducing air pollution for society, HEMSs lead to an enhancement in the socioeconomic development of low-carbon economies. This review aims to systematically analyze and summarize the development trends and challenges of HEMSs in recent years. This paper reviews the development history of the HEMS architecture and discusses the characteristics of several major communication technologies in the current HEMS infrastructure. In addition, the common objectives and constraints related to scheduling optimization are classified, and several optimization methods in the literature, including various intelligent algorithms, have been introduced, compared, and critically analyzed. Furthermore, experimental studies and challenges in the real world are also summarized and recommendations are given. This paper reveals the trend from simple to complex in the architecture and functionality of HEMSs, discusses the challenges for future improvements in modeling and scheduling, and shows the development of various modeling and scheduling methods. Based on this review, researchers can gain a comprehensive understanding of current research trends in HEMSs and open up ideas for developing new modeling and scheduling approaches by gaining insight into the trade-offs between optimum solutions and computational complexity.},
  keywords={Optimization;Energy management systems;Home appliances;Renewable energy sources;Market research;Reliability;Optimal scheduling;Demand response;home appliances;home energy management system;optimization;renewable energy resources;smart grid},
  doi={10.1109/ACCESS.2023.3248502},
  ISSN={2169-3536},
  month={},}@ARTICLE{6030926,
  author={Rodriguez, Rosa M. and Martinez, Luis and Herrera, Francisco},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Hesitant Fuzzy Linguistic Term Sets for Decision Making}, 
  year={2012},
  volume={20},
  number={1},
  pages={109-119},
  abstract={Dealing with uncertainty is always a challenging problem, and different tools have been proposed to deal with it. Recently, a new model that is based on hesitant fuzzy sets has been presented to manage situations in which experts hesitate between several values to assess an indicator, alternative, variable, etc. Hesitant fuzzy sets suit the modeling of quantitative settings; however, similar situations may occur in qualitative settings so that experts think of several possible linguistic values or richer expressions than a single term for an indicator, alternative, variable, etc. In this paper, the concept of a hesitant fuzzy linguistic term set is introduced to provide a linguistic and computational basis to increase the richness of linguistic elicitation based on the fuzzy linguistic approach and the use of context-free grammars by using comparative terms. Then, a multicriteria linguistic decision-making model is presented in which experts provide their assessments by eliciting linguistic expressions. This decision model manages such linguistic expressions by means of its representation using hesitant fuzzy linguistic term sets.},
  keywords={Pragmatics;Fuzzy sets;Grammar;Semantics;Decision making;Uncertainty;Humans;Context-free grammar;fuzzy linguistic approach;hesitant fuzzy sets;linguistic decision making;linguistic information},
  doi={10.1109/TFUZZ.2011.2170076},
  ISSN={1941-0034},
  month={Feb},}@ARTICLE{9613752,
  author={Angara, Prashanti Priya and Stege, Ulrike and MacLean, Andrew and Müller, Hausi A. and Markham, Tom},
  journal={IEEE Transactions on Quantum Engineering}, 
  title={Teaching Quantum Computing to High-School-Aged Youth: A Hands-On Approach}, 
  year={2022},
  volume={3},
  number={},
  pages={1-15},
  abstract={Quantum computing is aninterdisciplinary field that lies at the intersection of mathematics, quantum physics, and computer science, and finds applications in areas including optimization, machine learning, and simulation of chemical, physical, and biological systems. It has the potential to help solve problems that so far have no satisfying method solving them, and to provide significant speedup to solutions when compared with their best classical approaches. In turn, quantum computing may allow us to solve problems for inputs that so far are deemed practically intractable. With the computational power of quantum computers and the proliferation of quantum development kits, quantum computing is anticipated to become mainstream, and the demand for a skilled workforce in quantum computing is expected to increase significantly. Therefore, quantum computing education is ramping up. This article describes our experiences in designing and delivering quantum computing workshops for youth (Grades 9–12). We introduce students to the world of quantum computing in innovative ways, such as newly designed unplugged activities for teaching basic quantum computing concepts. We also take a programmatic approach and introduce students to the IBM Quantum Experience using Qiskit and Jupyter notebooks. Our contributions are as follows. First, we present creative ways to teach quantum computing to youth with little or no experience in science, technology, engineering, and mathematics areas; second, we discuss diversity and highlight various pathways into quantum computing from quantum software to quantum hardware; and third, we discuss the design and delivery of online and in-person motivational, introductory, and advanced workshops for youth.},
  keywords={Quantum computing;Quantum mechanics;Conferences;Qubit;Programming profession;Industries;Software;Computer science (CS) unplugged (CS Unplugged);education;entanglement;high-school-aged youth;measurement;qiskit;quantum computing;quantum computing games;quantum gates;quantum teleportation;qubit systems;superposition;teachers;training;workforce development},
  doi={10.1109/TQE.2021.3127503},
  ISSN={2689-1808},
  month={},}
