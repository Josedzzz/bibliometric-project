@inproceedings{10.1145/237814.237866,
author = {Grover, Lov K.},
title = {A fast quantum mechanical algorithm for database search},
year = {1996},
isbn = {0897917855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/237814.237866},
doi = {10.1145/237814.237866},
booktitle = {Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing},
pages = {212–219},
numpages = {8},
location = {Philadelphia, Pennsylvania, USA},
series = {STOC '96}
}

@inproceedings{10.1145/1390156.1390294,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and composing robust features with denoising autoencoders},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390294},
doi = {10.1145/1390156.1390294},
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1096–1103},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/512950.512973,
author = {Cousot, Patrick and Cousot, Radhia},
title = {Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},
year = {1977},
isbn = {9781450373500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512950.512973},
doi = {10.1145/512950.512973},
abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
pages = {238–252},
numpages = {15},
location = {Los Angeles, California},
series = {POPL '77}
}

@proceedings{10.1145/349299,
title = {PLDI '00: Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
year = {2000},
isbn = {1581131992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, British Columbia, Canada}
}

@inproceedings{10.1145/52324.52356,
author = {Jacobson, V.},
title = {Congestion avoidance and control},
year = {1988},
isbn = {0897912799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/52324.52356},
doi = {10.1145/52324.52356},
abstract = {In October of '86, the Internet had the first of what became a series of 'congestion collapses'. During this period, the data throughput from LBL to UC Berkeley (sites separated by 400 yards and three IMP hops) dropped from 32 Kbps to 40 bps. Mike Karels1 and I were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad. We wondered, in particular, if the 4.3BSD (Berkeley UNIX) TCP was mis-behaving or if it could be tuned to work better under abysmal network conditions. The answer to both of these questions was “yes”.Since that time, we have put seven new algorithms into the 4BSD TCP:
round-trip-time variance estimationexponential retransmit timer backoffslow-startmore aggressive receiver ack policydynamic window sizing on congestionKarn's clamped retransmit backofffast retransmit Our measurements and the reports of beta testers suggest that the final product is fairly good at dealing with congested conditions on the Internet.This paper is a brief description of (i) - (v) and the rationale behind them. (vi) is an algorithm recently developed by Phil Karn of Bell Communications Research, described in [KP87]. (viii) is described in a soon-to-be-published RFC.Algorithms (i) - (v) spring from one observation: The flow on a TCP connection (or ISO TP-4 or Xerox NS SPP connection) should obey a 'conservation of packets' principle. And, if this principle were obeyed, congestion collapse would become the exception rather than the rule. Thus congestion control involves finding places that violate conservation and fixing them.By 'conservation of packets' I mean that for a connection 'in equilibrium', i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call 'conservative': A new packet isn't put into the network until an old packet leaves. The physics of flow predicts that systems with this property should be robust in the face of congestion. Observation of the Internet suggests that it was not particularly robust. Why the discrepancy?There are only three ways for packet conservation to fail:
The connection doesn't get to equilibrium, orA sender injects a new packet before an old packet has exited, orThe equilibrium can't be reached because of resource limits along the path. In the following sections, we treat each of these in turn.},
booktitle = {Symposium Proceedings on Communications Architectures and Protocols},
pages = {314–329},
numpages = {16},
location = {Stanford, California, USA},
series = {SIGCOMM '88}
}

@article{10.1145/52325.52356,
author = {Jacobson, V.},
title = {Congestion avoidance and control},
year = {1988},
issue_date = {August 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/52325.52356},
doi = {10.1145/52325.52356},
abstract = {In October of '86, the Internet had the first of what became a series of 'congestion collapses'. During this period, the data throughput from LBL to UC Berkeley (sites separated by 400 yards and three IMP hops) dropped from 32 Kbps to 40 bps. Mike Karels1 and I were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad. We wondered, in particular, if the 4.3BSD (Berkeley UNIX) TCP was mis-behaving or if it could be tuned to work better under abysmal network conditions. The answer to both of these questions was “yes”.Since that time, we have put seven new algorithms into the 4BSD TCP:
round-trip-time variance estimationexponential retransmit timer backoffslow-startmore aggressive receiver ack policydynamic window sizing on congestionKarn's clamped retransmit backofffast retransmit Our measurements and the reports of beta testers suggest that the final product is fairly good at dealing with congested conditions on the Internet.This paper is a brief description of (i) - (v) and the rationale behind them. (vi) is an algorithm recently developed by Phil Karn of Bell Communications Research, described in [KP87]. (viii) is described in a soon-to-be-published RFC.Algorithms (i) - (v) spring from one observation: The flow on a TCP connection (or ISO TP-4 or Xerox NS SPP connection) should obey a 'conservation of packets' principle. And, if this principle were obeyed, congestion collapse would become the exception rather than the rule. Thus congestion control involves finding places that violate conservation and fixing them.By 'conservation of packets' I mean that for a connection 'in equilibrium', i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call 'conservative': A new packet isn't put into the network until an old packet leaves. The physics of flow predicts that systems with this property should be robust in the face of congestion. Observation of the Internet suggests that it was not particularly robust. Why the discrepancy?There are only three ways for packet conservation to fail:
The connection doesn't get to equilibrium, orA sender injects a new packet before an old packet has exited, orThe equilibrium can't be reached because of resource limits along the path. In the following sections, we treat each of these in turn.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {314–329},
numpages = {16}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/954339.954342,
author = {Zhao, W. and Chellappa, R. and Phillips, P. J. and Rosenfeld, A.},
title = {Face recognition: A literature survey},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/954339.954342},
doi = {10.1145/954339.954342},
abstract = {As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.},
journal = {ACM Comput. Surv.},
month = dec,
pages = {399–458},
numpages = {60},
keywords = {Face recognition, person identification}
}

@proceedings{10.5555/3041021,
title = {WWW '17 Companion: Proceedings of the 26th International Conference on World Wide Web Companion},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {We welcome you to the WWW2017 conference, the 26th of the series, and only the second one to be held in Australia.The annual World Wide Web Conference is the premier international forum to present and discuss progress in research, development, standards, and applications related to the Web. This conference is organised under the aegis of the International World Wide Web Conference Committee (IW3C2) in collaboration with local conference organisers in the host city, in this case, the four public universities in Western Australia: Curtin University, Murdoch University, the University of Western Australia and Edith Cowan University. This year, WWW 2017 is offered as the centerpiece of the inaugural Festival of the Web in Perth, a week-long celebration.WWW 2017 provides an opportunity to hear from the leaders of the web including three distinguished keynote speeches by world-class experts: Mark Pesce, Yoelle Maarek, and Melanie Johnston-Hollitt. There is a rich environment of technical activities, including 164 high quality papers in the Research Tracks, 54 papers in the four alternate tracks, over 100 papers in 15 workshops, 13 tutorial sessions, a Ph.D. Symposium track comprising presentations by seven doctoral students, an Industry track consisting of 20 papers focused on applied research, 20 demonstrations, a W3C track examining the latest Web standards and emerging technologies and 64 posters with, for the first time, a number of these offered as e-posters to augment the static poster panels. Overall, WWW2017 provides more than 400 high quality presentations on the key research and development issues of the World Wide Web.Co-located events in the Festival of the Web 2017 include the 4th Big Data Innovators Gathering (BIG 2017), the Web for All conference (W4A2017), and the 5th Serious Games and Applications for Health conference (SeGaH'17). In addition, several new events include Collaboration-Innovation, a one day conference focusing on building smart business innovation through collaboration; the Trust Factory, a curated forum exploring issues of trust and privacy on the web; and Bytes and Rights, a conference focused on issues of web governance, copyright, digital rights, privacy and security on the web. Finally, the Big Day In is a one-day IT careers conference designed by students for students, including tips and advice for secondary school students interested in IT and the web.Given Perth's location in one of the world's richest areas of natural resources, DeepSensor, a world class gathering of industry professionals, is being conducted as part of the Festival as an opportunity for professionals to share their real-world insights into the continuing development of the internet of things in the mining, oil and gas industries.},
location = {Perth, Australia}
}

@article{10.1145/272991.272995,
author = {Matsumoto, Makoto and Nishimura, Takuji},
title = {Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/272991.272995},
doi = {10.1145/272991.272995},
abstract = {A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 219937 −1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 ≤ v ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p2) where  p is the degree of the polynomial.We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jan,
pages = {3–30},
numpages = {28},
keywords = {k-distribution, m-sequences, GFSR, MT19937, Mersenne primes, Mersenne twister, TGFSR, finite fields, incomplete array, inversive-decimation method, multiple-recursive matrix method, primitive polynomials, random number generation, tempering}
}

@inproceedings{10.1145/1014052.1014073,
author = {Hu, Minqing and Liu, Bing},
title = {Mining and summarizing customer reviews},
year = {2004},
isbn = {1581138881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1014052.1014073},
doi = {10.1145/1014052.1014073},
abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {168–177},
numpages = {10},
keywords = {reviews, sentiment classification, summarization, text mining},
location = {Seattle, WA, USA},
series = {KDD '04}
}

@article{10.1145/963770.963772,
author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
title = {Evaluating collaborative filtering recommender systems},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963772},
doi = {10.1145/963770.963772},
abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {5–53},
numpages = {49},
keywords = {Collaborative filtering, evaluation, metrics, recommender systems}
}

@inproceedings{10.1145/2736277.2741093,
author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
title = {LINE: Large-scale Information Network Embedding},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741093},
doi = {10.1145/2736277.2741093},
abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available onlinefootnote{url{https://github.com/tangjianpku/LINE}}.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1067–1077},
numpages = {11},
keywords = {dimension reduction, feature learning, information network embedding, scalability},
location = {Florence, Italy},
series = {WWW '15}
}

@article{10.5555/1248547.1248548,
author = {Dem\v{s}ar, Janez},
title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
year = {2006},
issue_date = {12/1/2006},
publisher = {JMLR.org},
volume = {7},
issn = {1532-4435},
abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1–30},
numpages = {30}
}

@article{10.1145/235815.235821,
author = {Barber, C. Bradford and Dobkin, David P. and Huhdanpaa, Hannu},
title = {The quickhull algorithm for convex hulls},
year = {1996},
issue_date = {Dec. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/235815.235821},
doi = {10.1145/235815.235821},
abstract = {The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it used less memory. computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serous errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.},
journal = {ACM Trans. Math. Softw.},
month = dec,
pages = {469–483},
numpages = {15},
keywords = {Delaunay triangulation, Voronoi diagram, convex hull, halfspace intersection}
}

@article{10.1145/363235.363259,
author = {Hoare, C. A. R.},
title = {An axiomatic basis for computer programming},
year = {1969},
issue_date = {Oct. 1969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/363235.363259},
doi = {10.1145/363235.363259},
abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.},
journal = {Commun. ACM},
month = oct,
pages = {576–580},
numpages = {5},
keywords = {axiomatic method, formal language definition, machine-independent programming, program documentation, programming language design, theory of programming' proofs of programs}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/2807442,
title = {UIST '15: Proceedings of the 28th Annual ACM Symposium on User Interface Software \&amp; Technology},
year = {2015},
isbn = {9781450337793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are very excited to welcome you to the 28th Annual ACM Symposium on User Interface Software and Technology (UIST), held from November 8-11th 2015, in Charlotte, North Carolina, USA.UIST is the premier forum for the presentation of research innovations in the software and technology of human-computer interfaces. Sponsored by ACM's special interest groups on computer-human interaction (SIGCHI) and computer graphics (SIGGRAPH), UIST brings together researchers and practitioners from diverse areas including graphical \&amp; web user interfaces, tangible \&amp; ubiquitous computing, virtual \&amp; augmented reality, multimedia, new input \&amp; output devices, fabrication, wearable computing and CSCW.UIST 2015 received 297 technical paper submissions. After a thorough review process, the 39-member program committee accepted 70 papers (23.6\%). Each anonymous submission that entered the full review process was first reviewed by three external reviewers, and a meta-review was provided by a program committee member. If, after these four reviews, the submission was deemed to pass a rebuttal threshold, we asked the authors to submit a short rebuttal addressing the reviewers' concerns. A second member of the program committee was then asked to examine the paper, rebuttal, and reviews, and to provide their own meta-review. The program committee met in person in Berkeley, California, USA on June 25th and 26th, 2015, to select which papers to invite for the program. Submissions were accepted only after the authors provided a final revision addressing the committee's comments.In addition to papers, our program includes two papers from the ACM Transactions on Computer-Human Interaction journal (TOCHI), as well as 22 posters, 45 demonstrations, and 8 student presentations in the eleventh annual Doctoral Symposium. Our program also features the seventh annual Student Innovation Contest. Teams from all over the world will compete in this year's contest, which focuses on blurring the lines between art and engineering and creating tools for robotic storytelling. UIST 2015 will feature two keynote presentations. The opening keynote will be given by Ramesh Raskar (MIT Media Lab) on extreme computational imaging. Blaise Aguera Y Arcas from Google will deliver the closing keynote on machine intelligence.We welcome you to Charlotte, a city full of southern hospitality. We hope that you will find the technical program interesting and thought-provoking. We also hope that UIST 2015 will provide you with enjoyable opportunities to engage with fellow researchers from both industry and academia, from institutions around the world.},
location = {Charlotte, NC, USA}
}

@inproceedings{10.1145/345910.345920,
author = {Intanagonwiwat, Chalermek and Govindan, Ramesh and Estrin, Deborah},
title = {Directed diffusion: a scalable and robust communication paradigm for sensor networks},
year = {2000},
isbn = {1581131976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/345910.345920},
doi = {10.1145/345910.345920},
abstract = {Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.},
booktitle = {Proceedings of the 6th Annual International Conference on Mobile Computing and Networking},
pages = {56–67},
numpages = {12},
location = {Boston, Massachusetts, USA},
series = {MobiCom '00}
}

@proceedings{10.1145/1094811,
title = {OOPSLA '05: Proceedings of the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications},
year = {2005},
isbn = {1595930310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to OOPSLA 2005 in San Diego, California, USA, October 16-20, 2005. This is the proceedings of the 20th annual ACM SIGPLAN Conference on Object-Oriented Programming, systems, languages, and Applications, which was initiated in 1986. Technical papers present new ideas, new research, or in-depth reflections on languages, systems, and applications, focusing on objects. This year is first time that the technical program has been split into three parts: research papers, which describe substantiated new research or novel technical results, advance the state of the art, or report on significant experience or experimentation; Onward! papers, which describe new paradigms or metaphors in computing, new thinking about objects, new framings of computational problems or systems, and new technologies; and essays, which are explorations of technology and its impacts, presenting in-depth reflections on technology, its relation to human endeavors, and its philosophical, sociological, psychological, historical, r anthropological underpinnings.Each submission was judged on these criteria:Technical contribution--how substantial is the contributionNovelty--how novel or innovative are the ideasSubstantiation--how well proven is the contributionPresentation--how clearly written and presented is the materialArgument--how compelling or well-made are the arguments in the paperArt/Craft--how well does the paper demonstrate, describe, or promote excellence of artistry or craft in architecture, design, implementation, methodology, or documentationEach paper was assigned to at least three reviewers. Moreover, each paper and its reviews were further reviewed by the leaders of 16 focus groups (Analysis and Design Methods; Design Patterns; Distributed Systems; Experience with OO Applications and Systems; Frameworks and Components; Languages/Design; Languages/Implementation; Languages/Aspects; Object Databases and Persistence; Object Testing and Metrics; Parallel Systems; Programming Environments; Real-Time Systems; Reflection and Metaobject Models; Software Engineering Practices; Theoretical Foundations) who could assign further reviews. Therefore, each paper had in effect four reviews, with some having as many as nine.Each submitted paper co-authored by a program committee member was held to a much higher standard of review through a specific voting process designed to be auditable by the conference chair; each such paper was reviewed by at least six other committee members, on the basis of strict anonymity. The author of a program committee paper was required to leave the meeting room while the paper was being discussed. This year, five papers were submitted by program committee members and none were accepted.In all, 74 papers were submitted and 32 were accepted. OOPSLA 2005 continues the tradition of presenting well-written, carefully selected papers that should be of lasting value to the programming and object communities. And I hope that the new traditions started this year in pursuit of the themes of Explore / Discover / Understand serve well the community of researchers, practitioners, educations, and software thinkers.},
location = {San Diego, CA, USA}
}

@article{10.1145/359576.359585,
author = {Hoare, C. A. R.},
title = {Communicating sequential processes},
year = {1978},
issue_date = {Aug. 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/359576.359585},
doi = {10.1145/359576.359585},
abstract = {This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.},
journal = {Commun. ACM},
month = aug,
pages = {666–677},
numpages = {12},
keywords = {classes, concurrency, conditional critical regions, coroutines, data representations, guarded commands, input, iterative arrays, monitors, multiple entries, multiple exits, nondeterminacy, output, parallel programming, procedures, program structures, programming, programming languages, programming primitives, recursion}
}

@proceedings{10.1145/237721,
title = {POPL '96: Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
year = {1996},
isbn = {0897917693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. Petersburg Beach, Florida, USA}
}

@proceedings{10.1145/178243,
title = {PLDI '94: Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
year = {1994},
isbn = {089791662X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, Florida, USA}
}

@article{10.1145/3298981,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3298981},
doi = {10.1145/3298981},
abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {19},
keywords = {Federated learning, GDPR, transfer learning}
}

@inproceedings{10.1145/312624.312649,
author = {Hofmann, Thomas},
title = {Probabilistic latent semantic indexing},
year = {1999},
isbn = {1581130961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/312624.312649},
doi = {10.1145/312624.312649},
booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {50–57},
numpages = {8},
location = {Berkeley, California, USA},
series = {SIGIR '99}
}

@article{10.1145/1177352.1177355,
author = {Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
title = {Object tracking: A survey},
year = {2006},
issue_date = {2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1177352.1177355},
doi = {10.1145/1177352.1177355},
abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
journal = {ACM Comput. Surv.},
month = dec,
pages = {13–es},
numpages = {45},
keywords = {Appearance models, contour evolution, feature selection, object detection, object representation, point tracking, shape tracking}
}

@inproceedings{10.1145/279943.279962,
author = {Blum, Avrim and Mitchell, Tom},
title = {Combining labeled and unlabeled data with co-training},
year = {1998},
isbn = {1581130570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/279943.279962},
doi = {10.1145/279943.279962},
booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
pages = {92–100},
numpages = {9},
location = {Madison, Wisconsin, USA},
series = {COLT' 98}
}

@article{10.1145/2133806.2133826,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = apr,
pages = {77–84},
numpages = {8}
}

@proceedings{10.1145/113445,
title = {PLDI '91: Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
year = {1991},
isbn = {0897914287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Toronto, Ontario, Canada}
}

@article{10.1145/1968.1972,
author = {Valiant, L. G.},
title = {A theory of the learnable},
year = {1984},
issue_date = {Nov. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/1968.1972},
doi = {10.1145/1968.1972},
journal = {Commun. ACM},
month = nov,
pages = {1134–1142},
numpages = {9},
keywords = {inductive inference, probabilistic models of learning, propositional expressions}
}

@article{10.1145/3326362,
author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
title = {Dynamic Graph CNN for Learning on Point Clouds},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3326362},
doi = {10.1145/3326362},
abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
journal = {ACM Trans. Graph.},
month = oct,
articleno = {146},
numpages = {12},
keywords = {Point cloud, classification, segmentation}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/1065010.1065034,
author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
title = {Pin: building customized program analysis tools with dynamic instrumentation},
year = {2005},
isbn = {1595930566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065010.1065034},
doi = {10.1145/1065010.1065034},
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {190–200},
numpages = {11},
keywords = {dynamic compilation, instrumentation, program analysis tools},
location = {Chicago, IL, USA},
series = {PLDI '05}
}

@article{10.1145/1064978.1065034,
author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
title = {Pin: building customized program analysis tools with dynamic instrumentation},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1064978.1065034},
doi = {10.1145/1064978.1065034},
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
journal = {SIGPLAN Not.},
month = jun,
pages = {190–200},
numpages = {11},
keywords = {dynamic compilation, instrumentation, program analysis tools}
}

@inproceedings{10.1145/2976749.2978318,
author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
title = {Deep Learning with Differential Privacy},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978318},
doi = {10.1145/2976749.2978318},
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {308–318},
numpages = {11},
keywords = {deep learning, differential privacy},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1145/93597.98741,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-tree: an efficient and robust access method for points and rectangles},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98741},
doi = {10.1145/93597.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {322–331},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}

@article{10.1145/93605.98741,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-tree: an efficient and robust access method for points and rectangles},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98741},
doi = {10.1145/93605.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
journal = {SIGMOD Rec.},
month = may,
pages = {322–331},
numpages = {10}
}

@inproceedings{10.1145/945445.945462,
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
title = {Xen and the art of virtualization},
year = {2003},
isbn = {1581137575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/945445.945462},
doi = {10.1145/945445.945462},
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
pages = {164–177},
numpages = {14},
keywords = {hypervisors, paravirtualization, virtual machine monitors},
location = {Bolton Landing, NY, USA},
series = {SOSP '03}
}

@article{10.1145/1165389.945462,
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
title = {Xen and the art of virtualization},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1165389.945462},
doi = {10.1145/1165389.945462},
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
journal = {SIGOPS Oper. Syst. Rev.},
month = oct,
pages = {164–177},
numpages = {14},
keywords = {hypervisors, paravirtualization, virtual machine monitors}
}

@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {Bayesian optimization, black-box optimization, hyperparameter optimization, machine learning system},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@proceedings{10.1145/268946,
title = {POPL '98: Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
year = {1998},
isbn = {0897919793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Diego, California, USA}
}

@inproceedings{10.1145/945445.945450,
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
title = {The Google file system},
year = {2003},
isbn = {1581137575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/945445.945450},
doi = {10.1145/945445.945450},
booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
pages = {29–43},
numpages = {15},
keywords = {clustered storage, data storage, fault tolerance, scalability},
location = {Bolton Landing, NY, USA},
series = {SOSP '03}
}

@article{10.1145/1165389.945450,
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
title = {The Google file system},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1165389.945450},
doi = {10.1145/1165389.945450},
journal = {SIGOPS Oper. Syst. Rev.},
month = oct,
pages = {29–43},
numpages = {15},
keywords = {clustered storage, data storage, fault tolerance, scalability}
}

@inproceedings{10.1145/168588.168596,
author = {Bellare, Mihir and Rogaway, Phillip},
title = {Random oracles are practical: a paradigm for designing efficient protocols},
year = {1993},
isbn = {0897916298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168588.168596},
doi = {10.1145/168588.168596},
abstract = {We argue that the random oracle model—where all parties have access to a public random oracle—provides a bridge between cryptographic theory and cryptographic practice. In the paradigm we suggest, a practical protocol P is produced by first devising and proving correct a protocol PR for the random oracle model, and then replacing oracle accesses by the computation of an “appropriately chosen” function h. This paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security. We illustrate these gains for problems including encryption, signatures, and zero-knowledge proofs.},
booktitle = {Proceedings of the 1st ACM Conference on Computer and Communications Security},
pages = {62–73},
numpages = {12},
location = {Fairfax, Virginia, USA},
series = {CCS '93}
}

@inproceedings{10.1145/233269.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: an efficient data clustering method for very large databases},
year = {1996},
isbn = {0897917944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/233269.233324},
doi = {10.1145/233269.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
booktitle = {Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data},
pages = {103–114},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {SIGMOD '96}
}

@article{10.1145/235968.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: an efficient data clustering method for very large databases},
year = {1996},
issue_date = {June 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/235968.233324},
doi = {10.1145/235968.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
journal = {SIGMOD Rec.},
month = jun,
pages = {103–114},
numpages = {12}
}

@article{10.1145/355984.355989,
author = {Paige, Christopher C. and Saunders, Michael A.},
title = {LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares},
year = {1982},
issue_date = {March 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/355984.355989},
doi = {10.1145/355984.355989},
journal = {ACM Trans. Math. Softw.},
month = mar,
pages = {43–71},
numpages = {29}
}

@proceedings{10.1145/1957656,
title = {HRI '11: Proceedings of the 6th international conference on Human-robot interaction},
year = {2011},
isbn = {9781450305617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI 2011). HRI is a single-track, highly selective annual conference that showcases the very best research and thinking in human-robot interaction. HRI is inherently interdisciplinary and multidisciplinary, reflecting work from researchers in robotics, psychology, cognitive science, HCI, human factors, linguistics, artificial intelligence, organizational behavior, and anthropology.The theme of HRI 2011 is "Real World HRI." The theme is intended to highlight HRI in which basic scientific research is further tested in real world settings or applied to questions that arise in real world settings. One central aspect of this type of research, in contrast to other realms of applied research, is that it is theoretically driven and feeds back to our theoretical understandings. As such, real world research fortifies our understanding of people, robots, and interaction between the two. This year's conference seeks to take up grand challenges of deploying real world human-robot systems.This year we have three keynote speakers. They will discuss their work on gesture (Sotaro Kita), biologically inspired computational vision (Randy O'Reilly), and cognitive robotics (Angelo Cangelosi). We also have a panel to highlight the conference theme: HRI in the real world. This panel brings together leaders from business and industrial robotics that are relying on current robotic technology to accomplish work in the world today.The call for papers attracted 149 full paper submissions (eight page papers) from Asia, Europe, the Middle East, and North America. The program committee conducted a rigorous review process for full papers, accepting 33 full papers for oral presentation and publication in the proceedings. This year, taking advantage of having both ACM and IEEE as the sponsor, all papers are archived in both the ACM Digital Library and IEEE Xplore.Furthermore, 123 late-breaking reports (two page brief papers) were screened for relevance and lightly reviewed; 99 were accepted for presentation at the HRI conference as posters, exposing a broader perspective of solutions, challenges and issues in HRI. They will be made available in the IEEE Xplore as well as the ACM Digital Library. Finally, a total of 18 videos (out of 36 submissions) were accepted based on importance, novelty and entertainment value. Four videos will be shown throughout the conference and the remaining videos will be shown in a special video session.Presented papers describe novel interaction techniques, the design of new robots, experimental evaluations of people and robots, and robots in real-world settings.This year the local hosts will provide three research laboratory tours during the lunch breaks. We hope that visitors enjoy the opportunity to experience the research ideas of the local hosts.},
location = {Lausanne, Switzerland}
}

@inproceedings{10.1145/1658939.1658941,
author = {Jacobson, Van and Smetters, Diana K. and Thornton, James D. and Plass, Michael F. and Briggs, Nicholas H. and Braynard, Rebecca L.},
title = {Networking named content},
year = {2009},
isbn = {9781605586366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1658939.1658941},
doi = {10.1145/1658939.1658941},
abstract = {Network use has evolved to be dominated by content distribution and retrieval, while networking technology still speaks only of connections between hosts. Accessing content and services requires mapping from the what that users care about to the network's where. We present Content-Centric Networking (CCN) which treats content as a primitive - decoupling location from identity, security and access, and retrieving content by name. Using new approaches to routing named content, derived heavily from IP, we can simultaneously achieve scalability, security and performance. We implemented our architecture's basic features and demonstrate resilience and performance with secure file downloads and VoIP calls.},
booktitle = {Proceedings of the 5th International Conference on Emerging Networking Experiments and Technologies},
pages = {1–12},
numpages = {12},
keywords = {content routing, content-based security, content-centric networking},
location = {Rome, Italy},
series = {CoNEXT '09}
}

@inproceedings{10.1145/800157.805047,
author = {Cook, Stephen A.},
title = {The complexity of theorem-proving procedures},
year = {1971},
isbn = {9781450374644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800157.805047},
doi = {10.1145/800157.805047},
abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
booktitle = {Proceedings of the Third Annual ACM Symposium on Theory of Computing},
pages = {151–158},
numpages = {8},
location = {Shaker Heights, Ohio, USA},
series = {STOC '71}
}

@proceedings{10.1145/199448,
title = {POPL '95: Proceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
year = {1995},
isbn = {0897916921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, California, USA}
}

@article{10.1145/3149.214121,
author = {Fischer, Michael J. and Lynch, Nancy A. and Paterson, Michael S.},
title = {Impossibility of distributed consensus with one faulty process},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/3149.214121},
doi = {10.1145/3149.214121},
abstract = {The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.},
journal = {J. ACM},
month = apr,
pages = {374–382},
numpages = {9}
}

@article{10.1145/116873.116880,
author = {Aurenhammer, Franz},
title = {Voronoi diagrams—a survey of a fundamental geometric data structure},
year = {1991},
issue_date = {Sept. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/116873.116880},
doi = {10.1145/116873.116880},
journal = {ACM Comput. Surv.},
month = sep,
pages = {345–405},
numpages = {61},
keywords = {cell complex, clustering, combinatorial complexity, convex hull, crystal structure, divide-and-conquer, geometric data structure, growth model, higher dimensional embedding, hyperplane arrangement, k-set, motion planning, neighbor searching, object modeling, plane-sweep, proximity, randomized insertion, spanning tree, triangulation}
}

@article{10.1145/321062.321069,
author = {Hooke, Robert and Jeeves, T. A.},
title = {`` Direct Search'' Solution of Numerical and Statistical Problems},
year = {1961},
issue_date = {April 1961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321062.321069},
doi = {10.1145/321062.321069},
journal = {J. ACM},
month = apr,
pages = {212–229},
numpages = {18}
}

@inproceedings{10.1145/3079856.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1–12},
numpages = {12},
keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@article{10.1145/3140659.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080246},
doi = {10.1145/3140659.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {1–12},
numpages = {12},
keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network}
}

@proceedings{10.1145/93542,
title = {PLDI '90: Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
year = {1990},
isbn = {0897913647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {White Plains, New York, USA}
}

