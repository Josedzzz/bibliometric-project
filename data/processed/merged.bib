@article{ELGARAHY2025116673,
title = {Biowaste Valorization: Integrating Circular Economy Principles with Artificial Intelligence -Driven Optimization for Sustainable Energy Solutions},
journal = {Journal of Environmental Chemical Engineering},
pages = {116673},
year = {2025},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2025.116673},
url = {https://www.sciencedirect.com/science/article/pii/S2213343725013697},
author = {Ahmed M. Elgarahy and M.G. Eloffy and Ahmed Alengebawy and Dina Aboelela and Ahmed Hammad and Khalid Z. Elwakeel},
keywords = {Biowaste valorization, Circular economy, Artificial intelligence, Techno-economic analysis, Life cycle assessment, Material flow analysis},
abstract = {Biowaste, encompassing food waste and agricultural residues, poses significant environmental challenges while offering transformative opportunities. Traditionally relegated to landfills or incineration, biowaste is increasingly recognized as a renewable resource for producing biofuels, biochemicals, biomaterials, and animal feed. This review offers a systems-level analysis of biowaste characteristics, conversion processes, and derived bioenergy products such as biofuels, biogas, biodiesel, biohydrogen, bioelectricity, and other valuable chemicals. Diverse conversion methods are explored, including anaerobic digestion, fermentation, microbial fuel cells, pyrolysis, and gasification, alongside conventional practices like landfilling and incineration. Emerging analytical advancements are revolutionizing biowaste valorization. Artificial intelligence (AI) and machine learning (ML) techniques are highlighted for their transformative impact. For example, AI-driven optimization of pyrolysis conditions has enhanced biochar yield and quality, while predictive modeling using neural networks has improved the efficiency of anaerobic digestion. Additionally, techno-economic analysis (TEA) demonstrates a significant reduction in operational costs through AI-driven process optimization, and life cycle assessment (LCA) quantifies reductions in environmental impacts, such as greenhouse gas emissions and energy consumption, due to AI-informed process adjustments. Advanced assessment tools, including material flow analysis (MFA), further evaluate resource dynamics within biowaste-to-energy systems. Artificial intelligence and ML optimize waste processing, improving efficiency and product quality. Key analytical tools like TEA, LCA, and MFA assess the financial and environmental viability of these processes.}
}

@article{CHAUDHARY2025100700,
title = {An integrated model to evaluate the transparency in predicting employee churn using explainable artificial intelligence},
journal = {Journal of Innovation & Knowledge},
volume = {10},
number = {3},
pages = {100700},
year = {2025},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2025.100700},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X25000502},
author = {Meenu Chaudhary and Loveleen Gaur and Amlan Chakrabarti and Gurmeet Singh and Paul Jones and Sascha Kraus},
keywords = {Explainable AI, Logistic regression, Random forest, Machine learning, Employee churn},
abstract = {Recent studies focus on machine learning (ML) algorithms for predicting employee churn (ECn) to save probable economic loss, technology leakage, and customer and knowledge transference. However, can human resource professionals rely on algorithms for prediction? Can they decide when the process of prediction is not known? Due to the lack of interpretability, ML models' exclusive nature and growing intricacy make it challenging for field experts to comprehend these multifaceted black boxes. To address the concern of interpretability, trust and transparency of black-box predictions, this study explores the application of explainable artificial intelligence (XAI) in identifying the factors that escalate the ECn, analysing the negative impact on productivity, employee morale and financial stability. We propose a predictive model that compares the best two top-performing algorithms based on the performance metrics. Thereafter, we suggest applying an explainable artificial intelligence based on Shapley values, i.e., the Shapley Additive exPlanations approach (SHAP), to identify and compare the feature importance of top-performing algorithms logistic regression and random forest analysis on our dataset. The interpretability of the predictive outcome unboxes the predictions, enhancing trust and facilitating retention strategies.}
}

@article{RAJALEHTO2025145524,
title = {Comparing feasibility of low-carbon heavy-duty road freight vehicles},
journal = {Journal of Cleaner Production},
pages = {145524},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.145524},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625008741},
author = {Clara Rajalehto and Petri Helo},
keywords = {Total Cost of Ownership, Feasibility, Monte Carlo, Alternative Fuels, Low-Carbon Vehicles, Liquid Biomethane, Electric Vehicle},
abstract = {The transportation sector remains heavily reliant on fossil fuels, rendering it one of the most significant energy consumers and a principal contributor to global pollution. In order to reduce emissions, the EU has established reduction targets and adopted new regulations that set emission standards for heavy-duty vehicles. A significant area of focus in recent literature has been the reduction of emissions through the utilisation of low-carbon vehicles. This case study aims to develop a Total Cost of Ownership model for low-carbon heavy-duty vehicles. This has been achieved by employing empirical data and Monte Carlo simulations to evaluate the viability of the available fuel options. The dataset comprised telemetry data collected from January to October 2023 from a Finnish food logistics company utilising low-carbon fuel options. The findings indicate that, within the context of the studied vehicles, liquid biomethane and electric trucks are currently cost-competitive alternatives. In 82% of cases, electric vehicle trucks exhibited a lower total cost of ownership than diesel or liquid biomethane trucks. Electric vehicles were best suited for shorter hauls, typically under 390 km, due to their limited range and thus higher cost per kilometre. Contribution of this paper is empirical demonstration showing that liquid biomethane vehicles offer the greatest overall potential for cost and emissions cost savings compared to diesel and is the most technically feasible option, but it also has the most cost uncertainty.}
}

@article{KAMAZANI2025115748,
title = {Multi-objective genetic optimization of embodied and operational energy and carbon impacts of buildings in current and future scenarios},
journal = {Energy and Buildings},
volume = {338},
pages = {115748},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.115748},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825004785},
author = {Maryam Abbasi Kamazani and Manish K. Dixit and Sejal Sanjay Shanbhag},
keywords = {Genetic algorithm, NSGA-II, Multi-objective optimization, Future climate, Embodied energy, Operational energy, Operational carbon emissions, Embodied carbon emissions},
abstract = {As climate change continues to pose significant challenges, redefining building design for enhanced lifecycle efficiency has become imperative. This paper investigates how different optimization objectives and varying climatic conditions shape optimal building configurations. This paper explores these performance objectives through an optimization framework that merges genetic algorithms with simulation techniques, leveraging the Energy Plus platform and incorporating embodied impact databases that include energy and carbon emission factors. This approach enables a comprehensive evaluation and optimization of operational and embodied energy, as well as carbon footprints. It also highlights the complexities of the energy-carbon relationship across different climate and energy scenarios. The methodology is applied to a representative office building model in two distinct optimization phases in current and future scenarios. The first phase optimizes the interconnected operational and embodied energy, whereas the second phase operational and embodied carbon emissions. Under current weather conditions, the first phase achieves a 28.17 % reduction in total primary energy consumption compared to the original design. In the second phase, the framework results in a 21.85 % decrease in the total carbon footprint. When future weather scenarios are examined, the first phase yields a 26.36 % reduction in total primary energy use, followed by a 17.9 % decrease in total carbon emissions in the second phase. These findings illustrate the significance of optimizing the energy and environmental impacts of buildings in current and future scenarios.}
}

@article{LEE2025100684,
title = {Success of EMI in higher education and its key components: A meta-analytic structural equation modelling approach},
journal = {Educational Research Review},
volume = {47},
pages = {100684},
year = {2025},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2025.100684},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X25000211},
author = {Hansol Lee and Heath Rose and Ernesto Macaro and Jang Ho Lee},
keywords = {English medium instruction, Meta-analytic structural equation modelling, Higher education, Content leaning, Language learning},
abstract = {The increasing global demand for English Medium Instruction (EMI) in higher education (HE) highlights the need for empirical research to contribute to its success and suggest ways of mitigating the diverse challenges faced by students and institutions. Identifying the key factors that contribute to successful EMI is critical for improving both content and language learning outcomes for students. In the wake of the recent surge in EMI research, more research synthesis is needed to understand how these factors interact to influence EMI success rather than focusing solely on their isolated, bivariate relationships. To address this gap, we conducted meta-analytic structural equation modelling (MASEM) to examine the structural relationships among the key factors affecting the success of EMI. Synthesising data from 50 studies (N = 15,032), our analysis demonstrates that learners’ engagement and English proficiency are pivotal for both content and language learning, with English proficiency being more important for language learning outcomes, although itself considerably also influenced by learners’ anxiety and motivation. Additionally, the moderator analysis shows that learner engagement plays a more significant role in partial EMI contexts, leading to our recommendation for differentiated institutional support in full and partial EMI settings. However, owing to the limited availability of correlation coefficients, some key factors and moderators were excluded, prompting the need for further empirical research to explore these relationships in greater depth.}
}

@article{SANTIAGO2025104644,
title = {Can the lateral mental timeline be automatically activated in language comprehension?},
journal = {Journal of Memory and Language},
volume = {143},
pages = {104644},
year = {2025},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2025.104644},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X25000373},
author = {Julio Santiago and Alessia Beracci and Andrea Flumini and Eva Sanjuan and Marc Ouellet and Pablo Solana},
keywords = {Time, Mental timeline, Automaticity, Conceptual metaphor, Embodied and grounded cognition},
abstract = {The mental representation of time recruits spatial representations, but is space an essential, inescapable feature of mental time? Supporting a positive answer to this question, recent research has reported that lateral (left–right) space is automatically activated in lexical decision tasks in which the temporal reference of the words is irrelevant for the goals of the task (implicit tasks). Here, using always the same set of Spanish verbs and pseudoverbs marked for past or future tense, we assess the space–time congruency effect in reaction time and mouse trajectories, both in an explicit time judgement task and an implicit lexical decision task. Moreover, we report the first confirmatory (preregistered) study in this field of research using long lateral movements in lexical decision. The congruency effect was always significant in time judgement, but non-significant in lexical decision. Moreover, in reaction time this effect was significantly smaller than a Smallest Effect Size Of Interest (SESOI) of 10 ms, and even smaller than a recently reported 9 ms effect. Therefore, it was considered negligible. We conclude that there is no convincing evidence for an automatic activation of the lateral mental timeline in lexical decision.}
}

@article{PANDEY2025106495,
title = {Engineering Advanced Mesoporous Nanomaterials for High Performance Supercapacitors: A Review},
journal = {Surfaces and Interfaces},
pages = {106495},
year = {2025},
issn = {2468-0230},
doi = {https://doi.org/10.1016/j.surfin.2025.106495},
url = {https://www.sciencedirect.com/science/article/pii/S2468023025007527},
author = {Mayank Pandey and K {Deepthi Jayan} and Kalim Deshmukh and V. Nalini and S. Manobalan and T.P. Sumangala and O.C. Pore and G.M. Lohar},
keywords = {Mesoporous nanomaterials, Nanohybrids, Supercapacitors, Nanocomposites},
abstract = {Mesoporous nanomaterials refer to a class of materials that possess a well-defined porous structure at the nanometer scale. These materials exhibit a substantial surface area and an orderly arrangement of interconnected pores, typically ranging in diameter from 2 to 50 nanometers. The term 'mesoporous' signifies that the pore size falls between the microporous range (less than 2 nanometers) and the macroporous range (greater than 50 nanometers). On the other hand, 'Nanohybrids' represent composite materials that amalgamate two or more types of nanoparticles or nanoscale structures to form a novel material with distinctive properties and functions. The exceptional characteristics enable rapid ion diffusion, a substantial specific surface area, and enhanced sites for adsorption and reactions. Consequently, they offer a promising solution for the fabrication of high-performance electrode and catalyst materials for next-generation energy storage and conversion devices. This review primarily offers a brief about various nanomaterials for supercapacitors and their recent advancements. Secondly, it delves into the various methods of synthesizing mesoporous nanomaterials and nanohybrids, with a particular focus on template-based, template-free, and chemical approaches. Thirdly, it explores the applications of mesoporous nanomaterials in the advancement of leading-edge supercapacitors, followed by a discussion of the future challenges and prospects for mesoporous nanomaterials in energy-related fields.}
}

@article{SKORJANC2025110185,
title = {GOReverseLookup: A gene ontology reverse lookup tool},
journal = {Computers in Biology and Medicine},
volume = {191},
pages = {110185},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110185},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525005360},
author = {Aljoša Škorjanc and Vladimir Smrkolj and Nejc Umek},
keywords = {Gene ontology reverse lookup, Reverse lookup, Gene ontology, Gene function, Candidate gene},
abstract = {Background and objective
The Gene Ontology (GO) project has been pivotal in providing a structured framework for characterizing genes and annotating them to specific biological concepts. While traditional gene annotation primarily focuses on mapping genes to GO terms, descriptors of biological concepts, there is a growing need for tools facilitating reverse querying. This paper introduces GOReverseLookup, a novel tool designed to identify over- or underrepresented genes in researcher-defined states of interest (phenotypes), described by sets of GO terms. GOReverseLookup supplements the existing power of Gene Ontology by the possibility of orthologous gene querying across several databases, such as Ensembl and UniProtKB. This combination allows for a more nuanced identification of significant genes across a range of cross-species research contexts.
Methods
GOReverseLookup queries genes associated with input GO terms. Bundles of GO terms encapsulate user-defined states of interest, e.g., angiogenesis. In the second stage of the analysis, all GO terms associated with each gene are fetched, and finally, the statistical relevance of the genes being involved in one (or all) of the defined states of interests is computed.
Results
The two presented use cases illustrate its utility in discovering genes related to rheumatoid arthritis and genes linked with chronic inflammation and tumorigenesis. In both cases, GOReverseLookup discovered a substantial number of genes significantly associated with the aforementioned states of interest.
Conclusions
GOReverseLookup proves to be a valuable resource for unraveling the genetic basis of phenotypes, with diverse practical potentials in functional genomics, systems biology, and drug discovery. We anticipate that GOReverseLookup will significantly aid in identifying potential gene targets during the initial research phases.}
}

@article{DANCKWARDTLILLIESTROM2025100906,
title = {Travelling through time in a process drama on plastic pollution – temporality in teaching about the complexity of wicked problems},
journal = {Learning, Culture and Social Interaction},
volume = {52},
pages = {100906},
year = {2025},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2025.100906},
url = {https://www.sciencedirect.com/science/article/pii/S221065612500025X},
author = {Kerstin Danckwardt-Lillieström and Maria Andrée and Carl-Johan Rundgren},
keywords = {Historying, Futuring, Process drama, Wicked problems, Chemistry education, Upper secondary school},
abstract = {The understanding of sustainability issues and preparedness to take action towards a sustainable future involves abilities to navigate between past, present, and future. This paper explores how the use of imaginary transitions in time – in the form of historying, and futuring in process drama – may afford student understanding of the wicked problem of plastics. The study draws on a design-based research study on process drama in upper-secondary school chemistry teaching which was conducted in collaboration with two teachers. During the process drama, the students and teachers travel in time to explore the uses of plastic; the motives and needs for using plastic as well as the consequences of plastic use in the form of plastic pollution today and in the future. The collected data consist of video- and audio recordings, which were analysed through qualitative content analysis that discerned how the students connected the temporalities, and which dimensions of the plastic problem were made visible in the temporal movements in the process drama. Our findings indicate that the temporal transitions made visible several dimensions of the plastic issue, and contributed to adding layers of complexity to the issue of plastics.}
}

@article{VADIATI2025100120,
title = {(En) coding care into digital urbanism: Vignettes of collective practices},
journal = {Digital Geography and Society},
pages = {100120},
year = {2025},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2025.100120},
url = {https://www.sciencedirect.com/science/article/pii/S2666378325000091},
author = {Niloufar Vadiati and Letizia Chiappini and Martin Bangratz},
keywords = {Care, Digital urbanism, Refusing, Commoning, Reappropriating},
abstract = {The tech-entrepreneurial model behind the computation of urban processes is (re) producing what has already been identified as a technocratic, solutionist, and commodifying model of urban planning. Within this model, not only is caring not a prerequisite of urban production, but decade(s)in of smartification and platformization practices shows diminishing the spaces, infrastructure, and socio-economic relations that were co-produced to generate care. Through the lens of feminist geography, care is examined as a multidimensional concept encompassing socio-spatial dynamics, power relations, and ethical urban practices. Using empirical data from three research projects, the study showcases alternative digital urbanism practices, categorized into three vignettes: refusal, commoning, and reappropriation. These categories are illustrated with cases such as grassroots food cooperatives, feminist hack-spaces, digital sovereignty initiatives, platform-based welfare experiments and civil society initiatives such as Code for Germany. By situating care within the spatial and social fabric of urban life, the paper argues for its potential as a politic, practice, and epistemology that challenges the exploitative logic of contemporary digital infrastructures. The findings reveal the embeddedness of care practices within local contexts, highlighting the dual need for trans-local networks and territorial embeddedness. This study contributes to the discourse on caring digital urbanism, advancing a feminist theorisation of everyday digital urbanism.}
}

@article{LIANG2025,
title = {Progress and trends in liquid hydrogen release},
journal = {International Journal of Hydrogen Energy},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925016362},
author = {Yanwei Liang and Ming He and Yongfeng Qu and Nan Peng and Jean-Michel Ghidaglia and Liqiang Liu},
keywords = {Liquid hydrogen, Bibliometric, Review, Release behavior, Experiment, Numerical model},
abstract = {Liquid hydrogen (LH2) is a promising method for large-scale, long-distance storage and transportation of green energy. However, LH2 leaks pose significant safety risks, such as fire and explosion, due to the rapid formation of gaseous hydrogen. Therefore, understanding the release behavior of LH2 is crucial for ensuring energy security and advancing hydrogen energy applications. This paper reviews the evolution of research on LH2 release behavior, highlighting key experimental and modeling studies. By combining bibliometric methods, the paper presents progress and trends from the micro level (specific research activities) to the macro level (statistical literature information). The Web of Science Core Collection was searched using keywords such as "liquid hydrogen" + "leakage," resulting in 571 relevant articles. After the manual screening, 349 valid articles were retained. The paper identifies a surge in research activity in recent years, particularly in countries such as China, the United States, and Japan. Despite this progress, there remains a lack of systematic studies and open-access data in certain areas. The paper also outlines advancements and trends in experimentation and modeling, which are crucial for the safe and effective use of LH2 in energy systems.}
}

@article{KAZI2025100756,
title = {Bridging the gap: A survey of document retrieval techniques for high-resource and low-resource languages},
journal = {Computer Science Review},
volume = {57},
pages = {100756},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100756},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000322},
author = {Samreen Kazi and Shakeel Khoja and Ali Daud},
keywords = {Document retrieval, Low-resource language, Neural ranking, Large language models, Cross-lingual document retrieval},
abstract = {With the increasing need for efficient document retrieval in low-resource languages (LRLs), traditional retrieval methods struggle to overcome linguistic challenges such as data scarcity, morphological complexity, and orthographic variations. To address this, hybrid and neural ranking approaches have been explored, integrating statistical retrieval with transformer-based models to enhance search accuracy. Unlike high-resource languages, LRL retrieval requires specialized strategies, including cross-lingual retrieval, domain adaptation, and culturally aware search mechanisms. This article provides a comprehensive review of document retrieval in LRLs, covering classical models, deep learning-based techniques, and their adaptation to resource-constrained languages. A structured taxonomy is introduced, classifying retrieval methods based on model architectures, linguistic processing, and ranking strategies.The paper concludes by highlighting key challenges, benchmarking efforts, and future directions for improving retrieval effectiveness in LRLs.}
}

@article{MATOS2025100571,
title = {A systematic review of artificial intelligence applications in education: Emerging trends and challenges},
journal = {Decision Analytics Journal},
pages = {100571},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100571},
url = {https://www.sciencedirect.com/science/article/pii/S277266222500027X},
author = {Tomás Matos and Walter Santos and Eftim Zdravevski and Paulo Jorge Coelho and Ivan Miguel Pires and Filipe Madeira},
keywords = {Artificial intelligence, ChatGPT, Educational technology, Machine learning, Adaptive learning, Systematic review},
abstract = {The academic world is becoming increasingly interested in the applications of Artificial Intelligence technology in education. A systematic review examines AI applications in education, focusing on their effectiveness, challenges, and implications. A comprehensive analysis of studies published between 2011 and 2024 encompassed 45 research articles from major databases, such as PubMed Central, IEEE Xplore, Elsevier, Springer, MDPI, ACM, and PMC. The findings highlight the predominant use of generative AI tools like ChatGPT (30%), followed by other advanced technologies, such as GPT-4, machine learning, and virtual reality. Research across global regions, particularly in Canada (18%), the United States (12%), and China (8%), highlights the multifaceted applications of AI in enhancing personalized learning, fostering critical thinking, and supporting professional education. Tools such as ChatGPT have demonstrated strong performance in theoretical knowledge delivery and medical education, while augmented and virtual reality excels in practical skill development. Despite these advances, challenges such as data privacy concerns, algorithmic bias, and the need for specialized educator training remain critical.}
}

@article{BEDOGNI2025107855,
title = {Fluid Computing & Digital Twins for intelligent interoperability in the IoT ecosystem},
journal = {Future Generation Computer Systems},
pages = {107855},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2025.107855},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X25001505},
author = {Luca Bedogni and Marco Mamei and Marco Picone and Marcello Pietri and Franco Zambonelli},
keywords = {Digital twins, Intelligence, Fluid Computing, Interoperability},
abstract = {The integration of physical and digital systems is fundamental to enabling intelligent, adaptive, and scalable solutions in modern IoT environments. This paper explores Fluid Digital Twins (FDTs), a novel framework combining Fluid Computing (FC) principles with Digital Twin (DT) technology, to address challenges related to interoperability, dynamic functionality, and adaptability in IoT ecosystems. FC introduces a paradigm shift, enabling seamless data and computational task flow across heterogeneous environments, dynamically adjusting to resource availability and system needs. This paper focuses on embedding intelligence within FDTs to enhance interoperability and enable IoT applications to adapt to changes across both physical and digital domains. By integrating intelligent interoperability mechanisms, FDTs ensure smooth data alignment and compatibility across platforms, adapting to both physical and digital changes. The proposed framework has been implemented, prototyped, and evaluated in the Modena Automotive Smart Area (MASA), a smart city testbed. The evaluation demonstrates FDTs’ ability to enhance smart mobility, optimize transportation systems, and provide actionable insights, highlighting their transformative potential in dynamic, data-rich environments. The results emphasize the practical applicability of FDTs in addressing real-world challenges and advancing the capabilities of IoT-driven smart cities.}
}

@article{DEVI2025100985,
title = {Electric motor modeling, analysis, and design for E-mobility applications: A state of the art},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {12},
pages = {100985},
year = {2025},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2025.100985},
url = {https://www.sciencedirect.com/science/article/pii/S2772671125000920},
author = {Lourembam Ranjita Devi and Sreenu Sreekumar and Rohit Bhakar and Dileep G. and Sanjeevikumar Padmanaban},
keywords = {E-mobility, Electric motor, Hybrid motor, PMSynRM, Motor analysis, Motor designing, Motor modeling},
abstract = {The transportation sector has steadily shifted towards Electro-Mobility (E-Mobility) to achieve net-zero carbon emissions. One of the most important components of E-mobility is the electric motor, which must be designed for high torque, a wide speed range, and high efficiency at all speeds. Hence, significant research has been conducted on modeling, analyzing, and designing various electric motors like Brushless DC Motors (BLDCM), Induction Motors (IM), Permanent Magnet Synchronous Motors (PMSM), Switched Reluctance Motors (SRM), and Synchronous Reluctance Motors (SynRM) and Permanent Magnet Assisted Synchronous Reluctance Motors (PMaSynRM) for E-mobility applications. These motors face several challenges in modeling, analyzing, and designing, such as handling nonlinearity, accurate thermal modeling, and selecting suitable tools/materials. Also, new motors like Permanent Magnet Synchronous Reluctance Motors (PMSynRM) have been introduced by different EV manufacturers. Hardly any research or reviews are available for dynamic modeling, performance analysis, and design optimization of PMSynRM. Accurate dynamic modeling, performance analysis, and design optimization can support industry and academia in developing improved motor modeling and designs. There is little attention paid to research reviews in the above areas. Therefore, this paper provides a detailed review of the modeling, analysis, and design of various motors, including new motors used in E-mobility applications. Furthermore, this review investigates the research challenges and future scopes in modeling, analyzing, and designing. Also, various motors are compared in terms of speed, torque, torque ripple, efficiency, weight, and cost. The review concludes that identified research challenges should be immediately addressed to achieve targeted net-zero goals.}
}

@article{QI2025113567,
title = {Stochastic fractal equilibrium optimizer with X-shaped dynamic transfer function for solving large-scale feature selection problems},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113567},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113567},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125006136},
author = {Yu-Liang Qi and Yu-Wei Song and Jie-Sheng Wang and Yu-Cai Wang and Shi Li and Si-Yu Jin and Zi-Rui Xu},
keywords = {Feature selection, Equilibrium optimizer, Stochastic fractal search, X-shaped transfer function, Dynamic time-varying},
abstract = {Large-scale feature selection (FS) is an important task in the field of data extraction and machine learning. Its core goal is to identify and screen out the feature subset that is most critical to the prediction target from the extensive initial attribute collection, in a bid to improve the efficiency and generalization of the model and reduce the computational complexity. A stochastic fractal equilibrium optimizer based on X-shaped dynamic transfer function is targeted for large-scale feature selection. Among them, the X-shaped transfer function is designed based on the scaling and flip changes of the basic transfer functions, and then the parameters related to the number of iterations in the equilibrium optimizer (EO) are further used as time-varying factors to dynamically change the X-shaped transfer function. The flower-shaped transfer function and dynamic flower-shaped transfer function are extended. At the same time, EO is optimized and improved by using the diffusion and updating process in the stochastic fractal search. To assess the efficacy and dominance of the designed FS method, 20 large-scale datasets were picked from UCI datasets for experiments, and SFEO-TF with the best performance was selected and set against 8 other intelligent optimization approaches. The experimental results show that the designed dynamic time-varying X-shaped transfer function is effective as well as the EO based on stochastic fractal search. Among them, in the two groups of controlled experiments, SFEO-TF has the best performance in performance metric and categorization precision, and even the performance of SFEO-TF set against 8 other intelligent optimization approaches in the minimum fitness value of all 20 large-scale datasets has reached the minimum value.}
}

@article{WANG2025102545,
title = {Research on the three-body kelp harvesting ship based on floating raft aquaculture mode},
journal = {Aquacultural Engineering},
volume = {110},
pages = {102545},
year = {2025},
issn = {0144-8609},
doi = {https://doi.org/10.1016/j.aquaeng.2025.102545},
url = {https://www.sciencedirect.com/science/article/pii/S0144860925000342},
author = {Xian Wang and Yanan Wang and Zhengzhong Li and Hailong Che and Lanlan Zhu and Tongfei Sheng and Hua Zhou and Duanyang Geng},
keywords = {Kelp, Floating raft aquaculture mode, Mechanized harvesting, Harvesting ship},
abstract = {Floating raft is the most widely promoted kelp aquaculture mode in China. Under this mode, kelp production is high, but the ropes are complex and intertwined. The main method of kelp harvesting is manual. To reduce the labor intensity and improve the harvesting efficiency of kelp, this study designs a three-body kelp harvesting ship based on the floating raft aquaculture mode. This ship can lift, transport, collect, and transfer kelp in a single operation. Apart from the manual assistance required for the release and hanging the seedling rope, all other processes are mechanically automated. The diameter and lead of the rotary-screw propulsor blade were determined through Fluent simulation tests. To verify the actual harvesting effect of the three-body kelp harvesting ship, kelp harvesting performance tests were conducted. The results showed that the working speed and efficiency of mechanized harvesting were significantly higher than manual. The mechanical harvesting efficiency was 18.75 times that of manual, and the per capita harvesting efficiency was 9.33 times that of manual. The kelp loss rate of the harvesting ship was 3.25 %, slightly higher than that of manual. This study can provide a certain reference for the mechanized harvesting of kelp in large-scale aquaculture.}
}

@article{SUO2025122195,
title = {A 3 × 3 approach to three-way conflict analysis in incomplete Pythagorean fuzzy situation tables},
journal = {Information Sciences},
volume = {714},
pages = {122195},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122195},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525003275},
author = {Langwangqing Suo and Hai-Long Yang and Zhi-Lian Guo},
keywords = {Three-way conflict analysis, Three-way decision, Incomplete Pythagorean fuzzy situation table, 3 × 3 approach, Three-way supplement method},
abstract = {Current studies of three-way conflict analysis primarily concentrate on complete situation tables, with limited consideration for incomplete situation tables. This paper focuses on three-way conflict analysis models based on an incomplete Pythagorean fuzzy situation table (IPFST). Following the principles of three-way decision, we propose a 3×3 approach for three-way conflict analysis grounded in the IPFST. The first 3 indicates the support-opposition-neutral triad, which deals with missing values in the IPFST from three different perspectives and is known as a three-way supplement method. We use this method to convert the IPFST into three complete Pythagorean fuzzy situation tables (CPFSTs), which are called the CPFSTs induced by the three-way supplement method. The second 3 represents the agent pairs-agents-issues triad, which is the basic task of trisecting agent pairs, agents, and issues for each part of the first 3. More specifically, we categorize agent pairs into conflict, neutrality, and alliance relations, classify agents into opposition, neutral, and support coalitions, and divide issues into main, secondary, and irrelevant reasons. In addition, we discuss maximum coalitions and feasible strategies for conflict resolution. Finally, we illustrate the usefulness and effectiveness of the 3×3 approach through a case study.}
}

@article{LIU2025101845,
title = {The Mad-genius Controversy: Estimating the Creativity of Suicide Poets Via a Dual Model},
journal = {Thinking Skills and Creativity},
pages = {101845},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101845},
url = {https://www.sciencedirect.com/science/article/pii/S187118712500094X},
author = {Yuqing Liu and Ameersing Luximon and Yenan Yang and Xiaoyu Li and Yao Song},
keywords = {creativity, suicide, computational linguistics, Chinese poetry},
abstract = {The "mad genius" controversy concerns the intricate relationship between creativity and psychopathology. Poets, for instance, are often noted for their mental health challenges and elevated suicide rates. This study investigates the link between creativity and suicide among modern and contemporary Chinese poets by employing computational techniques to analyze semantic creativity in their works. Examining 16 poets who died by suicide alongside 21 non-suicidal counterparts, we introduced a dual model that combines flow distance and co-occurrence networks to assess creative cognition. The findings indicate that suicidal poets exhibit significantly higher local and global flow distances, reflecting greater divergent thinking. Furthermore, their co-occurrence networks display more tightly interconnected and efficient structures, suggesting enhanced cognitive flexibility. By demonstrating that heightened creativity, characterized by distinct semantic network properties, is associated with mental health challenges, the study provides empirical support for the "mad genius" hypothesis. These results contribute to the understanding of the creativity-psychopathology nexus, offering novel insights and advancing computational methods for analyzing creative expression.}
}

@article{XU2025100685,
title = {Robotic roles in education: A systematic review based on a proposed framework of learner-robot relationships},
journal = {Educational Research Review},
pages = {100685},
year = {2025},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2025.100685},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X25000223},
author = {Weiqi Xu and Fan Ouyang},
keywords = {Robotics education, Robotic role, Learner, Educational system, Systematic review},
abstract = {With the development of computer and information techniques, robotics education provides potential to reshape the instructional and learning process in the educational system. From the perspective of the educational system, it is essential to understand the roles of robotics in education as well as their relationships with learners. Specifically, this systematic review proposed a conceptual framework based on two dimensions (i.e., learner agency, robotic interactivity) and reviewed 92 empirical studies from 2010 to 2023 to clarify four categories of robotic roles, namely learning about robotics, learning through robotics, learning from robotics, and learning with robotics. In learning about robotics, educational robotics are merely the learning objects or contents by learners. In learning through robotics, educational robotics serve as the learning tools or assists, and learners can take their agency to operate or design robotics. In learning from robotics, educational robotics act as tutors to convey and deliver knowledge and information to learners. In learning with robotics, educational robotics serve as the peers or companions of learners and collaborate with learners to provide emotional supports, solve problems and construct knowledge. Furthermore, this study examined the applied educational contexts (i.e., educational domain, educational level) and learning effects of the four robotic roles. Overall, the future development of robotics education highlights both learner agency and robotic interactivity to achieve the goal of a learner-centered, robot-friendly learning.}
}

@article{HARIKUMARAN2025113550,
title = {Optimizing Fertilizer Recommendations in Precision Agriculture: A Novel Defuzzification Approach with Adaptive Intelligent Optimization},
journal = {Knowledge-Based Systems},
pages = {113550},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113550},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005969},
author = {M. Harikumaran and Dr. P. Vijayalakshmi},
keywords = {Fuzzy inference systems, Optimal fertilizer recommendation, Adaptive intelligent fertilizer optimization system, Hybrid Somersault Panda search optimization},
abstract = {This paper addresses the critical need for advancing and innovating defuzzification methods in fuzzy inference systems (FISs) for optimal fertilizer recommendation in precision agriculture. Precision agriculture, aimed at sustainable and efficient food production, relies heavily on accurate fertilizer application. Traditional methods often result in over- or under-fertilization, leading to economic losses and environmental pollution. Fuzzy inference systems have shown promise in decision-making for precision agriculture, but the accuracy of fertilizer recommendations hinges on the defuzzification method. This paper introduces a novel framework, the adaptive intelligent fertilizer optimization system (AIFOS), comprising three phases. The first phase enhances the FIS through adaptive membership function design, specifically employing Gaussian membership functions. The second phase introduces data-driven defuzzification innovation, in which parameters are optimized through a hybrid optimization algorithm called hybrid somersault panda search optimization (HSPSO), which combines red panda optimization (RPO) and manta ray foraging optimization (MRFO). A multi-objective fitness function is explored to address yield maximization, cost optimization, and environmental impact. The third phase focuses on adaptive knowledge update and feedback, integrating reinforcement learning techniques for real-time learning and adaptation. The proposed framework aims to significantly improve the accuracy, robustness, and sustainability of fertilizer recommendations in precision agriculture.}
}

@article{MARCELLA2025116145,
title = {First shell EXAFS data analysis of nano catalysts via neural networks},
journal = {Journal of Catalysis},
pages = {116145},
year = {2025},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2025.116145},
url = {https://www.sciencedirect.com/science/article/pii/S0021951725002106},
author = {Nicholas Marcella and Ryuichi Shimogawa and Yongchun Xiang and Anatoly I. Frenkel},
abstract = {Understanding the mechanisms of work of nanoparticle catalysts requires the knowledge of their structural and electronic descriptors, often measured in operando X-ray absorption fine structure (XAFS) spectroscopy experiments. We introduce a neural-network-based framework for rapidly mapping extended XAFS (EXAFS) spectra onto structural parameters as an alternative to the commonly used non-linear least-squares fitting approaches. Our method leverages multilayer perceptron trained on theoretical EXAFS and validated against theoretical test data and experimental spectra of frequently used nanoparticle types. The network helps lower the correlation between parameters, achieves high accuracy in the presence of noise and glitches, and can provide real-time parameter predictions with minimal user intervention. Parameter uncertainties are estimated as well. This method can be readily integrated into beamline pipelines or laboratory data analysis workflow and has the potential to accelerate high-throughput and catalyst characterization and testing.}
}

@article{LYU2025102129,
title = {AI-powered personalized learning: Enhancing self-efficacy, motivation, and digital literacy in adult education through expectancy-value theory},
journal = {Learning and Motivation},
volume = {90},
pages = {102129},
year = {2025},
issn = {0023-9690},
doi = {https://doi.org/10.1016/j.lmot.2025.102129},
url = {https://www.sciencedirect.com/science/article/pii/S0023969025000360},
author = {Wenwen Lyu and Zarina Abdul Salam},
keywords = {AI-powered personalized learning, Digital literacy, Expectancy-value theory, Motivation, Self-efficacy, Adult EFL learners},
abstract = {Although the implementation of artificial intelligence (AI) in educational contexts has gained increasing prominence, empirical research specifically examining its influence on crucial learner-related variables (e.g., self-efficacy, motivation, and digital literacy) among adult male learners of English as a Foreign Language (EFL) in China remains limited. The present study addresses this gap by investigating the effects of AI-powered personalized learning interventions on these key constructs. A total of 183 intermediate-level Chinese male EFL learners were randomly assigned either to an experimental group (EG), which received AI-personalized instruction, or to a control group (CG), which engaged in traditional instruction methods. Data were gathered through pre- and post-intervention surveys and analyzed using independent t-tests. Results indicated that compared to participants in the CG, learners in the EG exhibited statistically significant improvements in self-efficacy, motivation, and digital literacy. These findings offer robust empirical evidence supporting the effectiveness of AI-personalized instructional strategies in enhancing essential learner attributes within the adult male EFL context in China. Thus, the study advocates for the strategic integration of AI-powered personalized learning, highlighting its considerable potential to optimize language learning outcomes within adult EFL education.}
}

@article{DUAN2025127718,
title = {LSBT-Net: A lightweight framework for fault diagnosis of bearings based on an interpretable spatial-temporal model},
journal = {Expert Systems with Applications},
volume = {281},
pages = {127718},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127718},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425013405},
author = {Yicheng Duan and Tongguang Yang and Chenlin Wang and Yongjian Zhang and Qingkai Han and Shuangping Guo},
keywords = {Intelligent Diagnosis, Insulated Bearings, LSBT-Net Framework, Interpretability},
abstract = {Intelligent fault diagnosis based on deep learning has emerged as a research focus in mechanical equipment due to its adaptive feature extraction capability. However, current models struggle with low accuracy, high computational costs, and poor interpretability when detecting faults in insulated bearings. To address these challenges, this paper proposes a novel lightweight spatiotemporal model-based intelligent diagnostic framework, named LSBT-Net, which aims to identify motor insulating bearing faults in practical engineering applications more accurately. Specifically, this research breaks the conventional thinking of “learning fault data feature information” by innovatively developing a spatiotemporal information fusion module. This module is cleverly integrated into the LSBT-Net framework, enabling the extraction of both local and global high-dimensional fault feature information from insulating bearings. At the same time, based on a lightweight design, it significantly reduces the total number of parameters and computational resources required by the framework, thus lowering its computational complexity. The t-SNE algorithm is introduced into the LSBT-Net framework to achieve local or global interpretability. Furthermore, by calculating the gradient information of the LSBT-Net framework on the fault types of insulating bearings through backpropagation, the interpretability of the framework with respect to the physical information is enhanced. Using insulating bearings and typical fault experiments as examples, the LSBT-Net framework demonstrates excellent diagnostic capability and generalization performance compared to other advanced methods.}
}

@article{ESMAEILNEJADAHRANJANI2025108365,
title = {Detoxification techniques for bacterial toxins: A pathway to effective toxoid vaccines},
journal = {Toxicon},
volume = {260},
pages = {108365},
year = {2025},
issn = {0041-0101},
doi = {https://doi.org/10.1016/j.toxicon.2025.108365},
url = {https://www.sciencedirect.com/science/article/pii/S0041010125001394},
author = {Parvaneh Esmaeilnejad-Ahranjani and Youcef Shahali and Maryam Dadar},
keywords = {Toxoid vaccine, Irreversible toxoid, Detoxification techniques, Bacterial toxins, Chemical reagent, Protein engineering},
abstract = {Bacterial toxins play a critical role in the virulence of many pathogens, leading to serious diseases such as tetanus, diphtheria, botulism, and entrotoxemia. As key virulence factors, these toxins cause significant tissue damage and disease manifestations in infected hosts. Vaccination against these toxins through toxoid vaccines, composed of inactivated forms of the toxins, represents a vital strategy for preventing toxin-mediated diseases. However, creating effective toxoid vaccines necessitates meticulous detoxification processes that ensure the loss of toxicity while retaining the immunogenic properties inherent in the native toxins. This review offers a comprehensive evaluation of the diverse methodologies employed for detoxifying bacterial toxins, highlighting their advantages, limitations, and implications for vaccine development. By detailing comparisons of efficacy, stability, residual toxicity, and clinical applicability, we demonstrate that while traditional methods utilizing chemical reagents (such as formaldehyde) remain widely used, emerging technologies like genetic inactivation and protein engineering present significant advantages. These innovations promise to advance the development of durable and irreversible toxoid vaccines that protect public health and contribute to future vaccine formulation improvements. Ultimately, this knowledge synthesis aims to guide future research efforts and facilitate the design of safer and more effective toxoid vaccines to combat the public health threats posed by toxin-producing bacteria.}
}

@article{SIWACH2025100125,
title = {Exploring IoT integration challenges: Causal relationships and strategic implications for business models},
journal = {Digital Business},
volume = {5},
number = {2},
pages = {100125},
year = {2025},
issn = {2666-9544},
doi = {https://doi.org/10.1016/j.digbus.2025.100125},
url = {https://www.sciencedirect.com/science/article/pii/S2666954425000201},
author = {Parveen Siwach and Disha Gulia and Dhiraj Kumar Yadav and Mohit Malik and Vijay Kumar Gahlawat},
keywords = {IoT integration challenges, Fuzzy DEMATEL, Business model innovation, Influencing factors, Digital transformations, Internet of things (IoT)},
abstract = {Implementing the Internet of Things (IoT) into business models has become an influential catalyst, supporting novel revenue generation, improved operational efficiencies, and competitive dominance. Still, its implementation has been challenged by barriers to technological, organizational, and societal dimensions. The current study utilizes the Fuzzy Decision-Making Trial and Evaluation Laboratory (Fuzzy DEMATEL) technique to comprehensively identify and assess the interconnections among fundamental IoT integration challenges. Thirteen crucial components were analysed, influencing interoperability challenges, data security and privacy issues, standardized issues, high initial investment, scaling obstacles, and adverse environmental effects. The results indicate a clear differentiation between seven cause variables such as lack of skilled workforce, interoperability issues, standardization gaps, high initial investment, regulatory issues, resistance to change, and data security and privacy issues were the most influential challenges and six effects (dependent) factors including vendor dependency risks were the most impacted effect factors. These findings highlight the necessity to tackle fundamental problems, including standardization, workforce development, and regulatory clarity. This research strengthens theoretical aspects in IoT using Fuzzy DEMATEL by delivering a robust decision-making roadmap. The results highlights actionable practical recommendations for regulators and businesses for effective resource utilization to improve their IoT adoption in compliance with sustainable development. Businesses can utilize the full potential of IoT by addressing such problems and promote innovative environment to ensure a sustainable and resilient future.}
}

@article{CHAUDHARI2025106924,
title = {From Nature to Nanotech: Unlocking Berberine’s Therapeutic Approaches},
journal = {Journal of Drug Delivery Science and Technology},
pages = {106924},
year = {2025},
issn = {1773-2247},
doi = {https://doi.org/10.1016/j.jddst.2025.106924},
url = {https://www.sciencedirect.com/science/article/pii/S1773224725003272},
author = {Shubham Chaudhari and Manoj Dalabehera and Rudra Narayan Subudhi and Kamal Dua and Malkiet Kaur and Keshav Raj Paudel and Jatin Kumar},
keywords = {Berberine (BBR), Mechanism, Nanomaterials, Nanocarriers, Synergistic Potential},
abstract = {Despite being widely used, the therapeutic outcomes of Berberine (BBR)have remained suboptimal due to its poor aqueous solubility, absorption, rapid metabolism, and limited targeting. This results in pitiable bioavailability, which in turn demands frequent administration. The employment of a nanomaterials-based framework embedded with BBR could lessen the deprived pharmacokinetics and amplify the curative intervention. This manuscript delves into various nanomaterials and methods-based systems designed to address these drawbacks. Nanotechnology-based delivery systems, including magnetic, gold, selenium, and liposomal nanoparticles, as well as polymeric and mesoporous silica carriers, have demonstrated significant potential to overcome Berberine’s pharmacokinetic limitations. These systems enhance bioavailability, provide targeted delivery, and offer sustained release, showing promising results in preclinical and clinical models for cancer, metabolic, and neurodegenerative disorders. The manuscript also frameworks an in-depth analysis of BBR's underlying pharmacological mechanism, various synergistic studies, clinical trials, and associated patent data. By employing nano-BBR with surface modification, we could sidestep the constraints of traditional forms by attenuating regrettable attributes of BBR. Implementation of combinatorial chemistry, in addition to the in-silico approach, might refine the clinical outcomes of BBR. However, steering into regulatory complexity is important, which may affect its market success because of its physiological drawbacks.}
}

@article{CHAYKINA2025237014,
title = {Critical outlook on separator layers for solid-state lithium batteries: Solid electrolyte materials, anode interface engineering, & scalable separator production},
journal = {Journal of Power Sources},
volume = {643},
pages = {237014},
year = {2025},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2025.237014},
url = {https://www.sciencedirect.com/science/article/pii/S037877532500850X},
author = {Diana Chaykina and Meena Ghosh and Ömer Ulaş Kudu},
keywords = {Lithium-ion batteries, Solid state electrolytes, Argyrodites, Thin films, Interface engineering},
abstract = {Energy storage as batteries is important for the electrification of several different technologies including transportation. For such applications, it is also clear that the state-of-the-art lithium-ion batteries need to be improved, especially in terms of energy density, safety, and other aspects. Solid-state batteries stand out as promising candidates to fill this gap, utilizing a solid-state electrolyte separator instead of flammable liquid electrolytes soaked in polymeric membranes. This review provides a comprehensive overview of several types of solid-state electrolytes (oxides, sulphides, halides, polymer, composite), with a special focus on sulphide argyrodite solid electrolytes as promising separators which offer the best balance of performance and processability. Furthermore, we discuss the stability issues of argyrodites with next generation anodes such as Li and Si, suggesting that interface engineering strategies by thin film methods are a scalable and effective way to mitigate the stability issues at the interface. Additionally, we give an overview of the process to make solid-state electrolyte membranes which involves several steps. Although many tools and approaches are available for the fabrication of these membranes, dry processing technology is identified as an important component to the successful realization of solid-state batteries.}
}

@article{MICHAEL2025102043,
title = {Intrinsically motivated norm compliance and the sense of obligation},
journal = {Current Opinion in Psychology},
pages = {102043},
year = {2025},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2025.102043},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X25000569},
author = {John Michael and Luca Tummolini},
keywords = {norms, sense of obligation, intrinsic motivation, psychological needs, norm change},
abstract = {What is the motivational force of the sense of obligation that drives us to intrinsically comply with social norms even in the absence of external incentives? To integrate recent theoretical and empirical research aiming to illuminate the motivational power of psychological obligations, we combine the theory of basic psychological needs with recent work in intrinsically motivated reinforcement learning. This enables us to provide a fresh perspective upon the relationship among existing accounts of normative motivation, and to identify key questions to guide and structure future research.}
}

@article{PELLETIER2025107936,
title = {A Theoretical Study of the Bonding Properties of R4Sb3 Compounds},
journal = {Solid State Sciences},
pages = {107936},
year = {2025},
issn = {1293-2558},
doi = {https://doi.org/10.1016/j.solidstatesciences.2025.107936},
url = {https://www.sciencedirect.com/science/article/pii/S1293255825001141},
author = {Vincent Pelletier and Hugo Bouteiller and Bruno Fontaine and David Berthebaud and Jean-Claude Crivello and Franck Gascoin and Takao Mori and Jean-François Halet and Régis Gautier},
keywords = {Rare-earth antimonides, electronic structure, thermoelectrics, DFT},
abstract = {This study investigates the electronic structure and bonding properties of rare-earth antimonide compounds, specifically Yb4Sb3 and La4Sb3, utilizing density functional theory calculations. The analysis reveals that Yb4Sb3 exhibits a predominantly ionic character whereas La4Sb3 displays a greater degree of covalent bonding. Moreover, the presence of divalent ytterbium leads to p-type conduction at high temperatures in Yb4Sb3. Conversely, La4Sb3 displays n-type conduction because of a larger electronic transfer from the rare-earth metal towards antimony. These findings provide valuable insights into the structural and electronic properties that govern the performance of R4Sb3 compounds, contributing to the development of advanced materials for thermoelectric energy conversion.}
}

@article{SHAH2025104940,
title = {A Deep Learning based Multiple RNA Methylation Sites Prediction Across Species},
journal = {Results in Engineering},
pages = {104940},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104940},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025010163},
author = {Sajid Shah and Saima Jabeen and Mohammed ElAffendi and Ishrat Khan and Muhammad Almas Anjum and Mohamed A. Bahloul},
keywords = {RNA methylation, RNA modification prediction, m1A, m6A, m5C, A to I, RNA, CNN, Deep Learning, Sequence Analysis, Transformer},
abstract = {Methylation of ribonucleic acid (RNA) is an essential post-transcriptional alteration that has a major effect on many biological processes. Identifying RNA methylation sites is essential for understanding gene regulation and potential therapeutic targets. The contribution of this study is multi-folded. Firstly, this study introduces two novel deep learning models for predicting RNA methylation sites: Convolutional Neural Network (CNN)-based and transformer-based models. These models are trained and evaluated on human and mouse benchmark datasets for m1A, m6A, m5C, and A to I, methylation types. Secondly, this work investigates the effect of different encoding techniques on model performance, including one-hot encoding, Gene2Vec, and position encoding, as well as their combinations using concatenation, summation, and multiplication. Thirdly, this study also aims to investigate the prediction strength of motif-based and attention-based classifiers. The obtained results demonstrate that both models achieve high accuracy in predicting RNA methylation sites, outperforming existing state-of-the-art approaches in terms of multiple performance metrics. Moreover, the selection of encoding strategy has a substantial impact on prediction accuracy; the best approaches vary based on the particular species and type of methylation. The findings also indicate that the motif-based classifier is more stable than the attention-based classification when predicting RNA methylation. In the future, we aim to expand our research beyond human and mouse models to explore RNA methylation in plants.}
}

@article{JANSSON2025100121,
title = {The coming of the post-digital workplace? A survey of how white-collar workers experience and cope with digital media reliance},
journal = {Digital Geography and Society},
pages = {100121},
year = {2025},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2025.100121},
url = {https://www.sciencedirect.com/science/article/pii/S2666378325000108},
author = {André Jansson and Karin Fast and Paul C. Adams},
keywords = {Digitalization, Office work, Territoriality, Communicative agency, Mediatization, Digital work, Technological dependency},
abstract = {The coming of the post-digital workplace? A survey of how white-collar workers experience and cope with digital media reliance. New media technology can both hamper and amplify workers' agency. Much research shows that the ambiguities of digital reliance are accentuated among office workers, especially knowledge workers, who spend most of their working time handling different types of information and data. Thus, in times of constant connectivity, people might feel compelled to create time-spaces for disconnection, or find spatial and temporal routines for restricting their use of digital tools. This article provides a quantitative analysis, based on a survey, of how private and public officials (“white-collar workers”) in Sweden experience and handle digital media reliance at work, with a special focus on whether they think communicative and territorial agency are enhanced or constrained under digitalized working conditions. Based on a principal component analysis (PCA), five dispositions toward (the handling of) digital media reliance are identified: the skepctical, the embracing, the captivated, the reluctant and the disciplined. These dispositions are further analyzed in relation to demographic and contextual variables, pointing especially to the significance of employment sector. While digital media reliance is appreciated and associated with extended agency by many informants, the study also reveals different facets of post-digital sentiments and tactics. These are particularly constitutive of the skeptical disposition, reflecting inclinations to avoid certain media and find alternatives to digital tools, but also in the disciplined disposition which encompasses internalized routines for media use. The study also shows that the normalization of digitalized work processes is entwined with, and necessitates, different forms of territorial micro-politics extending beyond the workplace per se.}
}

@article{LIN2025111350,
title = {Resting-state Functional Brain Networks in Hypertensive Retinopathy},
journal = {Brain Research Bulletin},
pages = {111350},
year = {2025},
issn = {0361-9230},
doi = {https://doi.org/10.1016/j.brainresbull.2025.111350},
url = {https://www.sciencedirect.com/science/article/pii/S0361923025001625},
author = {Si-Min Lin and Yi Han and Jin‑Yu Hu and Xiao-Yu Wang and Yan-Mei Zeng and Hong Wei and Yi Shao and Yao Yu},
keywords = {Hypertensive retinopathy, resting-state functional MRI, independent component analysis, graph theory analysis},
abstract = {Objective
Hypertensive retinopathy (HR) is known to have effects on the brain's function. This neuroimaging investigation aimed to evaluate alterations in functional network connectivity and the topological properties of brain networks in in patients with HR.
Methods
The study involved twenty patients with HR and forty-one healthy controls (HC), all of whom underwent resting-state functional MRI scans. Independent component analysis and graph theory analysis were calculated to identify functional connectivity and topological property abnormalities between the two groups.
Results
Compared to HC, patients with HR demonstrated increased internetwork functional connectivity. Furthermore, these patients showed increased intranetwork functional connectivity within the right precuneus of the default mode network. Graph theory analysis revealed that both groups demonstrated a small-world topology. However, significant differences were observed in global and regional network metrics in HR patients compared to HC.
Conclusion
These findings highlight the alterations in functional connectivity and topological properties of brain networks in patients with HR, offering valuable insights into the potential neural mechanisms underlying their condition.}
}

@article{LI2025136144,
title = {Enhancing CO2 Hydrate Sequestration through Underlying Methane Hydrate Production: A Novel Strategy for Carbon Storage},
journal = {Energy},
pages = {136144},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.136144},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225017864},
author = {Yuxuan Li and Zhaobin Zhang and Bo Zhang and Rick Chalaturnyk and Shouding Li and Jianming He and Zhuoran Xie and Hang Bian and Xiao Li and Cheng Lu and Xuwen Qin},
keywords = {CO sequestration, Methane hydrate production, Multi-physical, Sequestration efficiency, Storage safety},
abstract = {Seafloor CO2 sequestration in the form of gas hydrates offers expansive geological opportunities for carbon neutrality strategies. Guided by phase equilibrium conditions, this study employs a self-developed multi-physical simulator to investigate how methane hydrate production beneath a CO2 sequestration zone can facilitate carbon storage. A dual-horizontal-well model is constructed and validated based on field measurements. Through numerical simulations, the effects of methane hydrate production timing, CO2 injection rate, and injection depth on carbon sequestration over a 100,000-year timescale are examined. Two novel metrics—sequestration efficiency and safe distance—are introduced. Results indicate that extracting methane hydrates in the later stages of CO2 injection is optimal. Furthermore, an efficient sequestration window exists for both injection rate and depth. By exploring the relationship between sequestration efficiency and storage safety, this study provides a new perspective for implementing CO2 sequestration pathways.}
}

@article{AHANI2025103418,
title = {Exploring Technological Innovations in Employing Structural Health Monitoring for Glass},
journal = {NDT & E International},
pages = {103418},
year = {2025},
issn = {0963-8695},
doi = {https://doi.org/10.1016/j.ndteint.2025.103418},
url = {https://www.sciencedirect.com/science/article/pii/S0963869525000994},
author = {Elshan Ahani and Jian Yang and Sima Bahram Ghannad},
keywords = {Structural glass, structural health monitoring, image processing, artificial intelligence},
abstract = {The implementation of structural health monitoring (SHM) for glass structures is still at an early stage of development. Limited information can be found in existing literature regarding the use of SHM on glass elements. However, recent technological advancements have significantly improved our understanding in this area. Both SHM and the utilization of laminated glasses (LGs) for structural purposes are relatively new concepts with a short history. To conduct a comprehensive assessment of glass elements, it is crucial to have a solid understanding of both glass and SHM sciences. This research aims to explore various approaches for implementing SHM on structural glass elements, with a specific focus on the utilization of artificial intelligence (AI) and machine learning (ML) techniques. By harnessing the power of ML, the SHM system can analyze large amounts of data collected from sensors placed on glass structures. Furthermore, the application of AI in SHM can facilitate real-time monitoring, predictive maintenance, and risk assessment for glass structures. The findings emphasize the critical role of early detection in preventing damage to glass elements and demonstrate the effectiveness of SHM in achieving this objective. The use of AI features, such as ML, presents promising opportunities for advancing the capabilities of SHM in the structural glass domain. The incorporation of AI features, such as ML algorithms, could offer additional advantages in this field.}
}

@article{TOOBY2025106687,
title = {The evolution of war and its cognitive foundations},
journal = {Evolution and Human Behavior},
volume = {46},
number = {3},
pages = {106687},
year = {2025},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2025.106687},
url = {https://www.sciencedirect.com/science/article/pii/S1090513825000364},
author = {John Tooby and Leda Cosmides},
keywords = {War, Coalitional aggression, Cooperation, Cognition, Conflict},
abstract = {Coalitional aggression evolved because it allowed the participants to promote their fitness by gaining access to disputed, reproduction-enhancing resources that would otherwise be denied to them. Few species engage in coalitional aggression, even though the social conditions that would favor its evolution seem to be widespread. Why? Forming coalitions to exploit these opportunities requires individuals to solve highly complex and specialized information processing problems involving cooperation, coordination, and social exchange. The difficulty of evolving cognitive mechanisms capable of solving these problems—especially when the individuals involved are not kin—may explain why multi-individual coalitions are phylogenetically rare. We propose that humans and a few other cognitively pre-adapted species have evolved specialized cognitive programs that govern coalitional behavior, which constitute a distinctive coalitional psychology. To derive a preliminary map of this psychology, we started with a task analysis of the adaptive information-processing problems that arise during coalitional aggression. This exercise can shine light on our evolved psychology because algorithms that motivate and organize coalitional aggression would need design features that solve these problems well to be favored by selection. These problems include decisions about when to form a coalition or join one, when to initiate an attack, and how to allocate the costs and benefits that result from coalitional action. The risk contract of war identifies circumstances under which natural selection would favor decisions to initiate an attack. When the conditions of this model are met, mortality rates will not negatively impact the fitness of males in the winning coalition. This outcome has implications for the design of computational systems that motivate coalitional attacks; it may explain why warfare is so favored an activity among men, despite its risks to the participating individuals' welfare.}
}

@article{OLADIGBOLU2025101018,
title = {EV charging stations for sustainable urban transport electrification in the Arabian Peninsula: Performance assessment, social-economic aspects, opportunities, implementation challenges and strategic policies},
journal = {Energy Conversion and Management: X},
pages = {101018},
year = {2025},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2025.101018},
url = {https://www.sciencedirect.com/science/article/pii/S2590174525001503},
author = {Jamiu O. Oladigbolu and Asad Mujeeb and Mohd Bilal and Yusuf A. Al-Turki and Saleh S. Alharbi and Salah S. Alharbi},
keywords = {Renewable energy source, Hybrid grey wolf cuckoo search algorithm, Electric vehicle, Strategic policies, Battery storage, Arabian Peninsula},
abstract = {The rapid adoption of electric vehicles, renewable energy sources, and advancements in battery technologies necessitate efficient, sustainable, and socio-economically viable interdependent energy systems. The rising penetration of EVs intensifies interdependencies between urban transport and electric power systems through charging infrastructures. This study presents a novel hybrid energy system that integrates renewable energy sources such as solar photovoltaic and wind turbines, conventional diesel generators, battery storage, and distribution grids to reliably satisfy the charging demands of electric vehicles in urban environments. A Hybrid Grey Wolf Cuckoo Search Algorithm (HGWCSA), which synergistically combines the exploitation capabilities of Grey Wolf Optimization (GWO) and the exploration strengths of Cuckoo Search (CS), is employed to optimize the system’s sizing and operational performance of the charging system. The primary advantages of HGWCSA include faster convergence, superior solution accuracy, and robust capability to avoid local optima. The effectiveness and innovation of HGWCSA are validated through comparative performance assessments against conventional metaheuristic algorithms. The study further contributes novel insights by thoroughly assessing techno-economic feasibility, socio-economic opportunities, strategic policies, and implementation challenges within the context of sustainable e-mobility from a case study perspective in the Arabian Peninsula. Comprehensive simulations and comparative analyses demonstrate that the HGWCSA achieves the lowest Levelized cost of energy (LCOE) at $0.08/kWh and the minimum Total net present cost (TNPC) of $103,267, significantly reducing CO2 emissions compared to traditional optimization methods. Furthermore, sensitivity analyses validate the optimized system’s adaptability and long-term viability across diverse operational scenarios, significantly advancing sustainable urban transport electrification strategies.}
}

@article{WEI2025104318,
title = {Probability evaluation of blade flutter in a transonic compressor with inlet distortion using SSA-DBEN model},
journal = {Journal of Fluids and Structures},
volume = {135},
pages = {104318},
year = {2025},
issn = {0889-9746},
doi = {https://doi.org/10.1016/j.jfluidstructs.2025.104318},
url = {https://www.sciencedirect.com/science/article/pii/S0889974625000532},
author = {Jingshan Wei and Zhidong Chi and Shimin Wang and Qun Zheng and Wei Yan and Bin Jiang},
keywords = {Axial compressor, Total pressure distortion, Deep belief network, Sparrow search algorithm, Flutter probability assessment},
abstract = {Inlet distortion significantly impacts the aeroelastic stability of aircraft engines, posing potential risks to their reliability and performance. Evaluating the probability of flutter in compressor blades is an effective approach to quantifying uncertain vibration characteristics and assessing blade aeroelastic stability. To improve modeling accuracy and computational efficiency in this analysis, a prediction method based on the sparrow search algorithm -deep extreme belief network (SSA-DEBN) model is proposed. The proposed method is evaluated through a case study involving the flutter probability assessment of a typical compressor rotor under inlet total pressure distortion. The results demonstrate that the aerodynamic modal damping ratio initially decreases and then increases as the wavelength of the inlet distortion decreases, reaching a minimum when the sinusoidal wave number of the distortion is two. Under inlet distortion conditions, the aerodynamic modal damping ratio of the compressor blade follows an approximate normal distribution, with a flutter reliability of 98.48 %. The primary factors influencing compressor blade flutter are rotational speed, inlet total temperature, vibration frequency, inlet total pressure, outlet static pressure, and distortion amplitude. The SSA-DEBN method has high accuracy and efficiency in the evaluation of compressor blade flutter failure mode by comparative analysis.}
}

@article{WU2025,
title = {Unleashing the power of text for credit default prediction: Comparing human-written and generative AI-refined texts},
journal = {European Journal of Operational Research},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2025.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0377221725003170},
author = {Zongxiao Wu and Yizhe Dong and Yaoyiran Li and Baofeng Shi},
keywords = {OR in banking, Generative AI, Large language model, Credit risk, Text mining},
abstract = {This study explores the integration of a representative large language model, ChatGPT, into lending decision-making with a focus on credit default prediction. Specifically, we use ChatGPT to analyse and interpret loan assessments written by loan officers and generate refined versions of these texts. Our comparative analysis reveals significant differences between generative artificial intelligence (AI)-refined and human-written texts in terms of text length, semantic similarity, and linguistic representations. Using deep learning techniques, we show that incorporating unstructured text data, particularly ChatGPT-refined texts, alongside conventional structured data significantly enhances credit default predictions. Furthermore, we demonstrate how the contents of both human-written and ChatGPT-refined assessments contribute to the models’ prediction and show that the effect of essential words is highly context-dependent. Moreover, we find that ChatGPT’s analysis of borrower delinquency contributes the most to improving predictive accuracy. We also evaluate the business impact of the models based on human-written and ChatGPT-refined texts, and find that, in most cases, the latter yields higher profitability than the former. This study provides valuable insights into the transformative potential of generative AI in financial services.}
}

@article{GONG2025112513,
title = {Emerging Horizons in Hygroscopic Actuation: Cellulose and Agarose for Biomimetic Humidity-Sensitive Systems},
journal = {Materials Today Communications},
pages = {112513},
year = {2025},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2025.112513},
url = {https://www.sciencedirect.com/science/article/pii/S2352492825010256},
author = {Hao Gong and YuHan Zhang},
keywords = {Humidity-sensitive material, Cellulose, Agarose, Actuator},
abstract = {Water resources have always been active in nature's great cycle system in the form of solid-liquid-gas triplets. With the growing demand for renewable energy, novel biomimetic humidity-sensitive (HS) actuators are gaining increasing attention from researchers due to their environmental friendliness, simple structure and wide range of actuating stimuli. These biomimetic hygroresponsive actuators are fundamentally structured as composite systems integrating a matrix phase for bulk material continuity and a reinforcement phase optimized for performance amplification. Within this framework, cellulose and agarose (AG)—as naturally derived biomaterials—are engineered as complementary hygroscopic phases—reinforcement and matrix respectively—exhibiting pronounced hygroactuation dynamics with a propensity for spontaneous bending under humidity-driven free energy gradients, quantified by the response/recovery speed (RS/Rec.S). This review, however, reveals through multidisciplinary analysis that these two materials exhibit intrinsic performance advantages in hygroactuator construction without functional coupling dependencies, while simultaneously demonstrating superior interfacial compatibility with diverse material systems. However, such potential remains largely underexplored, particularly regarding AG-based architectures, which poses significant challenges for large-scale commercial implementation. To optimize this protocol, this review establishes its theoretical foundation on representative studies, systematically analyzing the materials' prominent properties from fundamental principles in each chapter, introduces typical processing methods and characterization, and further demonstrates the prominence of each property. Additionally, the paper identifies underappreciated interdisciplinary insights through cross-domain analysis. Finally, Some applications of cellulose/AG-based actuators are given in this paper concisely, anticipates potential challenges in future development, and proposes green, feasible solutions to advance their practical application in humidity-driven technologies.}
}

@article{QU2025104026,
title = {Compliance assessment oriented microcystin prediction: A Bayesian adaptive LASSO Tobit quantile regression approach},
journal = {Algal Research},
volume = {89},
pages = {104026},
year = {2025},
issn = {2211-9264},
doi = {https://doi.org/10.1016/j.algal.2025.104026},
url = {https://www.sciencedirect.com/science/article/pii/S2211926425001353},
author = {Fan Qu and Lingjing Lin and Changbo Qin and Fuli Peng and Runzi Wang and Nengwang Chen and Gang Zhao and Wentao Lu and Zhongyao Liang},
keywords = {Bayesian inference, Tobin's thinking, Left-censored response variable, Compliance assessment, Quantile regression, Microcystin management},
abstract = {Microcystin has been one of major contaminants impacting health of aquatic ecosystems and threatening human health. The development of drivers-microcystin relationship is of vital importance to microcystin management. However, current practices often focused on the mean response of microcystin concentration and cannot meet the requirement of percentile-based compliance assessment. Despite of many informative studies on the development of drivers–microcystin relationship, there remains a gap between the relationship development and the percentile-based compliance assessment of microcystin concentration. In this study, Bayesian adaptive LASSO Tobit quantile regression (BALTQR) model was introduced to environmental and ecological studies for the first time. The model is specially designed for the prediction of left-censored response variable. We applied the BALTQR model to develop the drivers–microcystin relationship of lakes across the US continent. Based on the results of parameters estimation, Chlorophyll a (CHL), pH, and water temperature (WT) were identified as key drivers to the microcystin concentration. We found that CHL was approximate the same important as pH and both of them had positive effects on the microcystin concentration at all the five regression quantiles. WT was relatively less important and had a surprisingly negative effect at the 0.7, 0.8, and 0.9 regression quantiles. We demonstrated that the BALTQR model successfully established the linkage between the development of drivers–microcystin relationship and the compliance assessment of microcystin concentration. We further revealed important implications of these findings to microcystin management. We believed that the BALTQR model has great potential of generalization to model other left-censored response variable in environmental and ecological studies.}
}

@article{WANG2025110752,
title = {Neurodegenerative disorders: A Holistic study of the explainable artificial intelligence applications},
journal = {Engineering Applications of Artificial Intelligence},
volume = {153},
pages = {110752},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110752},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625007523},
author = {Hongyuan Wang and Shiva Toumaj and Arash Heidari and Alireza Souri and Nima Jafari and Yiping Jiang},
keywords = {Neurodegenerative disorders, eXplainable artificial intelligence, Machine learning, Deep learning, Early diagnosis},
abstract = {Neuro Degenerative Disorders (NDDs) involve progressive nerve cell loss, impacting functions like sensation, movement, memory, and cognition, posing life-threatening risks. Despite extensive research, viable therapies remain elusive due to complex pathophysiology. Artificial Intelligence (AI), particularly Machine Learning (ML) and Deep Learning (DL), shows promise in NDD diagnosis and treatment by leveraging vast datasets for accurate predictions. However, because AI models are “black boxes,” explainable AI (XAI) had to be created to make sure that physicians and patients would trust and accept it. Early detection is critical to stop degeneration and make things better for patients. Many in-depth studies on XAI are designed explicitly for NDDs. Existing research does not constantly look at how to interpret NDDs, how to evaluate them, or how to keep them safe. This paper fills in these gaps by looking at and grouping XAI methods for different NDDs, to make them easier to understand and use in medical settings. In this paper, we look at the interpretability methods used in various NDD studies. The methods are split into five groups based on the conditions they are used to treat: Frontotemporal Dementia (FTD), Multiple Sclerosis (MS), Amyotrophic Lateral Sclerosis (ALS), and Alzheimer's Disease (AD). It organizes XAI methods into groups and talks about their pros, cons, and clinical importance. The study also finds some important research gaps. For example, it says that there are no good security frameworks and that XAI is hard to use in real-life healthcare settings. By giving helpful information and a plan for future research, this paper shows how XAI could change how NDDs are found, treated, and predicted. AI technologies will be used more in healthcare, and this will help us learn more about these challenging conditions.}
}

@article{MARVI2025102908,
title = {Dynamics of user engagement: AI mastery goal and the paradox mindset in AI–employee collaboration},
journal = {International Journal of Information Management},
volume = {83},
pages = {102908},
year = {2025},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2025.102908},
url = {https://www.sciencedirect.com/science/article/pii/S0268401225000404},
author = {Reza Marvi and Pantea Foroudi and Naja AmirDadbar},
keywords = {Artificial Intelligence, Employee–AI Collaboration, User Engagement, AI Mastery Goal, Paradox Mindset, AI Empathy, Technological Frame},
abstract = {Given the scarcity of previous studies on employee–AI collaboration and its impact on employee behavior and user engagement, we investigated its potential to drive user engagement using a mixed-method approach. Grounded in qualitative findings from 27 participants in a healthcare setting, we propose a robust model that emphasizes the impact of AI–employee collaboration on AI mastery goal, user engagement, and a paradox mindset, as well as the moderating role of AI empathy and technological frames. Using a quantitative method, we collected data from 452 participants in a healthcare setting across two studies. Our findings showed that AI–employee collaboration can drive AI mastery goal and a paradox mindset. We also found empirical evidence that both AI mastery goal and the paradox mindset can mediate the relationship between employee–AI collaboration and user engagement. Moreover, our findings revealed interesting moderating results across two studies. In Study 1, significant effects were found for both employee–AI collaboration and AI mastery goal at low AI empathy, but not at high levels. In Study 2, while the interaction between employee–AI collaboration and AI empathy was not significant, the influence of AI mastery goal became significant at high empathy levels, and the paradox mindset showed a significant effect only at high levels of AI empathy. These findings provide managers with valuable insights into the essential operations dynamic of employee–AI collaboration, underscoring its important role in enhancing user engagement.}
}

@article{ALSCHER2025102370,
title = {When will they know what they don’t know? Political knowledge and the infamous “Unskilled and Unaware” effect},
journal = {Contemporary Educational Psychology},
volume = {81},
pages = {102370},
year = {2025},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2025.102370},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X25000359},
author = {Pascal Alscher and Ulrich Ludewig and Ruben Kleinkorres and Nele McElvany},
keywords = {Adolescence, Unskilled and unaware effect, Dunning-Kruger effect, Metacognitive insight, Self-assessment, Political knowledge},
abstract = {The unskilled and unaware effect, also known as the Dunning-Kruger effect, describes that low performers tend to overestimate and high performers tend to underestimate their ability and that on average low performers provide less accurate estimates of their ability than high performers. Based on data from N = 1047 students in Grade 7 (n = 613) and Grade 10 (n = 434), we examined whether the unskilled and unaware effect exists with regard to high school students’ political knowledge, whether the effect is different in Grade 7 and Grade 10, and how teachers’ behavior (i.e., cognitively activating and motivating teaching) affects students’ judgement accuracy and judgement direction. The results show that the unskilled and unaware effect exists in both grade levels. Furthermore, visual and statistical examination of the data suggest that the pattern of data is very similar in both grade levels. Finally, we further find that perceived motivational quality is associated with judgement accuracy, but also with overconfidence. Perceived cognitive activation is negatively associated with judgement accuracy. Through the usage of two-cohort data, this study applies a novel approach and bears important implications for research on the unskilled and unaware effect. Furthermore, the results regarding the teachers’ behavior provide important insights for educational practice in civic education.}
}

@article{DU2025100148,
title = {Experimental evaluation of cognitive agents for collaboration in human-autonomy cyber defense teams},
journal = {Computers in Human Behavior: Artificial Humans},
pages = {100148},
year = {2025},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2025.100148},
url = {https://www.sciencedirect.com/science/article/pii/S2949882125000325},
author = {Yinuo Du and Baptiste Prébot and Tyler Malloy and Fei Fang and Cleotilde Gonzalez},
keywords = {Human-autonomy teaming, Cognitive agent, Cybersecurity},
abstract = {Autonomous agents are becoming increasingly prevalent and capable of collaborating with humans on interdependent tasks as teammates. There is increasing recognition that human-like agents might be natural human collaborators. However, there has been limited work on designing agents according to the principles of human cognition or in empirically testing their teamwork effectiveness. In this study, we introduce the Team Defense Game (TDG), a novel experimental platform for investigating human-autonomy teaming in cyber defense scenarios. We design an agent that relies on episodic memory to determine its actions (Cognitive agent) and compare its effectiveness with two types of autonomous agents: one that relies on heuristic reasoning (Heuristic agent) and one that behaves randomly (Random agent). These agents are compared in a human-autonomy team (HAT) performing a cyber-protection task in the TDG. We systematically evaluate how autonomous teammates’ abilities and competence impact the team’s interaction and outcomes. The results revealed that teams with Cognitive agents are the most effective partners, followed by teams with Heuristic and Random agents. Evaluation of collaborative team process metrics suggests that the cognitive agent is more adaptive to individual play styles of human teammates, but it is also inconsistent and less predictable than the Heuristic agent. Competent agents (Cognitive and Heuristic agents) require less human effort but might cause over-reliance. A post-experiment questionnaire showed that competent agents are rated more trustworthy and cooperative than Random agents. We also found that human participants’ subjective ratings correlate with their team performance, and humans tend to take the credit or responsibility for the team. Our work advances HAT research by providing empirical evidence of how the design of different autonomous agents (cognitive, heuristic, and random) affect team performance and dynamics in cybersecurity contexts. We propose that autonomous agents for HATs should possess both competence and human-like cognition while also ensuring predictable behavior or clear explanations to maintain human trust. Additionally, they should proactively seek human input to enhance teamwork effectiveness.}
}

@article{TRIANTAFYLLOPOULOS2025101802,
title = {Vishing: Detecting social engineering in spoken communication — A first survey & urgent roadmap to address an emerging societal challenge},
journal = {Computer Speech & Language},
volume = {94},
pages = {101802},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2025.101802},
url = {https://www.sciencedirect.com/science/article/pii/S0885230825000270},
author = {Andreas Triantafyllopoulos and Anika A. Spiesberger and Iosif Tsangko and Xin Jing and Verena Distler and Felix Dietz and Florian Alt and Björn W. Schuller},
keywords = {Vishing, Social engineering, Human–computer interaction, Computational paralinguistics},
abstract = {Vishing – the use of voice calls for phishing – is a form of Social Engineering (SE) attacks. The latter have become a pervasive challenge in modern societies, with over 300,000 yearly victims in the US alone. An increasing number of those attacks is conducted via voice communication, be it through machine-generated ‘robocalls’ or human actors. The goals of ‘social engineers’ can be manifold, from outright fraud to more subtle forms of persuasion. Accordingly, social engineers adopt multi-faceted strategies for voice-based attacks, utilising a variety of ‘tricks’ to exert influence and achieve their goals. Importantly, while organisations have set in place a series of guardrails against other types of SE attacks, voice calls still remain ‘open ground’ for potential bad actors. In the present contribution, we provide an overview of the existing speech technology subfields that need to coalesce into a protective net against one of the major challenges to societies worldwide. Given the dearth of speech science and technology works targeting this issue, we have opted for a narrative review that bridges the gap between the existing psychological literature on the topic and research that has been pursued in parallel by the speech community on some of the constituent constructs. Our review reveals that very little literature exists on addressing this very important topic from a speech technology perspective, an omission further exacerbated by the lack of available data. Thus, our main goal is to highlight this gap and sketch out a roadmap to mitigate it, beginning with the psychological underpinnings of vishing, which primarily include deception and persuasion strategies, continuing with the speech-based approaches that can be used to detect those, as well as the generation and detection of AI-based vishing attempts, and close with a discussion of ethical and legal considerations.}
}

@article{HUANG2025111132,
title = {Considering regret psychology and non-cooperative competition among alternatives for heterogeneous multi-attribute group decision making},
journal = {Computers & Industrial Engineering},
pages = {111132},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111132},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225002785},
author = {Yang Huang and Meiqiang Wang},
keywords = {Data envelopment analysis, Regret theory, Game cross-efficiency, Multi-attribute group decision making, Heterogeneous information},
abstract = {In reality, to select the best one from a set of alternatives, there are widespread situations that require several experts to assess the attribute values of each alternative with heterogeneous information. The purpose of this paper is to solve the heterogeneous multi-attribute group decision making (HMAGDM) problems with attribute values involving real numbers, intervals, intuitionistic fuzzy sets, interval type-2 fuzzy sets, and interval-valued hesitant fuzzy sets from the perspective that alternatives have regret psychology with respect to the assessment information and that there is a non-cooperative competitive relationship among alternatives. Therefore a method based on regret theory and interval data envelopment analysis (DEA) game cross-efficiency model is proposed for HMAGDM. The method adopts a process of aggregation followed by exploration. In the aggregation phase, a regret theory-based expert weight determination model is constructed to derive the weights of experts, and the individual decision matrices provided by individual experts are further aggregated into a collective decision matrix. Then, the heterogeneous information in the collective decision matrix is uniformly converted into intervals, which are represented as variables with unknown parameters. According to these variables, a regret theory-based interval DEA game cross-efficiency model is proposed to calculate the comprehensive values of alternatives and thus rank alternatives. The feasibility and effectiveness of the proposed method are illustrated by a supplier selection example.}
}

@article{LEPPANEN2025,
title = {Newsvendor stockouts and option discriminability},
journal = {European Journal of Operational Research},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2025.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0377221725002528},
author = {Ilkka Leppanen},
keywords = {Behavioural OR, Newsvendor, Conflict, Cognition},
abstract = {Decision making is the process of resolving conflict between different options that vary in discriminability. We study how conflict between the goals of profit maximisation and customer satisfaction determines decision making in the newsvendor problem. The paper consists of three studies which explore conflict from different positions. In Study 1 we show that stockouts cause newsvendor subjects to increase their stocking levels, whereas this effect is absent in a neutrally framed version of the same problem. Conflict is reflected in longer response times under low profit margin, where there is an increased likelihood of ex-post conflict, than under high profit margin. We also find that some subjects are more concerned than others of nonpecuniary factors, and this affects their decision making. In Study 2 we show that an endogenous conflict manipulation affects newsvendor behaviour. We theorise that broad bracketing (reappraisal of the choice situation) should decrease conflict, for which we find some evidence, but we also find that subjects decide less optimally when they use broad bracketing than when they use narrow bracketing. In Study 3 we use a binary newsvendor problem and model the decision process of the newsvendor using the diffusion decision model. The results show that, as in Study 1, the profit margin environment affects how newsvendors respond to conflict. Furthermore, the relative decision evidence towards the optimal choice accumulates at a slower rate after there has been a stockout. Our findings highlight that understanding decision biases in operations should include non-monetary goals, such as avoiding stockouts.}
}

@article{MEEKINGS2025101262,
title = {What's the point of talking? Auditory targets and communicative goals},
journal = {Journal of Neurolinguistics},
volume = {75},
pages = {101262},
year = {2025},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2025.101262},
url = {https://www.sciencedirect.com/science/article/pii/S0911604425000181},
author = {Sophie Meekings and Sophie K. Scott},
abstract = {Human speech production is a complex action requiring minute control over the articulators and sensitivity to the surrounding environment. Computational and empirical work has attempted to identify the specific neural mechanisms and cognitive processes that allow us to reliably produce speech sounds. This work has established that humans can use their perception of the auditory and somatosensory consequences of their actions to guide subsequent speech movements. However, speech predominantly takes place in a communicative context, and this context is also known to modulate the way that people speak: human voices are highly flexible. In this paper, we try to unite the traditional motor control conception of internally defined acoustic and somatosensory goals with linguistic research showing that talkers respond and entrain to their conversational partners. We provide an overview of the theoretical and empirical work surrounding the use of sensory feedback monitoring in speech production and discuss practical constraints that have limited more naturalistic investigations into dyadic interaction. To conclude, we argue that the variability of results seen in the speech motor control literature reflects a more complex underlying neural architecture, and an overarching communicative goal that supersedes specific phonetic targets.}
}

@article{MELVILLE2025,
title = {Abstracts},
journal = {Historia Mathematica},
year = {2025},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2025.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0315086025000230},
author = {Duncan J. Melville and Kim Plofker},
abstract = {The purpose of this department is to give sufficient information about the subject matter of each publication to enable users to decide whether to read it. It is our intention to cover all books, articles, and other materials in the field. Books for abstracting and eventual review should be sent to this department. Materials should be sent to Duncan J. Melville, Department of Mathematics, Computer Science and Statistics, St. Lawrence University, Canton, NY 13617, U.S.A. (e-mail: dmelville@stlawu.edu). Readers are invited to send reprints, autoabstracts, corrections, additions, and notices of publications that have been overlooked. Be sure to include complete bibliographic information, as well as transliteration and translation for non-European languages. We need volunteers willing to cover one or more journals for this department. In order to facilitate reference and indexing, entries are given abstract numbers which appear at the end following the symbol #. A double numbering system is used: the first number indicates the volume, the second the sequential number within that volume. For example, the abstracts for Volume 50, are numbered: 50.1, 50.2, 50.3, etc. The initials in parentheses at the end of an entry indicate the abstractor. In this issue there are abstracts by Kim Plofker and Duncan J. Melville.}
}

@article{FORCAEL2025100667,
title = {Enhanced Robotic Cross-Laminated Timber Panel Assembly process},
journal = {Developments in the Built Environment},
pages = {100667},
year = {2025},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2025.100667},
url = {https://www.sciencedirect.com/science/article/pii/S2666165925000675},
author = {Eric Forcael and Ramón Mata and Bryan González and Alexander Opazo-Vega and Rodrigo García-Alvarado and Marcelo González and Eduardo Núñez and Javiera Padilla},
keywords = {Built environment construction, Robotic timber construction, Robotic fabrication, Non-standard timber structures, Computational design},
abstract = {ABSTRACT
The use of Cross-Laminated Timber (CLT) panels in construction is often constrained by their weight, making handling and installation challenging. These limitations frequently result in on-site planning and manual assembly, increasing risks and inefficiencies. This study proposes an integrated framework that combines Building Information Modeling (BIM), discrete event simulation, and robotic assembly to optimize the installation of CLT structures within the built environment. By leveraging these technologies, the methodology addresses material handling challenges while enhancing construction efficiency and adaptability to urban and prefabricated settings. Numerical simulations and robotic assembly experimental tests were conducted to evaluate the framework’s performance. Results demonstrate an improvement in assembly efficiency, reducing both accident risks and installation time compared to manual methods. Strong agreement between numerical and experimental findings underscores the potential of computational tools in advancing automated construction practices. This research provides actionable recommendations to promote the broader adoption of automated processes in CLT construction, contributing to safer, more efficient, and sustainable building practices within the evolving built environment.}
}

@article{XIAO2025124120,
title = {Digital disruption, knowledge and collaborative networks and green innovation in China manufacturing transformation},
journal = {Technological Forecasting and Social Change},
volume = {216},
pages = {124120},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124120},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525001519},
author = {Yao Xiao and Rong Xiang and Yong-lei Sun and Jin Chen and Yun-hong Hao},
keywords = {Digital disruptive events, Knowledge network, Collaboration network, Green innovation},
abstract = {Green innovation is essential for sustainable development but often incurs high costs, reducing economic returns. Grounded in disruptive innovation and social network theory, this study examines whether disruptive events experienced by Chinese manufacturing firms during their digital transformation can reduce green R&D and manufacturing costs, thereby promoting green innovation. Using patent data from China's high-tech manufacturing sector (2014–2023), the study finds an inverted U-shaped relationship between Digital Disruptive Events (DDE) and Green Innovation (GI). The network structure of knowledge and collaboration plays varying moderating roles in this relationship. Structural holes in a firm's knowledge network negatively moderate the effect of DDE on GI, while degree centrality shows no significant moderation. Conversely, more structural holes and lower degree centrality in collaboration networks positively influence GI. The findings highlight the importance of leveraging multilayered network structures to drive green innovation during digital transformation.}
}

@article{LI2025103353,
title = {A multi-task engineering design intention recognition approach based on Vision Transformer and EEG data},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103353},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103353},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002460},
author = {Mingrui Li and Zuoxu Wang and Fan Li and Jihong Liu},
keywords = {Design intention recognition, Engineering design, EEG, Vision Transformer},
abstract = {Engineering product design involves a variety of tasks and scenarios, including design modeling, design calculation, process planning, etc. When performing these design tasks, designers generate constantly shifting design intentions. Accurately recognizing these design intentions allows for a more thorough exploration of design processes from the perspective of cognition, facilitating the advancement of intelligent engineering design. Electroencephalogram (EEG) technology has emerged as an effective tool in recent years, which can provide direct insight into designers’ cognitive processes and intentions. However, the current application of EEG technology in engineering design faces difficulties in adapting to multi-task scenarios and rarely targets the design process directly. This study proposed a design intention recognition approach based on Vision Transformer (ViT) and EEG data applicable to multiple engineering design tasks. An image-like representation matrix is introduced to organize designers’ EEG data with the retention of its spatial and frequency features. Then, standard EEG data under different design intentions as well as the EEG data from real design processes is utilized to train and fine-tune a ViT-based design intention recognition model. An experiment workflow for collecting the two types of EEG data is also presented, along with detailed examples of three design tasks. The comparative experiment results and the case study demonstrates the feasibility of the proposed design intention recognition approach.}
}

@article{MIRZAKHANI2025105973,
title = {Application of machine learning in understanding urban neighborhood gentrification: A meta-synthesis review},
journal = {Cities},
volume = {162},
pages = {105973},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.105973},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125002732},
author = {Niloofar Mirzakhani and Arian {Ali Madad Soltani} and Maedeh Hedayatifard},
keywords = {Gentrification, Machine learning, Neighborhood dynamics, Meta-synthesis, Systematic review},
abstract = {Interest in urban gentrification has grown in recent years due to changes in urban neighborhoods. Using a meta-synthesis approach, this study offers a thorough analysis of datasets and papers pertaining to machine learning applications in gentrification studies from 1979 to 2024. This rigorous meta-synthesis, following a systematic review, screened 217 initial articles, refined to 83 through multi-stage screening including duplicate removal (185), title/abstract (156), and full-text review. By means of this analysis, the study clarifies several facets of gentrification from three main angles: important causes, spatial effects, and algorithmic applications. A strong conceptual framework that takes into account the connections between important indicators is offered, offering a comprehensive comprehension of the phenomenon. The results demonstrate how machine learning may be used to predict future events and spot gentrification patterns. Three main axes drive this predictive capability: first, supervised learning models that use social, economic, environmental, and physical-spatial indicators to predict gentrification; second, unsupervised learning methods that find similar neighborhoods and uncover underlying patterns; and third, hybrid learning methods that combine both supervised and unsupervised approaches (such as semi-supervised learning and ensemble methods) to leverage their complementary strengths in handling multidimensional urban data and enhancing prediction robustness. These advancements enable more in-depth analyses of urban dynamics and a deeper comprehension of the complexities of gentrification. This study provides policymakers with a foundation for developing workable strategies to mitigate the adverse effects of gentrification and advance sustainable urban growth. Furthermore, by identifying research gaps and suggesting possible avenues for future work, the study broadens the scope of the subject and contributes to the development of new ideas and empirical research on urban gentrification. This in-depth examination highlights how crucial machine learning is to understanding and managing urban change, as well as how it may promote evidence-based planning and equitable urban growth.}
}

@article{SELCUK2025106136,
title = {AI-driven civil litigation: Navigating the right to a fair trial},
journal = {Computer Law & Security Review},
volume = {57},
pages = {106136},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106136},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000094},
author = {Seyhan Selçuk and Nesibe {Kurt Konca} and Serkan Kaya},
keywords = {AI in legal proceedings, Automated legal reasoning, ECHR article 6, European union AI act, Judicial independence and AI, Publicity principle in AI justice, Right to a fair trial},
abstract = {The integration of artificial intelligence (AI) into legal proceedings has gained significant traction in recent years, particularly following the Covid-19 pandemic. As part of the broader movement toward the digitalization of legal systems, AI is seen as a tool to improve access to justice, enhance efficiency, and adopt a human-centered approach. However, the rapid advancement of AI necessitates careful consideration of fundamental human rights, especially the right to a fair trial as enshrined in Article 6 of the European Convention on Human Rights (ECHR). Recently, the European Union's Artificial Intelligence Act classifies AI systems used in the judiciary as high-risk, requiring impact assessments on fundamental rights, including the right to a fair trial. This paper explores the impact of AI-driven judicial tools on the right to a fair trial, focusing on key components such as the right to be heard, judicial independence, impartiality, and the principle of publicity. This paper explores the impact of AI-driven judicial tools on the right to a fair trial, focusing on key components such as the right to be heard, judicial independence, impartiality, and the principle of publicity, while examining the risks and opportunities posed by AI in civil litigation, including challenges like algorithmic discrimination, digital exclusion, and the potential erosion of human judges' cognitive abilities.}
}

@article{KENYON2025100962,
title = {Africanus II. QuartiCal: Calibrating radio interferometer data at scale using Numba and Dask},
journal = {Astronomy and Computing},
pages = {100962},
year = {2025},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2025.100962},
url = {https://www.sciencedirect.com/science/article/pii/S2213133725000356},
author = {J.S. Kenyon and S.J. Perkins and H.L. Bester and O.M. Smirnov and C. Russeeawon and B.V. Hugo},
keywords = {Radio astronomy, Calibration, Software, Distributed computing, Numerical methods},
abstract = {Calibration is, and will remain, an integral component of radio interferometric data reduction. However, as larger, more sensitive radio interferometers are conceived and built, the calibration problem grows in both size and difficulty. The increasing size can be attributed to the fact that the data volume scales quadratically with the number of antennas in an array. Additionally, new instruments may have up to two orders of magnitude more channels than their predecessors. Simultaneously, increasing sensitivity is making calibration more challenging: low-level RFI and calibration artefacts (in the resulting images) which would previously have been subsumed by the noise may now limit dynamic range and, ultimately, the derived science. It is against this backdrop that we introduce QuartiCal: a new Python package implementing radio interferometric calibration routines. QuartiCal improves upon its predecessor, CubiCal, in terms of both flexibility and performance. Whilst the same mathematical framework - complex optimization using Wirtinger derivatives - is in use, the approach has been refined to support arbitrary length chains of parameterized gain terms. QuartiCal utilizes Dask, a library for parallel computing in Python, to express calibration as an embarrassingly parallel task graph. These task graphs can (with some constraints) be mapped onto a number of different hardware configurations, allowing QuartiCal to scale from running locally on consumer hardware to a distributed, cloud-based cluster. QuartiCal’s qualitative behaviour is demonstrated using MeerKAT observations of PSR J2009-2026. These qualitative results are followed by an analysis of QuartiCal’s performance in terms of wall time and memory footprint for a number of calibration scenarios and hardware configurations.}
}

@article{WU2025200236,
title = {HC-means Clustering Algorithm for Precision Marketing on E-commerce Platforms},
journal = {Systems and Soft Computing},
pages = {200236},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200236},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000547},
author = {Dan Wu and Xin Liu},
keywords = {Big data, Cluster analysis, K-means algorithm, Precision marketing, Customer segmentation},
abstract = {Abstracts
With the rapid development of e-commerce industry, precision marketing has become a key means for enterprises to enhance competitiveness and profitability. However, traditional marketing methods often cannot accurately identify the characteristics of customers, leading to the waste of e-commerce resources. In this context, e-commerce enterprises urgently need a more accurate and efficient marketing method to meet the growing business needs. To this end, this study attempts to optimize the traditional K-means algorithm, and fundamentally improve the clustering effect in precision marketing by optimizing the selection of initial clustering centers and similarity measurement methods. Based on this, the research constructs an e-commerce marketing system based on HC-means algorithm to more accurately divide customer groups, identify high-value customers, potential customers and lost customers, and formulate differentiated marketing strategies for different groups. Experiments show that the average accuracy of HC-means algorithm in Glass database is 93.71, which is 15.48-15.79 higher than the highest accuracy of other two kinds of algorithms in the same kind of database. When the cluster number is 8, the Mahalanobis distance of HC-Means is reduced by 2.1 and 1.2 respectively compared with K-means and DBSCAN, which indicates that the clustering results are more reasonable in data distribution. When the cluster number is 3, more than half of the customers' consumption interval days are mainly concentrated between 8-12 days, and about 10% of the customers make purchases every 2 days. These accurate customer behavior insights provide a strong basis for marketing strategy development. To sum up, the HC-Means system constructed by the research has achieved remarkable results in e-commerce precision marketing, greatly improving user satisfaction, and providing a valuable reference scheme for e-commerce enterprises to optimize marketing mode and achieve sustainable development.}
}

@article{WU2025109151,
title = {Examining the role and neural electrophysiological mechanisms of adjective cues in size judgment},
journal = {Neuropsychologia},
pages = {109151},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109151},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000867},
author = {Yihan Wu and Ronglian Zheng and Huili Xing and Yining Kou and Yufeng Wang and Xin Wu and Feng Zou and Yanyan Luo and Meng Zhang},
keywords = {Size judgement, language, ERP, EEG Microstate},
abstract = {Numerous influential theories have attempted to elucidate the relationship between language and thought. The debate persists on whether language and thought are distinct entities or if language is deeply embedded in individual cognitive processes. This study employs adjective cues combined with a mental imagery size judgment task as an experimental paradigm, utilizing neurophysiological techniques to preliminarily explore the role of adjectives in size judgment tasks and their underlying neurophysiological mechanisms. Findings reveal that performance is best when adjectives are congruent with the size of the object, with EEG microstate results indicating strong activity in Class A, related to language networks under this condition. Additionally, when adjectives conflict with object size, the discovery of the Ni component suggests that individuals monitor and inhibit the conflict between adjectives and object size, leading to decreased task performance in this condition. Moreover, when object size is ambiguous, individuals' size judgments do not benefit significantly from clear adjective cues. Event-related potentials and EEG microstate results suggest that under this condition, top-down cognitive resources are recruited more extensively. In conclusion, language plays a more crucial role in simpler judgment tasks; as tasks become more complex, judgment processes engage a greater number of distributed brain regions to collaborate, while the language system remains active. This study provides initial cognitive neuroscience evidence for understanding the relationship between language and simple forms of thought, offering preliminary insights for future investigations into the connection between language and thought.}
}

@article{SUN2025135352,
title = {Research on the distribution characteristics of a novel oil displacement system for conformance control in microporous media},
journal = {Fuel},
volume = {396},
pages = {135352},
year = {2025},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2025.135352},
url = {https://www.sciencedirect.com/science/article/pii/S0016236125010774},
author = {Zhe Sun and Xiujun Wang and Weijun Shen},
keywords = {A novel hydrogel system, Conformance control technology, Seepage behavior, Spatial distribution characteristics, Mechanism analysis},
abstract = {This study aims to investigate the distribution characteristics of a novel hydrogel system (HDS) for in-depth conformance control. Through a combination of static gelation experiments and core flow experiments, the thickening performance and seepage characteristics in porous media of the novel HDS system was studied. Considering the viscosity changes and dynamic chain extension reactions of the HDS system, the Darcy-Brinkman-Stokes (DBS) micro-continuum approach was applied to develop a multi-physical mathematical model for the migration of HDS system in a pore-throat network. Therefore, a 4-meter core experiment and multi-scale numerical modeling were conducted to analyze the dynamic chain extension efficiency and spatial distribution characteristics of the HDS system. Additionally, machine learning techniques were used to define the dynamic chain extension reaction index (RI) and identify its critical threshold, exploring the effects of varying injection parameters, solution concentrations, and permeabilities on the chain extension behavior. Results show that HDS molecular chains form aggregated structures that evolve into a unique spatial network. As the network structure becomes denser, the viscosity of HDS system increases rapidly. As the core permeability increases, the core throat size enlarges, and the compatibility between the HDS system molecular aggregates and the pore throat improves. HDS system shows good transmission performance during migration in the long core. Numerical simulations and machine learning techniques were employed to define the dynamic chain extension RI, examining how RI changes under varying injection parameters, solution concentrations, and permeabilities. As permeability, HDS system concentration, and injection rate increase, the RI also increases. The results are presented in the RI graph, offering theoretical insights for deep profile adjustment and the development of reagent systems.}
}

@incollection{MATYUS2025,
title = {The Molecular Quantum electro-Dynamics Research Group in Budapest},
series = {Advances in Quantum Chemistry},
publisher = {Academic Press},
year = {2025},
issn = {0065-3276},
doi = {https://doi.org/10.1016/bs.aiq.2025.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0065327625000322},
author = {Edit Mátyus},
keywords = {Rovibrational Schrödinger equation, Non-adiabatic effects, Relativistic and QED effects, Relativistic quantum electrodynamics, Precision spectroscopy},
abstract = {This article briefly overviews the scientific activities of the Molecular Quantum (electro-)Dynamics (MQD) Research Group in Budapest. Since its foundation in 2016, the MQD group has worked on molecular spectroscopy and molecular physics topics with primary applications and relevance to high-resolution and precision spectroscopy.}
}

@article{SWEET2025101233,
title = {Transdisciplinary coordination is essential for advancing agricultural modeling with machine learning},
journal = {One Earth},
volume = {8},
number = {4},
pages = {101233},
year = {2025},
issn = {2590-3322},
doi = {https://doi.org/10.1016/j.oneear.2025.101233},
url = {https://www.sciencedirect.com/science/article/pii/S2590332225000594},
author = {Lily-belle Sweet and Ioannis N. Athanasiadis and Ron {van Bree} and Andres Castellano and Pierre Martre and Dilli Paudel and Alex C. Ruane and Jakob Zscheischler},
keywords = {machine learning, crop models, agriculture, food security, model development, AgMIP, climate impacts},
abstract = {Summary
Crop models play a key role in the study of climate change impacts on food production as well as improving food systems resilience and analyzing the effect of potential adaptation interventions. Here, we illustrate opportunities that machine learning offers for tackling key challenges of agricultural modeling. However, to unlock the full potential of machine learning, and thereby accelerate progress toward a more secure and sustainable global food system, serious pitfalls must first be addressed. We argue that transdisciplinary coordination is needed to identify impactful research gaps, curate and maintain benchmark datasets, and establish domain-specific best practices.}
}

@article{GUO2025111213,
title = {Advances in mucosal vaccines: Design strategies for antigens, adjuvants, and delivery systems},
journal = {Chinese Chemical Letters},
pages = {111213},
year = {2025},
issn = {1001-8417},
doi = {https://doi.org/10.1016/j.cclet.2025.111213},
url = {https://www.sciencedirect.com/science/article/pii/S1001841725003985},
author = {Jiaxin Guo and Yongyi Xie and Muhammad Waqqas Hasan and Yongcheng Zhu and You Zhou and Zhengfeng Li and Wenjie Chen},
keywords = {Mucosal vaccine, Mucosal immunity, Adjuvants, Mucosal delivery systems, Antigen design},
abstract = {ABSTRACT
Mucosal vaccines would be game-changing for blocking pathogenic transmission, prompting protection where microorganism first enters that those intramuscular ones could not be able to achieve. The exploration of the vaccines at mucosal surfaces is gaining momentum due to the unique immune reservoir they offer in a minimally invasive manner. Nevertheless, the application of mucosal vaccines faces challenges, including barriers such as degrading enzymes, mucus interference, and clearance mechanisms. The field of mucosal vaccination is still in its early stages, and its advancement will significantly benefit from foundational inquiries into immune activation mechanisms and the innovation of delivery technologies for optimal efficacy. It is highly central to design efficient systems for mucosal vaccine development, herein, this article offers the insights towards the status, bottlenecks and solutions in this field, the intricacies of the immune response, fundamental mechanisms, applications of the delivery strategies for various forms of mucosal vaccines are explored. Collectively, this review conducts systematical analysis on biological and chemical strategies designed to augment vaccine uptake across mucosal tissues, antigen design and delivery methods strengthening vaccination efficacy, with emphasis on the emerging mRNA mucosal vaccines, offering new insights into recent advancements, trends and future scenarios, aiming to harness mucosal immunity (MI) for comprehensive protection against infections and other diseases.}
}

@article{SU2025123209,
title = {Numerical simulation and experimental study of ARS for the resourceful utilization of low-grade heat hazards from high-geothermal tunnels},
journal = {Renewable Energy},
pages = {123209},
year = {2025},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2025.123209},
url = {https://www.sciencedirect.com/science/article/pii/S0960148125008717},
author = {Liufeng Su and Qixiang Yan and Yifan Yang and Junnan Ren and Minjie Qiao and Yajun Xu},
keywords = {Low grade heat source, Resource utilization of heat damage, High geothermal tunnels, Absorption refrigeration system},
abstract = {To address the critical challenges posed by high geothermal heat hazards in tunnel construction that threaten worker safety and constrain operational efficiency, this study pioneers the utilization of high-temperature water inrush within the tunnel as a heat source to drive an absorption refrigeration system (ARS), thereby enabling proactive thermal regulation of the construction environment. A steady-state ARS model was established using Simulink to comprehensively analyze the impacts of parameters including heat source water, cooling water, and chilled water on the system's coefficient of performance (COP). The investigation revealed that elevated heat source water temperatures significantly enhance refrigeration performance, while increased cooling water temperatures cause synchronous reductions in both cooling capacity and COP. Chilled water flow rate variations demonstrate negligible effects on COP. Furthermore, an ARS laboratory test platform was constructed and validated through indoor experiments, incorporating environmental parameters from an ongoing high geothermal tunnel project and numerical simulation results. The findings demonstrate excellent agreement between computational predictions and experimental data, confirming the robust adaptability of the designed ARS under the specific tunnel conditions. In summary, this research establishes a theoretical foundation for resourceful utilization of high geothermal heat hazards, contributing to green construction practices and sustainable development in tunneling engineering.}
}

@article{REGO2025100506,
title = {Structural, mechanical, and electronic properties of single graphyne layers based on a 2D biphenylene network},
journal = {Carbon Trends},
pages = {100506},
year = {2025},
issn = {2667-0569},
doi = {https://doi.org/10.1016/j.cartre.2025.100506},
url = {https://www.sciencedirect.com/science/article/pii/S2667056925000562},
author = {Mateus Silva Rêgo and Mário Rocha dos Santos and Marcelo Lopes Pereira and Eduardo Costa Girão and Vincent Meunier and Paloma Vieira Silva},
keywords = {Graphynes, Biphenylene sheet, Local magnetic moment, Semiconductor},
abstract = {Graphene is a promising material for the development of applications in nanoelectronic devices, but the lack of a band gap necessitates the search for ways to tune its electronic properties. In addition to doping, defects, and nanoribbons, a more radical alternative is the development of 2D forms with structures that are in clear departure from the honeycomb lattice, such as graphynes, with the distinctive property of involving carbon atoms with both hybridizations sp and sp2. The density and details of how the acetylenic links are distributed allow for a variety of electronic signatures. Here we propose a graphyne system based on the recently synthesized biphenylene monolayer. We demonstrate that this system features highly localized states with a spin-polarized semiconducting configuration. We study its stability and show that the system’s structural details directly influence its highly anisotropic electronic properties. Finally, we show that the symmetry of the frontier states can be further tuned by modulating the size of the acetylenic chains forming the system.}
}

@article{MAPUI2025101598,
title = {Lyapunov-like prescribed-time stability of impulsive systems via event & self-triggered impulsive control},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {57},
pages = {101598},
year = {2025},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2025.101598},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X2500024X},
author = {Arnab Mapui and Santwana Mukhopadhyay},
keywords = {Prescribed-time stability, Impulsive systems, Event & self-triggered impulsive control, Lyapunov stability, Zeno behavior},
abstract = {The main emphasis of this paper is to address the issue of event- and self-triggered impulsive control for the prescribed-time stability of impulsive systems. In the first part of the article, some Lyapunov-like sufficient conditions are derived to stabilize an impulsive system within the prescribed-time, where impulsive instants are obtained through an event-triggered mechanism. Unlike event-triggered control, event-triggered impulsive control executes solely when the event is generated. The second part of the article provides sufficient Lyapunov-like criteria for the prescribed-time stability of impulsive systems via a self-triggered mechanism. Contrary to the event-triggered impulsive control, where triggering instants are obtained through continuous or periodic monitoring of the event-triggering conditions, the proposed self-triggered impulsive control strategy can estimate the next triggering instant by the information available in the currently triggered instant. Moreover, it is demonstrated that the Zeno behavior can be excluded from both of the proposed mechanisms. Finally, two instances are provided to illustrate the theoretical results numerically and verify the veracity of the proposed methodologies.}
}

@article{GHAVIHOSSEINZADEH2025110395,
title = {Artificial intelligence in veterinary and animal science: applications, challenges, and future prospects},
journal = {Computers and Electronics in Agriculture},
volume = {235},
pages = {110395},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2025.110395},
url = {https://www.sciencedirect.com/science/article/pii/S0168169925005010},
author = {Navid {Ghavi Hossein-Zadeh}},
keywords = {Artificial intelligence, Deep learning, Machine learning, Precision livestock farming, Veterinary diagnostics},
abstract = {Artificial intelligence (AI) is increasingly transforming animal and veterinary science through improved decision-making, predictive modeling, and automation. This review comprehensively covers applications of key AI technologies, including machine learning, deep learning, computer vision, natural language processing, robotics, and edge AI, in areas such as disease diagnosis, behavioral monitoring, multi-omics data integration, and precision livestock farming. It also highlights current limitations, including data fragmentation, high implementation costs, and ethical concerns. By addressing these challenges through interdisciplinary collaboration, standardized data systems, and the development of explainable and scalable AI tools, the field can advance toward more sustainable, efficient, and welfare-oriented practices. This review underscores the transformative potential of AI in achieving the goals of One Health and global food security while emphasizing the need for continued research, policy support, and equitable access to AI technologies.}
}

@article{KONG2025106353,
title = {A survey on the progression of artificial intelligence techniques in the nonferrous metallurgical industry},
journal = {Control Engineering Practice},
volume = {162},
pages = {106353},
year = {2025},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2025.106353},
url = {https://www.sciencedirect.com/science/article/pii/S0967066125001169},
author = {Peng Kong and Bei Sun and Yonggang Li and Chunhua Yang and Weihua Gui},
keywords = {Nonferrous metallurgical industry, Artificial intelligence, Comprehensive perception, Collaborative optimization, Intelligent autonomous control},
abstract = {Nonferrous metals serve as a material base for economic development. With the increasing focus on production safety, environmental protection, and sustainable resource utilization, nonferrous metal enterprises urgently need intelligent transformation and upgrading to remain competitive in the modern era. This paper concisely reviews the literature concerning modeling, process monitoring, optimization, and control in the nonferrous metallurgical (NFM) industry, including traditional approaches and the development and current state of artificial intelligence (AI) applications. AI is increasingly integrating with the unique characteristics of the NFM processes and playing a crucial role at various stages of production. Additionally, this paper explores future directions of intelligent development in the NFM industry, proposing a framework for intelligent optimization control. This framework encompasses a structured and comprehensive perception of production states, plant-wide cross-level collaborative optimization, and intelligent autonomous control at the device level, thus establishing a foundation for the intelligent transformation of NFM enterprises. In conclusion, integrating AI into the NFM industry is poised to enhance operational efficiency and innovation significantly, driving the industry toward a more sustainable and intelligent future.}
}

@article{YU2025103745,
title = {The impact of individual AI proficiency on human–agent collaboration: Higher sensitivity to discern the comprehension ability of intelligent agents for users with higher AI proficiency levels},
journal = {International Journal of Industrial Ergonomics},
volume = {107},
pages = {103745},
year = {2025},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2025.103745},
url = {https://www.sciencedirect.com/science/article/pii/S0169814125000514},
author = {Ruifeng Yu and Xinran Xu and Shuo Peng},
keywords = {AI proficiency, Human–agent collaboration, Trust, Perceived intelligence, Perceived anthropomorphism},
abstract = {This study explored the impact of individual artificial intelligence (AI) proficiency on task scores, human perceptions of the agent's intelligence and anthropomorphism, trust, and mental workload from the perspective of human–agent collaboration. A 2 (Individual AI Proficiency, between-subjects) × 2 (Intelligent Agent's Comprehension Ability for Human Intentions, within-subjects) mixed experimental design was implemented in a task in which the participants and intelligent agents collaborated to navigate an unknown map. Forty participants participated in this study. The results revealed significant interaction effects between human AI proficiency and the agents' comprehension abilities on human trust, perceived intelligence, and perceived anthropomorphism. Users with higher AI proficiency demonstrated greater trust, perceived intelligence, and anthropomorphism when interacting with intelligent agents that interpreted human intention based on both immediate and previous feedback, compared to intelligent agents based solely on immediate feedback, displaying a higher sensitivity to the change in agents' ability. Additionally, intelligent agents that interpret human intentions based on both immediate and previous feedback significantly reduce users' mental workload compared with those that rely solely on immediate feedback.}
}

@article{LI2025,
title = {Traffic signal coordinated control model for long arterial based on traffic flow spatiotemporal characteristics},
journal = {Transportation Letters},
year = {2025},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2025.2488971},
url = {https://www.sciencedirect.com/science/article/pii/S1942786725000256},
author = {Qile Li and Zhizhen Liu and Lan Yao and Hangze Li and Guanzhi Xiong and Yuxuan Zhang},
keywords = {Collaborative optimization, signal coordination control, subarea division, traffic flow characteristics analysis, traffic signal control},
abstract = {ABSTRACT
Traditional signal coordination methods face challenges in ensuring efficient traffic flow on long arterials due to urban expansion and complex spatiotemporal variations. However, existing methods struggle to achieve effective signal coordination under complex spatiotemporal variations, and lack methodological framework for universally applicable green wave coordination. To address this, a spatiotemporal partitioning-based green wave trajectory feature coordination optimization model is proposed. First, temporal partitioning is performed using an improved Fisher optimal segmentation method, while spatial subarea division is achieved via an enhanced K-Medoids algorithm. For each subarea, an arterial traffic signal control model is established based on green wave trajectory characteristics. Phase difference coordination equations are then applied to synchronize adjacent subareas. The model is validated on Foshan’s Lvjing Road, with evening peak performance compared against a classical green wave trajectory approach. Results indicate that the proposed model reduces vehicle average delay by 13.18% and the number of stops by 18.05%.}
}

@article{LIU2025111367,
title = {Alterations in structural and functional magnetic resonance imaging associated with cognitive function in patients with treatment-naïve first-episode major depressive disorder},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
pages = {111367},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2025.111367},
url = {https://www.sciencedirect.com/science/article/pii/S0278584625001216},
author = {Chenyu Liu and Hehua Li and Shixuan Feng and Ziyun Zhang and Miaolan Huang and Shisong Lin and Liangda Zhong and Dongchang Huang and Yuanyuan Huang and Kai Wu and Fengchun Wu},
keywords = {Major depressive disorder, Cognitive function, Sulcus depth, Local Gyrification index, Amplitude of low frequency fluctuations},
abstract = {Background
Cognitive impairment is a prominent feature in the clinical presentation of major depressive disorder (MDD). Patients with MDD have brain structural and functional abnormalities. However, the association between such abnormalities and cognitive function remains unclear.
Methods
For this research, 105 patients with treatment-naïve first-episode MDD and 53 healthy controls (HCs) underwent magnetic resonance imaging (MRI) and neuropsychological assessment. The MRI main indicators included sulcus depth (SD), local gyration index (LGI) and amplitude of low frequency fluctuations (ALFF). The MATRICS Consensus Cognitive Battery (MCCB) was used for neuropsychological assessment. The support vector machine (SVM) was used to assess the accuracy of the classification.
Results
Compared with the HCs, the patients with MDD had significant decreases in five dimensions of the MCCB, as well as in SD in the left superior temporal sulcus and inferior parietal cortex, but had an increases in LGI in the left precuneus cortex and pericalcarine cortex and ALFF of the left calcarine fissure and surrounding cortex. In addition, the visual learning score (one MCCB dimension) was negatively associated with the SD of the left superior temporal sulcus and positively associated with the ALFF of left calcarine fissure and surrounding cortex. The SVM has a relatively good ability to distinguish patients with MDD and HCs.
Conclusion
Cognitive impairment in patients with MDD was associated with abnormal an SD and ALFF. These findings help to further understand cognitive impairment in patients with MDD.}
}

@article{MORLOCK2025465946,
title = {Chemical safety screening of products – better proactive},
journal = {Journal of Chromatography A},
volume = {1752},
pages = {465946},
year = {2025},
issn = {0021-9673},
doi = {https://doi.org/10.1016/j.chroma.2025.465946},
url = {https://www.sciencedirect.com/science/article/pii/S0021967325002948},
author = {Gertrud E. Morlock},
keywords = {Hazard analysis, Safety screening of products, Toxicological profiling, Prioritization strategy, Sustainable 2LabsToGo-Eco},
abstract = {The increasing pressure to ensure product safety in a global market comes up against the current practice of targeting only known hazardous compounds in product safety analysis. However, product safety refers not only to known but also to unknown or hidden hazards that are very important to know and avoid. Shortcomings and limitations of currently used technologies seem to cause an obvious discrepancy between intended and actual consumer protection. Products are not as safe as claimed by stakeholders. An existing but overlooked proactive safety screening with a prioritization strategy is brought into focus as it offers a unique solution. It can handle the complexity of a product with thousands of compounds of unknown identity and unknown toxicity and can figure out the important hazardous compounds, both known and unknown. Using hardly any sample preparation and the effect detection at an early position in the workflow is a game changer not to overlook hazardous compounds. All analytical technologies are needed, but the key is the re-arrangement of the instrument order, i.e. firstly hazard-related screening (effect first) and secondly, focus on identification of prioritized hazardous compounds. Such a proactive safety screening revealed previously unknown hazardous compounds in products on the market claimed to be safe. The highly sustainable, affordable, and all-in-one 2LabsToGo-Eco with easy-to-use planar bioassays empowers stakeholders to implement proactive safety screening and dynamic risk management. The transition to greater efficacy in consumer protection needs incentives and the critical review aims to stimulate a debate.}
}

@article{SCHOLTE2025,
title = {Beyond binding: from modular to natural vision},
journal = {Trends in Cognitive Sciences},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2025.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661325000749},
author = {H. Steven Scholte and Edward H.F. {de Haan}},
keywords = {visual processing, feature integration, deep neural networks, binding problem, image statistics},
abstract = {The classical view of visual cortex organization as a collection of specialized modules processing distinct features like color and motion has profoundly influenced neuroscience for decades. This framework, rooted in historical philosophical distinctions between qualities, gave rise to the ‘binding problem’: how the brain integrates these separately processed features into coherent percepts. We present converging evidence from electrophysiology, neuroimaging, and lesion studies that challenges this framework. We argue that the binding problem may be an artifact of theoretical assumptions rather than a real computational challenge for the brain. Drawing insights from deep neural networks (DNNs) and recent empirical findings, we propose a framework where the visual cortex represents naturally co-occurring patterns of information rather than processing isolated features that need binding.}
}

@article{HOU2025105329,
title = {Measuring Undergraduate Students' Reliance on Generative AI During Problem-Solving: Scale Development and Validation},
journal = {Computers & Education},
pages = {105329},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105329},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000971},
author = {Chenyu Hou and Gaoxia Zhu and Vidya Sudarshan and Fun Siong Lim and Yew Soon Ong},
keywords = {Human-AI Collaboration, Problem-solving, Generative AI, Higher Education, Reliance on AI, Scale Development},
abstract = {Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students’ behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach’s alpha = .84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1,173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students’ reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.}
}

@article{FIROOZI2025104973,
title = {Transformative Impacts of Nanotechnology on Sustainable Construction: A Comprehensive Review},
journal = {Results in Engineering},
pages = {104973},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104973},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025010497},
author = {Ali Akbar Firoozi and Ali Asghar Firoozi and Mohammad Reza Maghami},
keywords = {Sustainable Nanotechnologies, Eco-efficient Construction, Green Building Materials, Nano-enhanced Concrete, Lifecycle Analysis in Construction, Environmental Nanotechnology},
abstract = {Integrating nanotechnology into civil engineering represents a pivotal advancement in construction methodologies, offering substantial improvements in material performance, structural monitoring, and lifecycle efficiency. This comprehensive review synthesizes peer-reviewed literature and case studies to evaluate both established and emerging applications of nanomaterials, including nano-enhanced concrete, self-healing systems, and embedded nanosensors. Quantitative outcomes reported in the literature, such as up to a 30% reduction in carbon emissions and a 25% gain in energy performance, are critically analyzed within broader frameworks of lifecycle assessment and regulatory alignment. In addition to technical evaluations, this study examines economic implications, workforce impacts, and the ethical considerations associated with nanotechnology adoption in construction. It also identifies persistent challenges related to large-scale implementation, cost barriers, and long-term safety. By highlighting the convergence of engineering innovation, environmental metrics, and policy development, this review encourages interdisciplinary collaboration to support the responsible integration of nanotechnologies in future civil infrastructure systems.}
}

@article{KIM2025105032,
title = {Unveiling teacher identity development: A case study of AI curriculum implementation in a rural middle school computer science class},
journal = {Teaching and Teacher Education},
volume = {160},
pages = {105032},
year = {2025},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2025.105032},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X25001088},
author = {Keunjae Kim and Kyungbin Kwon},
keywords = {Teacher identity development, AI curriculum, Rural middle school, Latent dirichlet allocation (LDA), STEM+C education},
abstract = {This study investigates how a CS teacher's professional identity develops during the implementation of an AI curriculum in a rural middle school. Addressing teachers' challenges with limited AI training, it focuses on identity dimensions of participation, self-categorization, and confidence. Data sources include classroom video observations and teacher interviews, analyzed using content analysis and Latent Dirichlet Allocation (LDA). The study finds that modeling, collaboration, and self-reflection strategies increase teacher participation and confidence, supporting positive self-categorization as an independent AI educator. The findings highlight the importance of targeted support to foster teacher identity development and preparedness in AI-integrated K-12 STEM + C education.}
}

@article{LI2025113147,
title = {Knowledge-Enhanced Large Language Models for Ideation to Implementation: A New Paradigm in Product Design},
journal = {Applied Soft Computing},
pages = {113147},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.113147},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625004582},
author = {Zhinan Li and Zhenyu Liu and Guodong Sa and Jiacheng Sun and Mingjie Hou and Jianrong Tan and Lei Sun and Jun Wei},
keywords = {Decision Support Systems, Knowledge Enhancement, Knowledge Graph, Large Language Models, Multi-Design Task Adapter, Product Innovation Design, Product Intelligent Design},
abstract = {ABSTRACT
Traditional product design processes often struggle to accurately capture complex user needs and generate market-relevant solutions due to a heavy reliance on subjective human input and limited decision support tools. While Large Language Models (LLMs) have shown proficiency in various domains, their application in product design remains limited, often resulting in generic outputs. To address this, we propose an innovative paradigm for intelligent product design generation, termed ProdGen. The core of ProdGen is the ProdGen-Agent system, which integrates LLMs with customized expert design tools, leveraging the proposed Multi-Design Task Adapter (MDT-A) method and a Dual Knowledge Enhancement Mechanism. The MDT-A method injects multimodal design task knowledge into LLMs through a unified knowledge fusion framework, enabling enhanced task decomposition and efficient interaction with custom design tools. The Dual Knowledge Enhancement Mechanism enriches LLM performance by incorporating domain-specific knowledge bases and structured graph-based data retrieval, ensuring more accurate and relevant design outputs. Demonstrated through kitchen design cases, ProdGen-Agent autonomously handles the entire design process, excelling in user need analysis, task breakdown, decision-making support, tool integration, and multidimensional design generation. Expert evaluations validate ProdGen-Agent’s effectiveness in automating complex design tasks, confirming its potential to revolutionize product design processes across various industries by leveraging LLMs in combination with domain expertise.}
}

@article{GUTIERREZIBANEZ2025,
title = {How do big brains evolve?},
journal = {Trends in Ecology & Evolution},
year = {2025},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2025.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169534725000631},
author = {Cristián Gutiérrez-Ibáñez and Pavel Němec and Martin Paré and Douglas R. Wylie and Louis Lefebvre},
keywords = {brain evolution, cognition, pallium, cerebellum, neuron numbers},
abstract = {In both birds and mammals, variation in brain size predominantly reflects variation in mass or volume of the pallium (neocortex) and, to a lesser extent, of the cerebellum, suggesting convergent coevolution of brains and cognition. When brain measures are based on neuron counts, however, a surprisingly different picture emerges: The number of neurons in the cerebellum surpasses those in the pallium of all mammals (including humans and other primates) and in many but not all birds studied to date. In particular, parrots and corvids, clades known for cognitive abilities that match those of primates, have brains that contain more pallial than cerebellar neurons. Birds and mammals may thus have followed different evolutionary routes of pallial–cerebellar coordination behind enhanced cognitive complexity.}
}

@article{CARSON2025100316,
title = {Results of a randomized evaluation of team-based learning exercises},
journal = {International Review of Economics Education},
volume = {49},
pages = {100316},
year = {2025},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2025.100316},
url = {https://www.sciencedirect.com/science/article/pii/S1477388025000088},
author = {Katherine Silz Carson and Jimena Gonzalez-Ramirez and Craig Heinicke and Mark Maier and Phil Ruder and Scott P. Simkins and Hiuko Adams and James “Michael” Latham and C. Lucy Malakar},
keywords = {Team-based learning, randomized controlled trial, active learning},
abstract = {This paper describes the results of a multi-site randomized-controlled evaluation of the effect on student learning of Team-Based Learning (TBL) application exercises vs. traditional exercises in Principles of Microeconomics courses using the TBL pedagogy. The use of random assignment of a site to treatment or control group for each of four study modules combined with student fixed effects enables identification of the treatment effect on student learning while controlling for student characteristics. The results show moderate positive treatment effects of the TBL application exercises.}
}

@article{FREY2025125603,
title = {Sample preparation techniques to enhance uniformity of Low-Dose blends mixed by resonant acoustic mixing technology},
journal = {International Journal of Pharmaceutics},
pages = {125603},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125603},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325004405},
author = {Kyle A. Frey and Helen Baker and Dale K. Purcell and Andrew L. Lewis},
keywords = {Resonant Acoustic Mixing (RAM), Blend Uniformity (BU), Low Dose, Agglomeration, Sample Preparation},
abstract = {In our previous work we presented the very first examples of uniform mixtures of < 0.1 % w/w API in a single step dry powder mixing process where caffeine (CAF), a morphologically challenging API, was successfully blended under finely tuned optimized RAM parameters in an idealized binary matrix. Presently, we introduce several effective methods to improve BU results for a lesser compatible formulation for which we had thus far been unable to prepare to acceptable levels of homogeneity to meet AV specification criteria. Simple, practical strategies for overcoming BU issues are proposed assessing a range of techniques, with results demonstrating a powerful potential for RAM performance to be improved upon through only small adjustments to initial sample conditions. Several anomalous results and behavior are reported which have eluded mechanistic understanding through conventional means. In the course of pursuing a unified theory capable of correct mechanistic interpretations of RAM results, a fresh perspective is offered from a refined theoretical development of RAM-specific principles, and mechanisms such as vibrational forcing and exponential amplification are proposed. By investigating the roles of RAM operational mechanics and fundamental acoustic principles, satisfactory explanations can begin to be obtained. By considering these facets to RAM mixing processes, microscale mixing potential was increased by a factor of 4x and allowed for homogeneity to be established at 150 μg using a semi-fine CAF sample and extend to levels down to at least 32.5 μg.}
}

@article{FAN2025110729,
title = {A new approach to failure mode and effect analysis based on regret theory and group satisfaction index in basin-type insulators},
journal = {Engineering Applications of Artificial Intelligence},
volume = {152},
pages = {110729},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110729},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625007298},
author = {Jianping Fan and Yihua Duan and Meiqin Wu},
keywords = {Failure mode and effect analysis, Regret theory, Group satisfaction index, The combined compromise solution method, Method based on the removal effects of criteria},
abstract = {Gas-insulated metal-enclosed switchgear, as an important part of the power transmission and transformation project of the global energy Internet, has the advantages of high reliability and small footprint. As an important part of the switchgear, basin-type insulators will seriously affect the reliability and safety of the switchgear operation in case of failure such as breakage or partial discharge. There are still many limitations in the current risk assessment methods for basin-type insulators, resulting in inaccurate risk identification and classification. Therefore, this paper proposes an extended failure mode and effect analysis (FMEA) based on the defects of traditional FMEA model based on regret theory(RT), group satisfaction index and the combined compromise solution (CoCoSo) method to assess the failure mode of basin-type insulators. First, the expert applies the 5Why analysis to obtain the failure modes of basin-type insulators. Then the risk assessment information given by the experts in uncertain linguistic terms (ULT) is assembled by probabilistic uncertain linguistic terms sets (PULTSs). Secondly, in order to improve the accuracy of the expert group information during the calculation, this paper proposes a new group satisfaction index for describing the degree of satisfaction and disagreement of expert opinions, and constructs a PUL-MEREC-SWARA integrated weight acquisition method based on this index for determining the weights of risk factors. Finally, in order to improve the accuracy and stability of the ranking, a new PUL-RT-CoCoSo integrated decision framework is proposed to rank the importance of failure modes in a more rational way. The stability, validity and superiority of the proposed model are demonstrated through the sensitivity analysis of parameters and comparison with several typical ranking methods.}
}

@article{RAPTI2025101849,
title = {A bibliometric and content analysis of educational robotics’ impact on communication, collaboration, critical thinking, and creativity in kindergarten},
journal = {Thinking Skills and Creativity},
pages = {101849},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101849},
url = {https://www.sciencedirect.com/science/article/pii/S1871187125000987},
author = {Sophia Rapti and Sokratis Tselegkaridis and Theodosios Sapounidis and Serafeim A. Triantafyllou},
keywords = {Kindergarten, communication, collaboration, critical thinking, creativity, bibliometric analysis, content analysis},
abstract = {ABSTRACT
Communication, collaboration, critical thinking, and creativity are 21st-century skills required to be developed in educational settings from an early age. Robotics seems to contribute to that. Therefore, this study focuses on kindergarten and explores the current research trends regarding children's development of these skills via robotics. It aims to a) construct links between the main factors in the existing research, b) map the related field, and identify gaps to enhance our understanding. In this context, a bibliometric analysis using the Bibliometrix package and CiteSpace and a content analysis utilizing the Atlas.ti Software is employed covering 203 of 3822 studies conducted between 2014 and December 2024. Based on the findings, the most frequently examined topics are social-anthropomorphic robots, human-robot interaction, and computational thinking under the umbrella of educational robotics and skills development. The highest publication performance belongs to the USA, Italy, Japan, China, and Greece. The strongest sources are ACM, IEEE International Conference on Human-Robot Interaction, the Education and Information Technologies Journal, the International Journal of Child-Computer Interaction, and the Computers and Education Journal. Besides, Papert, Wing, and Bers arise as the most influential authors. Furthermore, most studies explore one or two C-skills, examining critical thinking the most and creativity the least during interventions focused on several domains but rarely on skills promotion. Moreover, most researchers choose the qualitative method, using small sample sizes, wheeled, floor robots, and various learning theories in short-term interventions. Finally, robots are utilized as assistive instruction kits, teammates, and educators’ assistants while children act as researchers-constructors.}
}

@article{ALHAWARNEH202530,
title = {Life-cycle thinking and performance-based design of bridges: A state-of-the-art review},
journal = {Resilient Cities and Structures},
volume = {4},
number = {2},
pages = {30-45},
year = {2025},
issn = {2772-7416},
doi = {https://doi.org/10.1016/j.rcns.2025.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S277274162500016X},
author = {Alaa {Al Hawarneh} and M. Shahria Alam and Rajeev Ruparathna and Stavroula J. Pantazopoulou},
keywords = {Life-cycle analysis, Life-cycle sustainability, Life-cycle performance, Multiple-hazards, Resiliency, Climate change},
abstract = {Given the growing emphasis on life-cycle analysis in bridge design, the design community is transitioning from the concept of performance-based design in structural engineering to a performance-based design approach within a life-cycle context. This approach considers various indicators, including cost, environmental impact, and societal factors when designing bridges. This shift enables a comprehensive assessment of structural resilience by examining the bridge's ability to endure various hazards throughout its lifespan. This study provides a comprehensive review of two key research domains that have emerged in the field of bridge life-cycle analysis, namely life-cycle sustainability (LCS) and life-cycle performance (LCP). The discussion on the LCS of bridges encompasses both assessment-based and optimization-based studies, while the exploration of LCP focuses on research examining structures subjected to deterioration over their service life due to deprecating phenomena such as corrosion and relative humidity changes, as well as extreme hazards like earthquakes and floods. Moreover, this study discusses the integration between LCS and LCP, highlighting how combined consideration of these factors can minimize damage costs, improve resiliency, and extend the lifespan of the structure. A detailed evaluation encompasses various life-cycle metrics, structural performance indicators, time-dependent modelling techniques, and analysis methods proposed in the literature. Additionally, the research identifies critical gaps and trends in life-cycle analysis within the realm of bridge engineering, providing a concise yet thorough overview for advancing considerations in the life-cycle design of bridges.}
}

@article{ZHOU2025162778,
title = {Recent advances in biomimetic strategies for flexible battery design and applications},
journal = {Chemical Engineering Journal},
pages = {162778},
year = {2025},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2025.162778},
url = {https://www.sciencedirect.com/science/article/pii/S1385894725036046},
author = {Xiangkai Zhou and Peijian Zhou and Yanzhao Wu and Fucheng Zhang and Ying Wang},
keywords = {Flexible batteries, Bio-inspired design, Morphological bionics, Structural bionics, Functional bionics, Energy storage},
abstract = {Flexible batteries are critical energy sources for flexible electronics, wearable devices, soft robotics, and implantable medical devices. Their unique structural designs and superior energy storage capabilities are driving innovations in traditional energy technologies, yet challenges remain in achieving high energy output and robust mechanical adaptability. Drawing on the complex architectures and sophisticated functions honed by natural evolution, biomimetic strategies have emerged as pivotal in advancing flexible battery design. This review systematically delineates the fundamental operating principles flexible batteries and explores their intrinsic connections to biomimetics. Emphasis is placed on three core approaches—morphological, structural, and functional mimicry—and current manufacturing techniques are comprehensively summarized. Moreover, emerging applications in wearable electronics, healthcare, and smart robotics are critically examined. Finally, challenges associated with attaining high energy density and mechanical stability are analyzed, and future research directions are proposed. This work aims to inspire innovative design paradigms that will facilitate the broader deployment of flexible batteries in advanced technological systems.}
}

@article{LESCHIK2025,
title = {Aversive memory engrams in the hippocampus},
journal = {Brain Organoid and Systems Neuroscience Journal},
year = {2025},
issn = {2949-9216},
doi = {https://doi.org/10.1016/j.bosn.2025.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2949921625000092},
author = {Julia Leschik},
keywords = {hippocampus, memory, engram, fear, stress, neuropsychiatric disorder},
abstract = {Negative episodic memories exert important control of behavioral responses during a real or anticipated threatening situation. Under pathological states, however, this control can extend to non-threatening scenarios. For example, pathological states of aversive memory involve fear-overgeneralization in post-traumatic stress disorder (PTSD) or other anxiety disorders. Furthermore, negative bias in cognitive processing and memory formation is seen in depressed individuals displaying enhanced encoding and recall, less forgetting or repetition of negative memory (rumination) as well as impaired recall of positive memory. Beyond pathological conditions, researchers have long aimed to understand the basic biological entity of memory. This unit termed “engram” is the cellular and molecular component of enduring physiological changes in the brain, enabling learning and memory retrieval. Herein, the hippocampus is central in the formation of context-dependent episodic memories and therefore most often studied in animal experiments to elucidate complex memory traces. In addition, the hippocampus is critically involved in fear-circuits and stress-related dysfunction. This review summarizes current knowledge about memory engrams in hippocampal (sub)regions and their functional relevance regarding neuronal correlates and rodent behavior. A special focus is placed on the negative valence of a memory and the formation of engrams for aversive memories, specifically induced by fear or stress. Finally, limitations of current engram research and possible future directions to improve our understanding of negatively valued memory and its implications in neuropathological conditions will be discussed.}
}

@article{LIU2025106158,
title = {Common Neural Activations of Creativity and Exploration: A Meta-analysis of Task-based fMRI Studies},
journal = {Neuroscience & Biobehavioral Reviews},
pages = {106158},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2025.106158},
url = {https://www.sciencedirect.com/science/article/pii/S0149763425001587},
author = {Yingying Liu and Mengmeng Wang and Hengyi Rao},
keywords = {divergent thinking, convergent thinking, artistic creativity, exploration, meta-analysis, inhibitory control},
abstract = {Creativity is a common, complex, and multifaceted cognitive activity with significant implications for technological progress, social development, and human survival. Understanding the neurocognitive mechanisms underlying creative thought is essential for fostering individual creativity. While previous studies have demonstrated that exploratory behavior positively influences creative performance, few studies investigated the relationship between creativity and exploration at the neural level. To address this gap, we conducted a quantitative meta-analysis comprising 80 creativity experiments (1,850 subjects) and 23 exploration experiments (646 subjects) to examine potential shared neural activations between creativity and exploration. Furthermore, we analyzed the neural similarities and differences among three forms of creative thinking—divergent thinking (DT), convergent thinking (CT), and artistic creativity—and their relationship with exploration. The conjunction analysis of creativity and exploration revealed significant activations in the bilateral IFJ and left preSMA. Further conjunction analyses revealed that both CT and artistic creativity exhibited common neural activations with exploration, with CT co-activating the left IFJ and artistic creativity co-activating both the right IFJ and left preSMA, while DT did not. Additionally, the conjunction analyses across the three forms of creativity did not identify shared neural activations. Further functional decoding analyses of the overlapping brain regions associated with CT and exploration, as well as artistic creativity and exploration, revealed correlations with inhibitory control mechanisms. These results enhance our understanding of the role of exploration in the creative thinking process and provide valuable insights for developing strategies to foster innovative thinking.}
}

@article{SPENCE2025100221,
title = {The emotional depth of flood experience: the role of positive emotions in shaping perceptions and action on climate change},
journal = {Current Research in Ecological and Social Psychology},
pages = {100221},
year = {2025},
issn = {2666-6227},
doi = {https://doi.org/10.1016/j.cresp.2025.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2666622725000085},
author = {Alexa Spence and Charles Ogunbode and Christina Demski and Stuart Capstick},
keywords = {flooding, sustainable behavior, climate change, discrete emotion, affect},
abstract = {Flooding is an ongoing and predicted impact of climate change in many parts of the world. Previous research shows that many people who have experienced flooding exhibit a greater preparedness to act on climate change, especially when the experience relates to more pronounced emotional responses. However, this research has mainly focused on general negative emotional reactions to flooding. Here, we re-analysed a large UK survey dataset (N = 1997) using mixed-methods to examine discrete emotional responses to flooding, including positive emotions, and their relationship with environmental intentions and policy support. Whilst anxiety, anger, helplessness, and distress, dominate people’s experience, positive emotions were also reported as significantly higher in our flooded group, particularly gratitude and pride in response to the receipt of external and community support; surprise was also observed. Thematic analysis highlighted perceived impacts of flooding, and the experience of positive support, as being key to alleviating distress and anxiety, as well as promoting subsequent positive long-term actions to reduce flooding. Notably indirect experience of flooding was also impactful with a range of emotional responses also reported by observers. Regression analysis indicated that higher levels of anxiety, distress, and gratitude were associated with greater intentions to act environmentally in the future (alongside greater levels of anger and lower levels of indifference), and to support for environmental policies (alongside greater levels of sympathy). We suggest that the provision of support following flooding may promote considerations of morality and climate change and increase the likelihood (of both recipients and observers) to undertake pro social and pro-environmental behaviour themselves in the future.}
}

@article{BEBAWY2025125574,
title = {LDLR-targeted orlistat therapeutic nanoparticles: Peptide selection, assembly, characterization, and cell-uptake in breast cancer cell lines},
journal = {International Journal of Pharmaceutics},
volume = {676},
pages = {125574},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.125574},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325004119},
author = {George Bebawy and Pamela Collier and Philip M. Williams and Jonathan C. Burley and David Needham},
keywords = {LDL, LDL receptor, Orlistat, Flash nanoprecipitation, Drug-cored nanoparticles, FAS, Breast cancer, Nanoparticle-uptake kinetics},
abstract = {Motivation
Many cancers overexpress low-density lipoprotein receptors (LDLR), facilitating cholesterol metabolism for tumour growth. Targeting LDLR offers a promising strategy for selective drug delivery. Orlistat, a fatty acid synthase (FAS) inhibitor, has shown anti-cancer potential, particularly in tumours with high FAS expression. This study introduces an LDLR-Orlistat Targeted Nanoparticles (LDLR-OTNs) to enhance cancer cell uptake via LDLR-mediated endocytosis. The objectives include synthesizing lipid-based orlistat nanoparticles, functionalizing them with an 11-mer LDLR-binding peptide, assessing uptake and cytotoxicity in three LDLR- and FAS-expressing breast cancer cell lines (BT-474, MDA MB 453, MCF-7), and comparing uptake kinetics with non-targeted nanoparticles.
Methods
Orlistat nanoparticles (ONs) were synthesised via rapid solvent exchange, producing uncoated ONs, POPC-coated ONs (POPC-ONs), and LDLR-targeted ONs (LDLR-OTNs). Targeting was achieved by conjugating an 11-mer binding peptide (RLTRKRGLKLA) to DSPE-PEG5000 maleimide via click chemistry, confirmed by Ellman’s test. Nanoparticles were characterised using DLS and TEM. Cellular uptake over 24 hours was assessed using fluorescence-labelled POPC-ONs and LDLR-OTNs, and uptake kinetics were analysed. Suramin-blocking studies were used to confirm LDLR-mediated uptake. A 48-hour cytotoxicity assay quantified IC50 values in the aforementioned cell lines.
Results
TEM data showed that LDLR-OTNs (33 nm) were smaller than untargeted POPC-ONs (58 nm) and uncoated ONs (67 nm). Ellman’s test confirmed > 99.2% peptide conjugation. Cellular uptake of LDLR-OTNs was rapid, with significant fluorescence by 1 hour and a kinetic plateau at 24–48 hours, with data fitting to a modified exponential model, while that of untargeted POPC-ONs had lower initial uptake, following a logistic model. Suramin blocking reduced LDLR-OTN uptake, confirming receptor-mediated entry. Cytotoxicity assays yielded IC50 values of 23.8 µM (BT-474), 25.8 µM (MDA MB 453), and 8.2 µM (MCF-7), with maximal inhibition at 48 h.
Conclusions
LDLR-OTNs demonstrated receptor-mediated uptake and potent cytotoxicity in LDLR- and FAS- overexpressing breast cancer cells. These findings support LDLR-targeted nanoparticles as a promising approach for delivering FAS inhibitors to LDLR-rich tumours, meriting further investigation in targeted cancer therapy development.}
}

@article{ALVI2025106356,
title = {Global perspectives on digital twin smart cities: Innovations, challenges, and pathways to a sustainable urban future},
journal = {Sustainable Cities and Society},
volume = {126},
pages = {106356},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106356},
url = {https://www.sciencedirect.com/science/article/pii/S221067072500232X},
author = {Maira Alvi and Hrishikesh Dutta and Roberto Minerva and Noel Crespi and Syed Mohsan Raza and Manoj Herath},
keywords = {Digital twin, Smart city, Urban planning, Internet of Things (ioT), DT applications},
abstract = {The integration of digital twin (DT) technology within smart city frameworks is becoming increasingly significant in today’s digital age. As urban areas evolve into smart ecosystems characterized by interconnected devices and data-driven management systems, digital twins provide a dynamic approach to managing these smart cities. This paper delivers an in-depth exploration of the role of digital twins in advancing smart city frameworks, surveying recent research, methodologies, and case studies that highlight their applicability in urban management, including sectors like mobility, healthcare, energy management, and environmental monitoring. Additionally, the paper delves into the foundational concepts of digital twins and digital twin smart cities, presenting a multilayered structure that emphasizes the role of each layer in creating a comprehensive urban digital twin platform. We also examine the ongoing global digital twin projects based on their development maturity, discuss the challenges of deploying these systems, and explores emerging tools such as synthetic sensing for areas with limited sensor infrastructure. By summarizing the state of the art, identifying research gaps, and discussing future directions, this review serves as a foundational reference for researchers and practitioners seeking to harness digital twin technology for more sustainable, efficient, and resilient smart cities.}
}

@article{WIESE2025100405,
title = {AI ethics education: A systematic literature review},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100405},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100405},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000451},
author = {Lucas J. Wiese and Indira Patil and Daniel S. Schiff and Alejandra J. Magana},
keywords = {AI ethics, Systematic literature review, AI education, Ethics education, Assessment, Measurement},
abstract = {The potential of AI technology to transform human life, well-being, and daily work is faced with numerous risks and challenges yet to be fully accounted for. However, the complexity of AI ethics makes it hard to pin down what to teach, how to teach it, and how to assess its effectiveness. Drawing on an educational perspective, this paper presents a systematic literature review and qualitative analysis of the early years of AI ethics education as a formalized field to analyze whether its future trajectory is aligned with educational best practices. Our review highlights core challenges in AI ethics education and the content, assessment, and pedagogy used in real interventions over recent years. We find that efforts to teach AI ethics do helpfully draw on a holistic view (as opposed to a narrow view), and utilize progressive pedagogies like case studies and group projects that aim to meaningfully challenge students’ ethical reasoning skills in applied practices. However, many real- world AI ethics teaching interventions do not leverage well-supported assessment techniques known to support student learning; rather, assessment is conducted primarily for research evaluative purposes. This gap in rigorous assessment raises implications for researchers and practitioners, as responsible development and use of AI will be stymied if educators cannot successfully determine whether students have truly learned relevant AI ethics content or skills.}
}

@article{GIUBERGIA2025104474,
title = {Modeling spatial and social interdependency effects on commuting mode choice},
journal = {Transportation Research Part A: Policy and Practice},
volume = {196},
pages = {104474},
year = {2025},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2025.104474},
url = {https://www.sciencedirect.com/science/article/pii/S0965856425001028},
author = {Daniele Giubergia and Angela J. Haddad and Francesco Piras and Chandra R. Bhat and Italo Meloni},
keywords = {Social influence, Mode choice, Spatial model, Interdependency effect},
abstract = {In daily life, individuals are influenced by the behaviors of others. The question of how far-reaching this social influence extends to travel behaviors has received significant attention in recent decades, through the capture of dyadic interaction effects that may exist among individuals. Along these lines, in the current study, we apply a Spatial-Attitudinal Probit Model (SAPM) that assumes an autoregressive lag structure in the utilities underlying individuals’ travel mode choice, specifically focusing on the choice between car and public transportation for commuting trips. Notably, the magnitude of interdependency among decision agents is measured by a global weight matrix, accounting for a dual source of influence: (1) spatial proximity, measured as the Euclidean distance between individuals’ residential locations, and (2) attitudinal similarities, specifically perceptions regarding sustainable mobility and environmental awareness. To our knowledge, this represents the first application of an autoregressive travel mode choice model accounting for both geographical and attitudinal proximity as simultaneous sources of interaction. The dataset for our analysis includes 2,347 observations, corresponding to the one-way commute trips of 2,347 individuals, as reported during a survey conducted between October 2019 and January 2020 in the metropolitan area of Cagliari, Italy. Our results reveal the significant role of social autoregressive parameters and the presence of interdependency effects among individuals’ commute mode choices. The utilization of a social lag structure allows for the separate identification of direct and indirect effects of explanatory variables. Notably, around 40% of the total effect is attributed to the indirect effects arising from individuals’ social interdependencies. This finding holds important implications for evaluating and planning potential future measures aimed at increasing public transit usage.}
}

@article{DENG2025100530,
title = {A decision tree algorithm based on adaptive entropy of feature value importance},
journal = {Big Data Research},
pages = {100530},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100530},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000255},
author = {Shaobo Deng and Weili Yuan and Sujie Guan and Xing Lin and Zemin Liao and Min Li},
keywords = {entropy, eigenvalue, decision tree, singular value decomposition},
abstract = {Constructing an optimal decision tree remains a challenging task. Existing algorithms often utilize power coefficient methods or standardization techniques to weight the entropy value; however, these approaches do not sufficiently account for the importance of attributes. This paper introduces an Adaptive Entropy Decision Tree (EWDT) algorithm, which leverages eigenvalue importance and integrates singular value decomposition into the calculation of entropy values. Experimental results demonstrate that the proposed algorithm outperforms other decision tree algorithms in terms of accuracy, precision, recall, and F1-score.}
}

@article{BOADI2025100625,
title = {WOMEN'S ECONOMIC EMPOWERMENT AND POVERTY: SPECIFICATION EMPIRICS EXAMINING GLOBAL DATA},
journal = {Sustainable Futures},
pages = {100625},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.100625},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825001947},
author = {Isaac Boadi and Baba Seidu Adibura and Joseph Opuni-Frimpong and Andrews Ayiku},
keywords = {Women's economic empowerment, Poverty, Entrepreneurship, two-step GMM},
abstract = {Two measures of women's economic empowerment (1) number of female business owners and (2) the number of female sole proprietors are used as women's entrepreneurial surrogates to determine the impact of women's economic empowerment on poverty globally. Using a Fixed-effect model (FEM) and dominance analysis (DA) as an estimation technique, this study sampled carefully ninety-five (95) countries for which data is available for the years 2002 to 2021. These countries are further subdivided into regions for further examinations. Based on the global dataset, the study concludes that the number of female business owners across the study sample significantly contribute to poverty reduction. It is pertinent to emphasize that policy measures that aim to increase the share of female business owners on a worldwide scale should incorporate the poverty threshold. In terms of regional blocks classifications, varied results are produced for both number of female business owners and number of female sole proprietors. This study is anticipated to be valuable in terms of originality since it provides a precise and coherent understanding of the genuine measure on women's economic empowerment that must be placed, from the perspective of global dataset and regional blocks, to reduce global poverty more effectively.}
}

@article{ADRIANVENTURA202595,
title = {Authoritarianism and the brain: Structural MR correlates associated with polarized left- and right-wing ideology traits},
journal = {Neuroscience},
volume = {575},
pages = {95-103},
year = {2025},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2025.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0306452225003045},
author = {Jesús Adrián-Ventura and Diego Avendaño and Anna Miró-Padilla and Anastasia Cherednichenko and César Ávila and Angelo Fasce},
keywords = {Right-wing authoritarianism, Left-wing authoritarianism, structural MRI, Insula, Prefrontal cortex},
abstract = {Authoritarian attitudes across the political spectrum foster radical behaviors, which adversely affect the social fabric. Both left-wing (LWA) and right-wing (RWA) forms of authoritarianism have been described in relation to their psychological correlates, yet little is known about their neurobiological basis. In this study, we explored brain structural correlates (e.g., in cortical thickness (CT) and gray matter (GM) volume) of authoritarianism. For this purpose, we assessed authoritarian dispositions in a sample of 100 young adults and collected 3 T MR images. Images were computed using the CAT12 toolbox. Behaviorally, both the LWA and RWA were positively associated with negative urgency; the LWA also showed a robust positive association with trait anxiety. At the neural level, results showed a negative correlation (r = -0.48) between RWA and a GM volume cluster located in the dorsomedial prefrontal cortex (dmPFC). In addition, we also observed a negative correlation (r = -0.41) between the LWA anti-hierarchical aggression subscale and a CT cluster located in the right anterior insula. Additionally, the resulting clusters converged with further left-wing and right-wing ideology scales related to LWA and RWA, thus providing a robustness check. These results are supported by previous studies showing the relevance of the dmPFC and the anterior insula on social cognition and empathy/inhibitory control, respectively.}
}

@article{VAIGLOVA2025106220,
title = {How can we improve statistical training in archaeological science?},
journal = {Journal of Archaeological Science},
volume = {179},
pages = {106220},
year = {2025},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2025.106220},
url = {https://www.sciencedirect.com/science/article/pii/S030544032500069X},
author = {Petra Vaiglova},
abstract = {The aim of this paper is to shine light on fundamental statistical concepts that archaeologists do not talk about enough. I argue that more deliberate discussion of these statistical ‘elephants in the room’ can have a positive impact on improving statistical training and on steering us away from perpetuation of poor research practices. 1) Statistical thinking should come first. This will help us break down some of the stigma around numbers and statistics, and set us up for building analytical frameworks that will provide the most informative answers to our research questions. 2) Descriptive and inferential statistics have different interpretative potential. This will clarify how we can move from using tools that only allow us to talk about our studied samples to using tools that enable us to draw inferences about the underlying populations from which the samples derived. 3) p values can be extremely variable. This will help spread awareness about the misuses and misconceptions of Null Hypothesis Significance Testing (NHST) and demonstrate the dangers of using significance thresholds to interpret data. 4) Statistical precision is not the same as measurement precision. This will bring attention to the many different types of uncertainties that are built into archaeological datasets (e.g., statistical precision, instrument measurement error, natural variation),.Recognising this is key for drawing reliable inferences from our data. 5) Meta-analyses and forest plots can be useful for synthesising previous research. This will help spread awareness about the benefit of meta-analyses for creating evidence-driven summaries of previous findings. The discussion draws on examples from isotope archaeology, bioarchaeology, and organic residue analysis to illustrate how switching from a reliance on significance testing to a reliance on effect sizes can improve methodological rigour and the representativeness of our findings. The paper ends with a discussion of the roles and responsibilities of supervisors for creating an effective learning environment for statistical training. This includes, but is not limited to, acknowledging the problems of NHST and advocating for adherence to Open Science principles. Ultimately, the changes suggested in this paper will help us raise discipline-wide standards for quantitative training and improve both the breadth and the depth of archaeological research.}
}

@article{SANCHEZGIL2025100957,
title = {Towards an immersive experience in tactical sports: A systematic review and future directions for airsoft and paintball},
journal = {Entertainment Computing},
pages = {100957},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2025.100957},
url = {https://www.sciencedirect.com/science/article/pii/S1875952125000370},
author = {Juan J. Sánchez-Gil},
keywords = {Airsoft, Paintball, Tactical sports, Engineering, New technologies},
abstract = {Airsoft and paintball are tactical sports that combine physical activity, strategy, and teamwork, fostering a growing recreational and competitive culture. However, technological integration in these sports remains underexplored. This study systematically reviews technological advancements over the past 25 years, focusing on safety, interactivity, and automation to enhance gameplay and reduce risks. Key technologies identified include IoT-enabled smart vests, GPS tracking systems, autonomous turrets, and player monitoring. Additionally, an international survey of field owners highlights strong interest in interactive maps, automated scoring, and real-time mission updates, while also revealing barriers such as high costs, lack of technical knowledge, and infrastructure limitations. The analysis underscores the potential of technology to improve game dynamics, operational efficiency, and industry sustainability while addressing challenges like technical feasibility and accessibility. Furthermore, it explores the impact on business models and player engagement. The study provides actionable recommendations for creating immersive, safe, and inclusive environments, laying the foundation for future research on AI-driven automation, augmented reality applications, and adaptive gaming experiences in both recreational and competitive settings.}
}

@article{HORSTKOTTER2025100119,
title = {Decision-making on an AI-supported youth mental health app: A multilogue among ethicists, social scientists, AI-researchers, biomedical engineers, young experiential experts, and psychiatrists},
journal = {Journal of Responsible Technology},
pages = {100119},
year = {2025},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2025.100119},
url = {https://www.sciencedirect.com/science/article/pii/S2666659625000150},
author = {Dorothee Horstkötter and Mariël Kanne and Simona Karbouniaris and Noussair Lazrak and Maria Bulgheroni and Ella Sheltawy and Laura Giani and Margherita {La Gamba} and Esmeralda Ruiz Pujadas and Marina Camacho and Finty Royle and Irene Baggetto and Sinan Gülöksüz and Bart Rutten and Jim {van Os}},
keywords = {AI-supported mental health care, autonomy, biomedicine, co-creation, digital mental health, epistemic pluralism, Ethics, EU-funding, expertise by experience, human-decision making, judicial regulations, multilogue, trans-disciplinary development, youth mental health care},
abstract = {This article explores the decision-making processes in the ongoing development of an AI-supported youth mental health app. Document analysis reveals decisions taken during the grant proposal and funding phase and reflects upon reasons why AI is incorporated in innovative youth mental health care. An innovative multilogue among the transdisciplinary team of researchers, covering AI-experts, biomedical engineers, ethicists, social scientists, psychiatrists and young experts by experience points out which decisions are taken how. This covers i) the role of a biomedical and exposomic understanding of psychiatry as compared to a phenomenological and experiential perspective, ii) the impact and limits of AI-co-creation by young experts by experience and mental health experts, and iii) the different perspectives regarding the impact of AI on autonomy, empowerment and human relationships. The multilogue does not merely highlight different steps taken during human decision-making in AI-development, it also raises awareness about the many complexities, and sometimes contradictions, when engaging in transdisciplinary work, and it points towards ethical challenges of digitalized youth mental health care.}
}

@article{YANG2025103204,
title = {MM-InstructEval: Zero-shot evaluation of (Multimodal) Large Language Models on multimodal reasoning tasks},
journal = {Information Fusion},
pages = {103204},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103204},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525002775},
author = {Xiaocui Yang and Wenfang Wu and Shi Feng and Ming Wang and Daling Wang and Yang Li and Qi Sun and Yifei Zhang and Xiaoming Fu and Soujanya Poria},
keywords = {Multimodal Large Language Models, Multimodal reasoning tasks with vision-text contexts, Various instructions, Zero-shot evaluation, Comprehensive metrics},
abstract = {The emergence of multimodal large language models (MLLMs) has triggered extensive research in model evaluation. While existing evaluation studies primarily focus on unimodal (vision-only) comprehension and reasoning capabilities, they overlook critical assessments of complex multimodal reasoning tasks that require integrated understanding of both visual and textual contexts. Such multimodal tasks present unique challenges, demanding sophisticated reasoning across multiple modalities and deep comprehension of multimodal contexts. In this paper, we present MM-InstructEval, a comprehensive evaluation framework that incorporates diverse metrics to assess model performance across various multimodal reasoning tasks with vision-text contexts. We conduct extensive zero-shot evaluations on 45 models (including 36 MLLMs) across 16 multimodal datasets, encompassing 6 distinct tasks using 10 different instructions. Our framework introduces multiple innovative metrics, including the “Best Performance” metric to benchmark peak model capabilities, the “Mean Relative Gain” metric to assess overall efficacy across models and instructions, the “Stability” metric to measure robustness, and the “Adaptability” metric to quantify the compatibility between models and instructions. Through comprehensive evaluation and analysis, we uncover several significant insights about model architectures, instruction formats, and their interactions in multimodal reasoning tasks. Our findings establish new benchmarks for assessing the reasoning capabilities of MLLMs and provide strategic guidance for future developments. To facilitate continued research and evaluation in this field, we release our framework and resources at https://github.com/declare-lab/MM-InstructEval, with an interactive leaderboard available at MM-InstructEval Leaderboard.}
}

@article{KIM2025102139,
title = {Reward and competition in motivation and learning: Myths, pitfalls, and insights},
journal = {Learning and Instruction},
volume = {98},
pages = {102139},
year = {2025},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2025.102139},
url = {https://www.sciencedirect.com/science/article/pii/S0959475225000635},
author = {Sung-il Kim}
}

@article{XIAO2025200240,
title = {Integrating CNN and RANSAC for improved object recognition in industrial robotics},
journal = {Systems and Soft Computing},
volume = {7},
pages = {200240},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200240},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000584},
author = {Yingding Xiao},
keywords = {CNN, Grasp detection, Industrial robots, ORB, RANSAC, VGG19},
abstract = {This research introduces a robotic grasping system that merges ORB (Oriented FAST and Rotated BRIEF) feature detection, VGG19 convolutional neural networks, and RANSAC (Random Sample Consensus) geometric verification to achieve high-precision object manipulation in unstructured environments. The framework synergizes ORB's efficient, rotation-invariant keypoints with deep semantic features extracted from intermediate layers of VGG19 enabling robust object recognition under occlusions and lighting variations. ORB detects scale-agnostic keypoints and generates binary descriptors, while VGG19’s hierarchical features provide contextual understanding of object geometry. These complementary features are fused into compact descriptors, combining ORB's 256-bit binary patterns with aggregated VGG19 layer outputs to balance accuracy and computational efficiency. RANSAC is then employed to eliminate mismatched features and estimate precise spatial alignments through iterative homography calculations, ensuring reliable mapping between detected objects and the robot's workspace. Experimental validation on industrial dataset trials demonstrates a 99 % grasp success rate, highlighting the system's ability to address challenges in dynamic, cluttered settings. By bridging deep learning's perceptual capabilities with geometric verification, this work advances autonomous robotic systems, offering a scalable solution for industrial automation that prioritizes precision and adaptability.}
}

@article{HUANG2025,
title = {Efficiently optimized multi-fillers for rain gardens: Long-term pollution control performance},
journal = {Water Cycle},
year = {2025},
issn = {2666-4453},
doi = {https://doi.org/10.1016/j.watcyc.2025.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666445325000170},
author = {Tianyin Huang and Zhixin Wang and Yiming Nie and Hanhan Liu and Peirong Li and Jingjing Yang and Bingdang Wu},
keywords = {Rain garden, Analytical Hierarchy Process, Solid waste, Steel slag, Runoff pollution},
abstract = {Abstract:
Rain gardens play a pivotal role in the infiltration and purification of runoff, with the filler layer being a critical component. However, the selection and configuration of fillers in existing rain gardens often lack scientific rigor, leading to suboptimal performance. To address this issue, this study employs the Analytic Hierarchy Process (AHP) to systematically evaluate 11 types of fillers, including 5 natural materials, 3 industrial wastes, and 3 artificial materials, aiming to optimize filler configurations and enhance the pollutant removal efficiency of rain gardens. The results demonstrate that steel slag, coconut shell biochar, green zeolite, and fly ash ceramic granules exhibit superior performance in purifying both simulated and actual runoff. The optimal filler combination, comprising 25% green zeolite, 25% steel slag, and 50% coconut shell biochar, achieved removal rates of 50.49%, 76.12%, 44.12%, 89.94%, 58.38%, and 88.19% for COD, NH4+-N, TP, Cu(II), Cr(VI), and tetracycline, respectively. Long-term operational evaluation (>110 days) revealed that the optimized filler layer significantly improved the removal rates of COD, TN, and TP to 86.83%, 80.19%, and 88.42%, respectively. By comparing the physicochemical properties of the fillers before and after use, the mechanisms underlying runoff purification were preliminarily elucidated. Different fillers exhibited specific adsorption capabilities for distinct pollutants, and the synergistic effects of multiple fillers significantly enhanced the rain garden's capacity for source pollutant reduction. AHP was used in this study to validate the scientific validity of AHP in the device of rain gardens through filler combination adsorption experiments and long-term monitoring of rain garden installations, while concurrently offering a broader range of green solutions for the enhancement of rain gardens.}
}

@book{10.1145/3596711,
editor = {Whitton, Mary C.},
title = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {Volume 2},
abstract = {When we began planning how to celebrate 50 years of SIGGRAPH Conferences, there was unanimous agreement that one of the projects should be publishing a second volume of Seminal Graphics Papers. The first volume was published in 1998 as part of the celebration of the 25th SIGGRAPH conference. Seminal Graphics Papers Volume 2, perhaps more than any other activity undertaken in this milestone year, celebrates ACM SIGGRAPH's origins and continued success as a Technical and Professional Society. This collection of papers typifies the ground-breaking research that has been the conference's hallmark since 1974. A quick scan of the chapter and the paper titles shows just how far SIGGRAPH research has pushed the boundaries of our discipline and contributed to its evolution.The ACM Digital Library team has been supportive of this Seminal Graphics Papers project from the beginning. I am pleased to let you know that both Volumes 1 and 2 of Seminal Graphics Papers are freely available from the ACM Digital Library at these URLs: Volume 1: https://dl.acm.org/doi/book/10.1145/280811Volume 2: https://dl.acm.org/doi/book/10.1145/3596711}
}


@book{10.1145/280811,
editor = {Wolfe, Rosalee},
title = {Seminal graphics: pioneering efforts that shaped the field, Volume 1},
year = {1998},
isbn = {158113052X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {Volume 1}
}


@article{10.1145/3065386,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}


@proceedings{10.1145/3025453,
title = {CHI '17: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHI 2017 is the premier international conference for the field of Human-Computer Interaction (HCI). This year it was held in Denver, bordering the beautiful Rocky Mountain region in the U.S., and reflected in our logo. The CHI 2017 conference began with two days of workshops and symposia, followed by four days of a technical program with 17 parallel sessions of provocative papers, panels, case studies, SIGs (Special Interest Groups), courses, and the popular student research, design, and game competitions. The growing alt.chi forum, now in its twelfth year, presented stimulating new ideas in HCI. The Interactivity forum showcased cutting-edge technology. For its second year, the CHI Art Exhibit merged art and technology in fascinating ways.This innovative work can be found in the Proceedings and Extended Abstracts, archived in the ACM Digital Library. For papers, the conference received 2400 submissions which were rigorously reviewed, resulting in 600 accepted papers. To ensure a better fit with reviewers, new subcommittees were created. Across all tracks, CHI received nearly 5000 submissions and accepted over 1000.Our conference theme this year, Explore, Innovate, Inspire, informed our planning process. A new venue held this year was CHI Stories. We generally know little of the personalities that drive the research presented at CHI. CHI Stories is a chance for CHI community members to share personal stories of inspiration, challenge, successes and failures, and grit. We also focused on inclusion this year. Hundreds of CHI attendees volunteered their skills in a Day of Service partnering with non-profit organizations. Inclusion was also manifest throughout the conference, for example, in the Diversity and Inclusion Lunch and by using telepresence robots to enable people with disabilities to participate remotely in the conference. Our keynote speakers were chosen to reflect our conference theme. The speakers were Neri Oxman, who combines computational design, digital fabrication, materials science and synthetic biology; Ben Shneiderman a founder of the CHI conference who, along with some key CHI personalities, gave a perspective on CHI's history and future; Wael Ghonim, credited with starting the Arab Spring and nominated for the Nobel Peace Prize; and best-selling author Nicholas Carr who challenges us to examine the unforeseen impacts of technology, particularly with automation.The world has experienced a dramatic change this past year. We live in extraordinary times and this calls for extraordinary thinking, something that the CHI community excels at. One of the challenges the community faced this year was responding to a U.S. executive order to ban citizens of certain countries from entering the country to attend CHI. As CHI is committed to inclusion, we decided to hold events at the conference to discuss and plan how we can continue our commitment to inclusion. The conference held a panel to discuss impacts of current political events on science, and hastily organized a panel to promote a conversation of civil liberties in science and a SIG on how the CHI community can participate in change. We are proud to have expanded telepresence options through the use of robots to enable people to participate in the conference remotely if they were physically unable to enter the country. We had a keynote speaker who inspired us on the topic of Internet activism. Our art exhibit, I'll Be Watching You, examined the contemporary issue of surveillance.},
location = {Denver, Colorado, USA}
}


@proceedings{10.1145/1390156,
title = {ICML '08: Proceedings of the 25th international conference on Machine learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the papers accepted to the 25th International Conference on Machine Learning (ICML 2008). ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at http://www.machinelearning.org.This year, ICML was held July 5..9 at the University of Helsinki, in Helsinki, Finland, and was co-located with COLT-2008, the 21st Annual Conference on Computational Learning Theory, and UAI-2008, the 24th Conference on Uncertainty in Artificial Intelligence. No less than 583 papers were submitted to ICML 2008. There was a very thorough review process, in which each paper was reviewed double-blind by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. There were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Some papers were checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 158 papers were accepted to ICML this year, including a small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 27\%.ICML authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included one or two invited talks by a prominent researcher. We were very fortunate to be able to host Michael Collins, of the Massachusetts Institute of Technology; Andrew Ng, of Stanford University; and Luc De Raedt, of the Katholieke Universiteit Leuven, and John Winn of Microsoft Research Cambridge. In addition to the technical talks, ICML- 2008 also included nine tutorials held before the main conference, presented by Alex Smola, Arthur Gretton, and Kenji Fukumizu; Bert Kappen and Marc Toussaint; Neil Lawrence; MartinWainwright; Ralf Herbrich and Thore Graepel; Andreas Krause and Carlos Guestrin; Shai Shalev-Shwartz and Yoram Singer; Rob Fergus; and Matthias Seeger. This year our workshops were organized jointly with COLT and UAI as part of a special "overlap day," consisting of eleven workshops selected and arranged collaboratively by the respective workshop chairs of the three conferences. This day provided a rich opportunity for interaction among the attendees of the conferences.This year, ICML enlarged its award offerings to match several other well-established conferences. We hope these will help build our community, celebrate our advances, and encourage applications and long-term thinking. In addition to our previously traditional "Best Paper" and "Best Student Paper" awards, we also gave awards for "Best Application Paper" and "10-year Best Paper" (for the best paper of ICML 1998, optionally given in conjunction with a co-located conference). We thank the Machine Learning Journal for sponsoring some of our paper awards.},
location = {Helsinki, Finland}
}


@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}


@proceedings{10.1145/775047,
title = {KDD '02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The KDD 2002 conference, held from 23rd to 26th July 2002, was the eighth in the series. It represented a return to the country in which the series was launched: the first was held in Montreal, Canada, and this, the eighth, was held in Edmonton, Canada. In the years between the first conference in the series and this present one, data mining has be, come a well-established discipline. It has continued to strengthen its links to other data analytic disciplines, including statistics, machine learning, pattern recognition, visualization, and database technology, but has now clearly carved out a niche of its own. Over the period in which this series has been running, hardware technology has continued to advance in great leaps, with the result that large databases have continued to grow in both number and size. The implication is that the challenge of data mining is even more important, that the problems requiring data mining solutions are ever more ubiquitous, and that new tools and methods for tackling are even more necessary.KDD 2002 received a record number of submitted papers - 307 in total, 37 of which were considered for the industral/applicafion track. Among the 270 research submissions, 32 were selected (12\%) for full papers; and among the 37 industrial/application submissions, 12 (32\%) were selected for full papers. An additional 44 submissions were chosen to be presented as posters, a vast majority of which were research submissions. This low rate of acceptance reflects a conscious effort to maintain the very high standards of quality and relevance, which have been achieved by previous conferences in the series. It means that the papers and posters in the proceedings represent the cutting edge of data mining problemsl solutions, and technology. On the other hand, this policy inevitably meant that many excellent contributions did not make it to the final program. The choice had to be informed by balance as well as quality - KDD 2002 had to showcase research in data mining across the entire frontier of the discipline. This breadth was reflected in the choice of invited speakers, both well known in the data mining; community, but from different backgrounds: Daryl Pregibon and Geoff Hinton. The program also includes 6 workshops in such diverse areas as 'Data Mining in Bioinformatics', 'Web Mining', 'Multimedia Data Mining', 'Multi-Relational Data Mining', 'Temporal Data Mining', and 'Fractals in Data Mining' as well as 6 tutorials on 'Text Mining for Bioinformatics', 'Querying and Mining Data Streams', 'Link Analysis', 'Multivariate Density Estimation', 'Common Reasons Data Mining Projects Fail', and 'Visual Data Mining'.},
location = {Edmonton, Alberta, Canada}
}


@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}


@proceedings{10.1145/2339530,
title = {KDD '12: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The KDD conference has seen remarkable growth since its origins as an IJCAI workshop in Detroit in 1989, evolving into a full-fledged research conference in 1995, underscoring the important role data mining as a field has played in extracting knowledge and actionable insights from vast troves of data that is being generated in the digital world around us. This year we received a record 755 submissions to the research program, from which 133 papers were accepted, for an aggregate acceptance rate of 17.6\% (quite similar to recent years).Among the academic conferences, the KDD conference has typically more of an emphasis on research motivated by real-world applications. It is important to keep in mind that it is this synergy of research in areas like algorithms, computational geometry, database, graph theory, machine learning, natural language processing, statistics, visualization and many others when applied to problems arising in diverse fields such as web, medicine, climatology, marketing that drives our field forward, makes it vibrant and fun - who would know that ideas in computational geometry can be adapted to construct fast algorithms to improve online advertising and movie recommendations?The breadth of topics covered in this year's research program is truly comprehensive, including social networks, privacy, text mining, predictive modeling, time-series forecasting, spatial data analysis, geometry, and more. We are very fortunate to have 4 world-class keynote speakers this year spanning industry and academia, providing inspirational talks on cutting-edge techniques and issues in web mining, information networks, statistical inference for big data, and social computing.The process of whittling down the initial 734 submissions to the final set of 133 accepted papers required the coordination and time of a large number of willing volunteers. The program committee (PC) consisted of over 350 reviewers (PC members) and 50 senior PC members. In the first phase each submitted paper was automatically assigned to 3 reviewers (after a bidding process). Once the reviews from each of the 3 reviewers were completed, the program chairs rejected papers that did not receive much support from any of the reviewers. We rejected 259 papers at this stage. Special care was taken to minimize the error of rejecting a potentially good paper at this stage. The papers that survived the first phase were assigned to the senior PC members based on their bids, they had the option of initiating a discussion for any of their papers, e.g., if there was significant divergence in scores among reviewers, or if a paper was on the borderline of being accepted. Following the discussion phase, the senior PC members provided a recommendation score and a detailed meta-review for each paper. In the final phase, we (the program chairs) analyzed all of this information, starting with the obvious accept and reject decisions, and then gradually focusing in more detail on the papers near the borderline, seeking additional reviews and input from the PC and senior PC members where appropriate. We also initiated a shepherding phase with 15 papers having the opportunity of fixing mild issues we thought would be possible to address before they can be accepted. 13 of them were accepted after thorough revisions. Finally, it is quite likely that in hindsight some worthy papers may have been rejected as part of this process - these errors are an unfortunate reality of modern computer science conferences, and hard to avoid when a very large number of decisions have to be made over a short time span based on a subjective reviewing process. Nevertheless, we, the PC chairs, are responsible for those unfortunate errors and welcome suggestions on the matter.},
location = {Beijing, China}
}


@proceedings{10.1145/2487575,
title = {KDD '13: Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 19th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). The annual ACM SIGKDD conference is the premier international forum for data mining, knowledge discovery and big data. It brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD-2013 features plenary presentations, paper presentations, poster sessions, workshops, tutorials, exhibits, demonstrations, and the KDD Cup competition.Today, you hear a lot about big data, data science and data intensive computing. The core of this work is extracting knowledge and useful information from data, which for science leads to beautiful insights, and for applications leads to actions, alerts and decisions. The KDD community has always been at the center of this activity and it is clear from this conference that it will continue to drive this broader field of big data.This year there were 726 submissions to the KDD Research Track, and 125 papers were accepted. There were 136 submissions to the KDD Industry and Government Track, and 34 papers were accepted.KDD also has a history of inviting talks that are of broad interest to the KDD community. This year we chose to have 4 plenary talks. A program committee also selected 8 talks to present at the Industry Practice Exposition.A strength of the KDD conference is the number of workshops and tutorials that are co-located with it. This year there were 10 full-day workshops, 5 half-day workshops, and 6 tutorials.We thank all sponsors, who are a very important part of the conference, and the members of the Organizing Committee and our other colleagues who volunteered their time during the past year to make this conference a success. Special thanks goes to the Research Track Co-Chairs and the Industry and Government Track Co-Chairs. Also special thanks are due to the Local Arrangements Chair, the Treasurer, the Proceedings Co-Chairs, and the KDD Cup Committee.We are grateful to the several program committees that provided the advice necessary to put together a quality program - the Research Track Program Committee, the Research Track Senior Program Committee, the Industry and Government Track Program Committee, the Industry Practice Expo Program Committee, the Workshop Program Committee, the Tutorial Program Committee, and the Demo Program Committee.We know that you will find this year's exhibits and demonstrations exciting and remind you that some of the most interesting discussions can be found there.Please join us for KDD-2013 to gain new knowledge and to exchange exciting new research results, leading practices, and high impact applications in big data, knowledge discovery and data mining. We hope that you will find this program interesting and thought-provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
location = {Chicago, Illinois, USA}
}


@proceedings{10.1145/1102351,
title = {ICML '05: Proceedings of the 22nd international conference on Machine learning},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume, which is also available online from
http://www.machinelearning.org, contains the papers accepted for
presentation at ICML-2005, the 22nd lnternational Conference on
Machine Learning, which was held at the University of Bonn in
Germany from August 7 to August 11, 2005. ICML is the annual
conference of the lnternational Machine Learning Society (IMLS),
and forms an international forum for the discussion and
presentation of the latest results in the field of machine
learning. This year, ICML was co-located with the 15th
lnternational Conference on Inductive Logic Programming (ILP-2005),
the proceedings of which are published by Springer Verlag in a
separate volume.The papers in this volume were selected on the basis of a
thorough review process. In the first round of reviewing, three
program committee members produced individual reviews for a paper.
Authors then had the opportunity to view those reviews and submit
an author's reply to the reviewers. Led by the responsible area
chair, the reviewers then engaged in a discussion about the paper,
ultimately leading to the decision by the program chairs. In sum,
of the 491 papers that were initially submitted, 62 were accepted
immediately, and a further 81 were conditionally accepted and
reconsidered after resubmission in a second round of reviewing. Of
those 81 conditionally accepted papers, 72 were finally accepted,
leading to a total of 134 accepted papers, which translates into an
acceptance rate of 27.3 \%. The author reply was a new feature of
ICML this year, while the option of working with conditional
accepts has already become a tradition.In addition to the presentations of the accepted papers, the
ICML program included several other features. On the first and last
day of the conference, 11 workshops and 6 tutorials on current
topics of machine learning were held. For many of these,
proceedings and/or presentation materials are available online from
the ICML website. The other days of the conference each featured an
invited talk by a prominent researcher as a program highlight. We
were delighted that Johannes Gehrke of Cornell University, Michael
Jordan of the University of California at Berkeley, and Gerhard
Widmer of the University of Linz in Austria, agreed to deliver an
invited talk. The abstracts of their talks are also published as
part of these proceedings.Continuing a long standing tradition at ICML, all papers
presented in a talk at the conference were also exhibited at
evening poster sessions, giving everyone ample time to discuss the
results in depth. In order to emphasize the co-location with
ILP-2005, the program contained joint elements in both invited
speakers, paper sessions, poster sessions, and tutorials. As usual,
the scientific program was complemented by a social program, this
time featuring an excursion to the scenic surroundings of the city
of Bonn.During the conference best paper and best student paper awards
were presented, the former being sponsored by NICTA, the later by
the Machine Learning Journal.},
location = {Bonn, Germany}
}


@article{10.1145/359340.359342,
author = {Rivest, R. L. and Shamir, A. and Adleman, L.},
title = {A method for obtaining digital signatures and public-key cryptosystems},
year = {1978},
issue_date = {Feb. 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/359340.359342},
doi = {10.1145/359340.359342},
abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.},
journal = {Commun. ACM},
month = feb,
pages = {120–126},
numpages = {7},
keywords = {authentication, cryptography, digital signatures, electronic funds transfer, electronic mail, factorization, message-passing, prime number, privacy, public-key cryptosystems, security}
}


@inproceedings{10.1145/170035.170072,
author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
title = {Mining association rules between sets of items in large databases},
year = {1993},
isbn = {0897915925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170035.170072},
doi = {10.1145/170035.170072},
abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
booktitle = {Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data},
pages = {207–216},
numpages = {10},
location = {Washington, D.C., USA},
series = {SIGMOD '93}
}


@article{10.1145/170036.170072,
author = {Agrawal, Rakesh and Imieli\'{n}ski, Tomasz and Swami, Arun},
title = {Mining association rules between sets of items in large databases},
year = {1993},
issue_date = {June 1, 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/170036.170072},
doi = {10.1145/170036.170072},
abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
journal = {SIGMOD Rec.},
month = jun,
pages = {207–216},
numpages = {10}
}


@article{10.1145/1327452.1327492,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
title = {MapReduce: simplified data processing on large clusters},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327492},
doi = {10.1145/1327452.1327492},
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
journal = {Commun. ACM},
month = jan,
pages = {107–113},
numpages = {7}
}


@article{10.1145/331499.331504,
author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
title = {Data clustering: a review},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/331499.331504},
doi = {10.1145/331499.331504},
abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview
of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify
 cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
journal = {ACM Comput. Surv.},
month = sep,
pages = {264–323},
numpages = {60},
keywords = {cluster analysis, clustering applications, exploratory data analysis, incremental clustering, similarity indices, unsupervised learning}
}


@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}


@article{10.1145/3422622,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative adversarial networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422622},
doi = {10.1145/3422622},
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
journal = {Commun. ACM},
month = oct,
pages = {139–144},
numpages = {6}
}


@proceedings{10.1145/2723372,
title = {SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, "The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, "From Data to Insights @ Bare Metal Speed," explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, "Three Favorite Results," on Tuesday. Christopher R\'{e} will lead a panel on "Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?," with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5\%, and we believe that the revision processhas improved the quality of the technical program.},
location = {Melbourne, Victoria, Australia}
}


@inproceedings{10.1145/2939672.2939778,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@article{10.5555/944919.944937,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent dirichlet allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}


@proceedings{10.1145/1081870,
title = {KDD '05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining},
year = {2005},
isbn = {159593135X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining -- KDD'05. The KDD conferences provide a forum for novel research results and interesting applications in the areas of data mining and knowledge discovery. The conference series gives researchers and practitioners a unique opportunity to share their perspectives with others and to present new research ideas, applications, solutions, tools, systems, and research directions broadly related to knowledge discovery and data mining.The call for papers attracted 465 research submissions and 73 industrial submissions from around the world. The program committee accepted 40 research papers, 36 research posters, 14 industrial papers and 11 industrial posters. In addition, this year's conference includes three plenary talks and two panels.Putting together KDD'05 was a team effort. First, we would like to thank the authors for providing the content of the program and the program committee and external reviewers, who worked very hard in reviewing papers and providing suggestions for their improvements. Second, we would like to thank the Organizing Committee, who also worked very hard, and, as volunteers, didn't get much for it. I strongly encourage you to thank them, and even, perhaps, to let them get out of the elevator ahead of you. Finally, we would like to thank our sponsor, ACM SIGKDD, for their continued support.Daniel Hudson Burnham (1846-1912) was a partner in the Chicago based architecture firm Burnham and Root. Burnham and Root created the foundation for the modern skyscraper by using a floating foundation of cement that provided a stable foundation even when, as in many Chicago locations, it was not possible to reach bedrock. Burnham and Root was also the lead architect for, and in charge of, construction for the World Columbian Exposition (1893), which celebrated the 400th anniversary of the arrival of Columbus to North America. The World Columbian Exposition was the largest World's Fair to that date and drew 27.5 million attendees at a time when the US population was about 65 million. To achieve this, many logistic obstacles had to be overcome. Burnham's style is nicely captured by a quote associated with him: "Make no little plans. They have no magic to strike man's blood and probably will themselves not be realized.This is the theme we have chosen for this year's KDD Conference. The field of data mining and knowledge discovery is over ten years old, and is now mature enough to begin to make some big plans and to tackle some very difficult problems and challenges. We will begin discussions about these challenges at this year's conference and continue them throughout the year. Over the coming year, please look to the SIGKDD Explorations for further information.We hope that you will find this program interesting and thought provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.},
location = {Chicago, Illinois, USA}
}


@article{10.1145/1541880.1541882,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
title = {Anomaly detection: A survey},
year = {2009},
issue_date = {July 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1541880.1541882},
doi = {10.1145/1541880.1541882},
abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {15},
numpages = {58},
keywords = {Anomaly detection, outlier detection}
}


@article{10.5555/1953048.2078195,
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
title = {Scikit-learn: Machine Learning in Python},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {2825–2830},
numpages = {6}
}


@proceedings{10.1145/564691,
title = {SIGMOD '02: Proceedings of the 2002 ACM SIGMOD international conference on Management of data},
year = {2002},
isbn = {1581134975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 2002 ACM SIGMOD International Conference on Management of Data, was held June 4-6, 2002 at the spectacular Frank Lloyd Wright-designed Monona Terrace conference center in Madison, Wisconsin. The SIGMOD conference has long held its status a leading forum for database researchers, practitioners, developers, and users to explore cutting-edge ideas and to exchange techniques, tools, and experiences. This year we are pleased to have a particularly strong program, including significant representation from both industry and academia and contributions from around the world. The SIGMOD conference is sponsored by the Association for Computing Machinery (ACM) and its Special Interest Group on Management of Data (SIGMOD). As has become customary, two days of the conference were overlapped with the Symposium on Principles of Database Systems (PODS). In addition, there were a number of important events co-located with SIGMOD this year, including the 5th International Workshop on the Web and Databases (WebDB), the Workshop on Research Issues in Data Mining and Knowledge Discovery (DMKD), and the New Database Faculty Symposium.As with previous years, acceptance into the conference proceedings was extremely competitive. From the 240 research program submissions, the program committee selected 42 papers for presentation and inclusion in the proceedings. These papers span the range of traditional database topics as well as issues of emerging interest such as data streaming, middle-tier data management, and integration with web services and the WWW. The: program committee worked hard to select these papers through a detailed review process and active discussion both electronically and at the program committee meeting held in January at Berkeley. We are deeply indebted to Surajit Chaudhuri and Jonathan Simon for their efforts in developing and hosting the Conference Management Tool at Microsoft. We used the CMT to run the entire submissions and review process for research papers, including the program committee meeting. We would also like to thank Umesh Dayal, who took on extra work to help manage the review process.In addition to the research track, we had 41 submissions to the Demonstrations track, of which 21 projects were invited to present. The Demonstrations track has become a key venue for the early dissemination of cutting-edge prototype and systems development experience. Paul Aoki chaired the demos program committee, which put together a diverse and exciting program. The Industrial program committee, chaired by Mike Carey took an active role in soliciting quality submissions and have put together a program that is a key component of this year's conference. Likewise, Donald Kossmann and Peter Scheuermann and the tutorial committee organized an excellent slate of Panels and Tutorials to round out the program.},
location = {Madison, Wisconsin}
}


@inproceedings{10.1145/2939672.2939754,
author = {Grover, Aditya and Leskovec, Jure},
title = {node2vec: Scalable Feature Learning for Networks},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939754},
doi = {10.1145/2939672.2939754},
abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {855–864},
numpages = {10},
keywords = {feature learning, graph representations, information networks, node embeddings},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@inproceedings{10.1145/2623330.2623732,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: online learning of social representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {deep learning, latent representations, learning with partial labels, network classification, online learning, social networks},
location = {New York, New York, USA},
series = {KDD '14}
}


@inproceedings{10.1145/130385.130401,
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
title = {A training algorithm for optimal margin classifiers},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130385.130401},
doi = {10.1145/130385.130401},
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {144–152},
numpages = {9},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}


@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI \&amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}


@inproceedings{10.1145/383059.383071,
author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
title = {Chord: A scalable peer-to-peer lookup service for internet applications},
year = {2001},
isbn = {1581134118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383059.383071},
doi = {10.1145/383059.383071},
abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
booktitle = {Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications},
pages = {149–160},
numpages = {12},
location = {San Diego, California, USA},
series = {SIGCOMM '01}
}


@article{10.1145/964723.383071,
author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
title = {Chord: A scalable peer-to-peer lookup service for internet applications},
year = {2001},
issue_date = {October 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/964723.383071},
doi = {10.1145/964723.383071},
abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {149–160},
numpages = {12}
}


@article{10.1145/359545.359563,
author = {Lamport, Leslie},
title = {Time, clocks, and the ordering of events in a distributed system},
year = {1978},
issue_date = {July 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/359545.359563},
doi = {10.1145/359545.359563},
abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
journal = {Commun. ACM},
month = jul,
pages = {558–565},
numpages = {8},
keywords = {clock synchronization, computer networks, distributed systems, multiprocess systems}
}


@article{10.1145/1721654.1721672,
author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy and Konwinski, Andy and Lee, Gunho and Patterson, David and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
title = {A view of cloud computing},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1721654.1721672},
doi = {10.1145/1721654.1721672},
abstract = {Clearing the clouds away from the true potential and obstacles posed by this computing capability.},
journal = {Commun. ACM},
month = apr,
pages = {50–58},
numpages = {9}
}


@inproceedings{10.1145/37401.37406,
author = {Reynolds, Craig W.},
title = {Flocks, herds and schools: A distributed behavioral model},
year = {1987},
isbn = {0897912276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/37401.37406},
doi = {10.1145/37401.37406},
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
booktitle = {Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {25–34},
numpages = {10},
series = {SIGGRAPH '87}
}


@article{10.1145/37402.37406,
author = {Reynolds, Craig W.},
title = {Flocks, herds and schools: A distributed behavioral model},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {0097-8930},
url = {https://doi.org/10.1145/37402.37406},
doi = {10.1145/37402.37406},
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
journal = {SIGGRAPH Comput. Graph.},
month = aug,
pages = {25–34},
numpages = {10}
}


@inbook{10.1145/280811.281008,
author = {Reynolds, Craig W.},
title = {Flocks, herds, and schools: a distributed behavioral model},
year = {1998},
isbn = {158113052X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/280811.281008},
abstract = {The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the "animator." The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.},
booktitle = {Seminal Graphics: Pioneering Efforts That Shaped the Field, Volume 1},
pages = {273–282},
numpages = {10}
}


@inproceedings{10.1145/371920.372071,
author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
title = {Item-based collaborative filtering recommendation algorithms},
year = {2001},
isbn = {1581133480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/371920.372071},
doi = {10.1145/371920.372071},
booktitle = {Proceedings of the 10th International Conference on World Wide Web},
pages = {285–295},
numpages = {11},
location = {Hong Kong, Hong Kong},
series = {WWW '01}
}


@proceedings{10.1145/509907,
title = {STOC '02: Proceedings of the thiry-fourth annual ACM symposium on Theory of computing},
year = {2002},
isbn = {1581134959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The papers in this volume were presented at the Thirty-Fourth Annual ACM Symposium on Theory of Computing (STOC2002), held in Montreal, Quebec, Canada, May 19-21, 2002. The Symposium was sponsored by the ACM Special Interest Group on Algorithms and Computation Theory (SIGACT).In response to a call for papers, 287 paper submissions were received. All were submitted electronically. The program committee conducted its deliberations electronically, via an on-line meeting that ran from January 10 to January 19. The committee selected 91 papers from among the submissions. The submissions were not refereed, and many of these papers represented reports of continuing research. It is expected that most of them will appear in a more polished and complete form in scientific journals.The papers encompassed in wide variety of areas of theoretical computer science. The topics included algorithms and computational complexity bounds for classical problems in algebra, geometry, topology, graph theory, game theory, logic and machine learning, as well as theoretical aspects of security, databases, information retrieval, and networks, the web, computational biology, and alternative models of computation including quantum computation and self-assembly.},
location = {Montreal, Quebec, Canada}
}


@article{10.1145/324133.324140,
author = {Kleinberg, Jon M.},
title = {Authoritative sources in a hyperlinked environment},
year = {1999},
issue_date = {Sept. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/324133.324140},
doi = {10.1145/324133.324140},
abstract = {The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in  the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.},
journal = {J. ACM},
month = sep,
pages = {604–632},
numpages = {29},
keywords = {World Wide Web, graph algorithms, hypertext structure, link analysis}
}


@inproceedings{10.1145/342009.335388,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: identifying density-based local outliers},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335388},
doi = {10.1145/342009.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {93–104},
numpages = {12},
keywords = {database mining, outlier detection},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}


@article{10.1145/335191.335388,
author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J\"{o}rg},
title = {LOF: identifying density-based local outliers},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335388},
doi = {10.1145/335191.335388},
abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
journal = {SIGMOD Rec.},
month = may,
pages = {93–104},
numpages = {12},
keywords = {database mining, outlier detection}
}


@proceedings{10.1145/2517349,
title = {SOSP '13: Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
year = {2013},
isbn = {9781450323888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP 2013), held at the Nemacolin Woodlands Resort, Farmington, Pennsylvania, USA. This year's program includes 30 papers, and touches on a wide range of computer systems topics, from kernels to big data, from responsiveness to correctness, and from devices to data centers. The program committee made every effort to identify and include some of the most creative and thought-provoking ideas in computer systems today. Each accepted paper was shepherded by a program committee member to make sure the papers are as readable and complete as possible. We hope you will enjoy the program as much as we did in selecting it.},
location = {Farminton, Pennsylvania}
}


@article{10.5555/2627435.2670313,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, neural networks, regularization}
}


@inproceedings{10.1145/956750.956769,
author = {Kempe, David and Kleinberg, Jon and Tardos, \'{E}va},
title = {Maximizing the spread of influence through a social network},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956769},
doi = {10.1145/956750.956769},
abstract = {Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of "word of mouth" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63\% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {137–146},
numpages = {10},
keywords = {approximation algorithms, diffusion of innovations, social networks, viral marketing},
location = {Washington, D.C.},
series = {KDD '03}
}


@article{10.1145/182.358434,
author = {Allen, James F.},
title = {Maintaining knowledge about temporal intervals},
year = {1983},
issue_date = {Nov. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/182.358434},
doi = {10.1145/182.358434},
journal = {Commun. ACM},
month = nov,
pages = {832–843},
numpages = {12},
keywords = {interval reasoning, interval representation, temporal interval}
}


@proceedings{10.1145/1082473,
title = {AAMAS '05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems},
year = {2005},
isbn = {1595930930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Autonomous Agents and Multi-Agent Systems are, in the judgment
of many observers, the most significant new paradigm for software
modelling and development to emerge from Computer Science in the
last two decades. Autonomous Agents are computer programs that are
able to decide between different methods of achieving their
programmed goals. This is in contrast to the scripted, predefined
behaviour that a traditional program, for example a Unix Daemon,
would exhibit, and means that Autonomous Agents can cope with
dynamic environments, can be developed as a quick solution to
complex problems, and can tailor their behaviour to individual
users in a way that traditional software cannot.Multi-Agent Systems are computational environments in which
different programs, possibly developed in isolation, can
co-ordinate their behaviours to achieve their goals. The technology
of Multi-Agent Systems is, therefore, particularly applicable to
modern operational environments like e-business, ubiquitous
computing, and, of course, the Internet.These two topics of research are tightly coupled. For a
Multi-Agent System to be fully implemented, it is often necessary
to utilize the technology of Autonomous Agents. For an Agent to act
with true autonomy in a realistic, modern setting, it must often be
able to reason about other Agents and co-ordinate its behaviour
with them. It is this insight that has led to the development of
the AAMAS community and the spectacularly successful series of
conferences that have been running for the last for years.The AAMAS community has long recognised [Wooldridge &amp;
Jennings 1995] that the significance of the abstractions,
techniques, tools, and technologies developed by the research
community worldwide will only be recognised if they have practical
applications in the real world of science, technology, education,
healthcare, business, and commerce. The paradigm of AAMAS and
available agent technologies has reached a significant level of
maturity, and they are now widely regarded as ready for wider
adoption. The purpose of the First Industry Track of the Autonomous
Agent and Multi-Agent Systems conference is to provide a forum that
will bring real world applications that are being developed by
teams internationally to the attention of the AAMAS community and
the wider world.Why is this important? We have identified several
motivations:•By disseminating the news of successful application of
AAMAS technology we hope to provide researchers with clear evidence
of the success and usefulness of particular techniques.•Feedback from attempts to use AAMAS technology is
valuable in the formation of the AAMAS research agenda.•A forum for discussion of industrial and
application-orientated concerns has been lacking in the community
to date; by providing one, we hope that practitioners will be able
to share their experiences and, therefore, accelerate the uptake of
AAMAS technology.•Finally, we expect that, by providing an industry
track, we will encourage more participation in AAMAS by
practitioners, and that this will lead to a wider understanding of
the importance of AAMAS technology in commercial organisations.Fifteen papers that discuss the application of AAMAS technology
are presented in this collection. They were selected by a
refereeing process that aimed to find work that made use of the
particular properties of AAMAS systems and had actually been used
"in anger." The ratio of submitted to accepted papers was higher
than 2:1; that is, more papers were excluded from presentation than
were accepted.The papers that have been selected by the referees and programme
committee for the track allow the identification of some trends in
the development and application of AAMAS technology. Four out of
fifteen papers are in the field of transport, traffic, and
logistics. Natural problem decomposition, geographical
distribution, and requirements for autonomous decision making make
this domain particularly suitable for AAMAS technology.A further four papers are focused on Aerospace applications. In
this domain, the technological adventurousness of a highly
competitive and demanding industry may explain the work presented.
However, the use of AAMAS technology to provide autonomy for
systems that cannot be easily or cheaply supervised by human
intervention seems to be another clear driver.Three papers focus on using AAMAS technology for manufacturing
applications. Here, the drivers seem to have been to use Agent
Autonomy or co-ordination techniques to provide cost savings,
realised by removing large numbers of repetitive management and
control tasks.The track also features papers which describe the use of AAMAS
technology for electricity network management and to provide a
training system for naval personnel. Both of these papers point to
cost savings realised by automating decision making processes.Just as we can see trends in the papers that have been selected
for publication, it's also interesting to point to missing areas.
There are no papers from promising domains for AAMAS adoption
including telecommunications, internet, pharmaceuticals,
eGovernment, or healthcare. Similarl,y the submission rate from
small companies and start-ups was expected to be somewhat higher.
These are areas that have been explored extensively in the past
[Luck et-al 2003] so it is important for the AAMAS community to
investigate why the research that has been performed has not
matured into published case studies at this time.Although the collection is diverse, all the papers share two
common traits. Every paper is the result of the efforts of a team
that has been prepared to take the risk of adopting a new
technology in a high pressure environment, and every paper
summarises what happens when talented people take a risk and that
risk pays off. We salute all these pioneers, and look forward to
the presentation of the technical program and the discussions that
are bound to result.},
location = {The Netherlands}
}


@article{10.1145/505282.505283,
author = {Sebastiani, Fabrizio},
title = {Machine learning in automated text categorization},
year = {2002},
issue_date = {March 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/505282.505283},
doi = {10.1145/505282.505283},
abstract = {The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.},
journal = {ACM Comput. Surv.},
month = mar,
pages = {1–47},
numpages = {47},
keywords = {Machine learning, text categorization, text classification}
}


@inproceedings{10.1145/342009.335372,
author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
title = {Mining frequent patterns without candidate generation},
year = {2000},
isbn = {1581132174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/342009.335372},
doi = {10.1145/342009.335372},
abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
booktitle = {Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data},
pages = {1–12},
numpages = {12},
location = {Dallas, Texas, USA},
series = {SIGMOD '00}
}


@article{10.1145/335191.335372,
author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
title = {Mining frequent patterns without candidate generation},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/335191.335372},
doi = {10.1145/335191.335372},
abstract = {Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.},
journal = {SIGMOD Rec.},
month = may,
pages = {1–12},
numpages = {12}
}


@article{10.1145/361237.361242,
author = {Duda, Richard O. and Hart, Peter E.},
title = {Use of the Hough transformation to detect lines and curves in pictures},
year = {1972},
issue_date = {Jan. 1972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/361237.361242},
doi = {10.1145/361237.361242},
abstract = {Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.},
journal = {Commun. ACM},
month = jan,
pages = {11–15},
numpages = {5},
keywords = {Hough transformation, colinear points, curve detection, line detection, pattern recognition, picture processing, point-line transformation}
}


@article{10.1145/361219.361220,
author = {Salton, G. and Wong, A. and Yang, C. S.},
title = {A vector space model for automatic indexing},
year = {1975},
issue_date = {Nov. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/361219.361220},
doi = {10.1145/361219.361220},
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
journal = {Commun. ACM},
month = nov,
pages = {613–620},
numpages = {8},
keywords = {automatic indexing, automatic information retrieval, content analysis, document space}
}


@proceedings{10.1145/100216,
title = {STOC '90: Proceedings of the twenty-second annual ACM symposium on Theory of Computing},
year = {1990},
isbn = {0897913612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Baltimore, Maryland, USA}
}


@inproceedings{10.1145/190314.190336,
author = {Perkins, Charles E. and Bhagwat, Pravin},
title = {Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers},
year = {1994},
isbn = {0897916824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/190314.190336},
doi = {10.1145/190314.190336},
abstract = {An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.},
booktitle = {Proceedings of the Conference on Communications Architectures, Protocols and Applications},
pages = {234–244},
numpages = {11},
location = {London, United Kingdom},
series = {SIGCOMM '94}
}


@article{10.1145/190809.190336,
author = {Perkins, Charles E. and Bhagwat, Pravin},
title = {Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/190809.190336},
doi = {10.1145/190809.190336},
abstract = {An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = oct,
pages = {234–244},
numpages = {11}
}


@article{10.1145/1118178.1118215,
author = {Wing, Jeannette M.},
title = {Computational thinking},
year = {2006},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/1118178.1118215},
doi = {10.1145/1118178.1118215},
abstract = {It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.},
journal = {Commun. ACM},
month = mar,
pages = {33–35},
numpages = {3}
}


@proceedings{10.1145/3027063,
title = {CHI EA '17: Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
year = {2017},
isbn = {9781450346566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHI 2017 is the premier international conference for the field of Human-Computer Interaction (HCI). This year it was held in Denver, bordering the beautiful Rocky Mountain region in the U.S., and reflected in our logo. The CHI 2017 conference began with two days of workshops and symposia, followed by four days of a technical program with 17 parallel sessions of provocative papers, panels, case studies, SIGs (Special Interest Groups), courses, and the popular student research, design, and game competitions. The growing alt.chi forum, now in its twelfth year, presented stimulating new ideas in HCI. The Interactivity forum showcased cutting-edge technology. For its second year, the CHI Art Exhibit merged art and technology in fascinating ways.This innovative work can be found in the Proceedings and Extended Abstracts, archived in the ACM Digital Library. For papers, the conference received 2400 submissions which were rigorously reviewed, resulting in 600 accepted papers. To ensure a better fit with reviewers, new subcommittees were created. Across all tracks, CHI received nearly 5000 submissions and accepted over 1000.Our conference theme this year, Explore, Innovate, Inspire, informed our planning process. A new venue held this year was CHI Stories. We generally know little of the personalities that drive the research presented at CHI. CHI Stories is a chance for CHI community members to share personal stories of inspiration, challenge, successes and failures, and grit. We also focused on inclusion this year. Hundreds of CHI attendees volunteered their skills in a Day of Service partnering with non-profit organizations. Inclusion was also manifest throughout the conference, for example, in the Diversity and Inclusion Lunch and by using telepresence robots to enable people with disabilities to participate remotely in the conference. Our keynote speakers were chosen to reflect our conference theme. The speakers were Neri Oxman, who combines computational design, digital fabrication, materials science and synthetic biology; Ben Shneiderman a founder of the CHI conference who, along with some key CHI personalities, gave a perspective on CHI's history and future; Wael Ghonim, credited with starting the Arab Spring and nominated for the Nobel Peace Prize; and best-selling author Nicholas Carr who challenges us to examine the unforeseen impacts of technology, particularly with automation.The world has experienced a dramatic change this past year. We live in extraordinary times and this calls for extraordinary thinking, something that the CHI community excels at. One of the challenges the community faced this year was responding to a U.S. executive order to ban citizens of certain countries from entering the country to attend CHI. As CHI is committed to inclusion, we decided to hold events at the conference to discuss and plan how we can continue our commitment to inclusion. The conference held a panel to discuss impacts of current political events on science, and hastily organized a panel to promote a conversation of civil liberties in science and a SIG on how the CHI community can participate in change. We are proud to have expanded telepresence options through the use of robots to enable people to participate in the conference remotely if they were physically unable to enter the country. We had a keynote speaker who inspired us on the topic of Internet activism. Our art exhibit, I'll Be Watching You, examined the contemporary issue of surveillance.},
location = {Denver, Colorado, USA}
}


@article{10.1145/1970392.1970395,
author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
title = {Robust principal component analysis?},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/1970392.1970395},
doi = {10.1145/1970392.1970395},
abstract = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
journal = {J. ACM},
month = jun,
articleno = {11},
numpages = {37},
keywords = {ℓ1-norm minimization, Principal components, duality, low-rank matrices, nuclear-norm minimization, robustness vis-a-vis outliers, sparsity, video surveillance}
}


@ARTICLE{7906512,
  author={L’Heureux, Alexandra and Grolinger, Katarina and Elyamany, Hany F. and Capretz, Miriam A. M.},
  journal={IEEE Access}, 
  title={Machine Learning With Big Data: Challenges and Approaches}, 
  year={2017},
  volume={5},
  number={},
  pages={7776-7797},
  abstract={The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.},
  keywords={Big Data;Machine learning algorithms;Data mining;Algorithm design and analysis;Data analysis;Support vector machines;Classification algorithms;Big Data;Big Data Vs;data analysis;data analytics;deep learning;distributed computing;machine learning;neural networks},
  doi={10.1109/ACCESS.2017.2696365},
  ISSN={2169-3536},
  month={},}@ARTICLE{8732419,
  author={Rappaport, Theodore S. and Xing, Yunchou and Kanhere, Ojas and Ju, Shihao and Madanayake, Arjuna and Mandal, Soumyajit and Alkhateeb, Ahmed and Trichopoulos, Georgios C.},
  journal={IEEE Access}, 
  title={Wireless Communications and Applications Above 100 GHz: Opportunities and Challenges for 6G and Beyond}, 
  year={2019},
  volume={7},
  number={},
  pages={78729-78757},
  abstract={Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.},
  keywords={Wireless communication;Wireless sensor networks;Antenna arrays;Bandwidth;Communication system security;Cognition;Imaging;mmWave;millimeter wave;5G;D-band;6G;channel sounder;propagation measurements;Terahertz (THz);array processing;imaging;scattering theory;cone of silence;digital phased arrays;digital beamformer;signal processing for THz;position location;channel modeling;THz applications;wireless cognition;network offloading},
  doi={10.1109/ACCESS.2019.2921522},
  ISSN={2169-3536},
  month={},}@ARTICLE{10105236,
  author={Shoufan, Abdulhadi},
  journal={IEEE Access}, 
  title={Exploring Students’ Perceptions of ChatGPT: Thematic Analysis and Follow-Up Survey}, 
  year={2023},
  volume={11},
  number={},
  pages={38805-38818},
  abstract={ChatGPT has sparked both excitement and skepticism in education. To analyze its impact on teaching and learning it is crucial to understand how students perceive ChatGPT and assess its potential and challenges. Toward this, we conducted a two-stage study with senior students in a computer engineering program ( $n=56$ ). In the first stage, we asked the students to evaluate ChatGPT using their own words after they used it to complete one learning activity. The returned responses (3136 words) were analyzed by coding and theme building (36 codes and 15 themes). In the second stage, we used the derived codes and themes to create a 27-item questionnaire. The students responded to this questionnaire three weeks later after completing other activities with the help of ChatGPT. The results show that the students admire the capabilities of ChatGPT and find it interesting, motivating, and helpful for study and work. They find it easy to use and appreciate its human-like interface that provides well-structured responses and good explanations. However, many students feel that ChatGPT’s answers are not always accurate and most of them believe that it requires good background knowledge to work with since it does not replace human intelligence. So, most students think that ChatGPT needs to be improved but are optimistic that this will happen soon. When it comes to the negative impact of ChatGPT on learning, academic integrity, jobs, and life, the students are divided. We conclude that ChatGPT can and should be used for learning. However, students should be aware of its limitations. Educators should try using ChatGPT and guide students on effective prompting techniques and how to assess generated responses. The developers should improve their models to enhance the accuracy of given answers. The study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.},
  keywords={Chatbots;Education;Codes;Encoding;Performance evaluation;Oral communication;ChatGPT;students’ perceptions;education},
  doi={10.1109/ACCESS.2023.3268224},
  ISSN={2169-3536},
  month={},}@ARTICLE{8643084,
  author={Wang, Shuai and Ouyang, Liwei and Yuan, Yong and Ni, Xiaochun and Han, Xuan and Wang, Fei-Yue},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Blockchain-Enabled Smart Contracts: Architecture, Applications, and Future Trends}, 
  year={2019},
  volume={49},
  number={11},
  pages={2266-2277},
  abstract={In recent years, the rapid development of cryptocurrencies and their underlying blockchain technology has revived Szabo’s original idea of smart contracts, i.e., computer protocols that are designed to automatically facilitate, verify, and enforce the negotiation and implementation of digital contracts without central authorities. Smart contracts can find a wide spectrum of potential application scenarios in the digital economy and intelligent industries, including financial services, management, healthcare, and Internet of Things, among others, and also have been integrated into the mainstream blockchain-based development platforms, such as Ethereum and Hyperledger. However, smart contracts are still far from mature, and major technical challenges such as security and privacy issues are still awaiting further research efforts. For instance, the most notorious case might be “The DAO Attack” in June 2016, which led to more than $50 million Ether transferred into an adversary’s account. In this paper, we strive to present a systematic and comprehensive overview of blockchain-enabled smart contracts, aiming at stimulating further research toward this emerging research area. We first introduced the operating mechanism and mainstream platforms of blockchain-enabled smart contracts, and proposed a research framework for smart contracts based on a novel six-layer architecture. Second, both the technical and legal challenges, as well as the recent research progresses, are listed. Third, we presented several typical application scenarios. Toward the end, we discussed the future development trends of smart contracts. This paper is aimed at providing helpful guidance and reference for future research efforts.},
  keywords={Smart contracts;Blockchain;Bitcoin;Market research;Proposals;Blockchain;parallel blockchain;six-layer architecture;smart contracts},
  doi={10.1109/TSMC.2019.2895123},
  ISSN={2168-2232},
  month={Nov},}@ARTICLE{8764449,
  author={Poria, Soujanya and Majumder, Navonil and Mihalcea, Rada and Hovy, Eduard},
  journal={IEEE Access}, 
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances}, 
  year={2019},
  volume={7},
  number={},
  pages={100943-100953},
  abstract={Emotion is intrinsic to humans and consequently, emotion understanding is a key part of human-like artificial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasingly popular as a new research frontier in natural language processing (NLP) due to its ability to mine opinions from the plethora of publicly available conversational data on platforms such as Facebook, Youtube, Reddit, Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychological analysis), education (understanding student frustration), and more. In Addition, ERC is also extremely important for generating emotion-aware dialogues that require an understanding of the user’s emotions. Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms. However, it is a difficult problem to solve because of several research challenges. In this paper, we discuss these challenges and shed light on recent research in this field. We also describe the drawbacks of these approaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC.},
  keywords={Emotion recognition;Task analysis;Context modeling;Taxonomy;Natural language processing;Pragmatics;Emotion recognition;sentiment analysis;dialogue systems;natural language processing},
  doi={10.1109/ACCESS.2019.2929050},
  ISSN={2169-3536},
  month={},}@ARTICLE{8746184,
  author={Miklosik, Andrej and Kuchta, Martin and Evans, Nina and Zak, Stefan},
  journal={IEEE Access}, 
  title={Towards the Adoption of Machine Learning-Based Analytical Tools in Digital Marketing}, 
  year={2019},
  volume={7},
  number={},
  pages={85705-85718},
  abstract={Exponential technological expansion creates opportunities for competitive advantage by applying new data-oriented approaches to digital marketing practices. Machine learning (ML) can predict future developments and support decision-making by extracting insights from large amounts of generated data. This functionality greatly impacts and streamlines the strategic decision-making process of organizations. The research gap analysis revealed that a little is known about marketers' attitude toward, and knowledge about, ML tools and their adoption and utilization to support strategic and operational management. The research presented here focuses on the selection and adoption of the ML-driven analytical tools by three distinct groups: marketing agencies, media companies, and advertisers. Qualitative and quantitative research was conducted on a sample of these organizations operating in Slovakia. The findings highlight: 1) the important role of intelligent analytical tools in the creation and deployment of marketing strategies; 2) the lack of knowledge about emerging technologies, such as ML and artificial intelligence (AI); 3) the potential application of the ML tools in marketing, and; 4) the low level of adoption and utilization of the ML-driven analytical tools in marketing management. A framework consisting of enablers and a process map was developed to help organizations identify the opportunities and successfully execute projects that are oriented toward the deployment and adoption of the analytical ML tools in digital marketing.},
  keywords={Tools;Decision making;Strategic planning;Companies;Internet;Big data;data-driven analytical tools;digital marketing;machine learning (ML);marketing agencies;marketing analysis},
  doi={10.1109/ACCESS.2019.2924425},
  ISSN={2169-3536},
  month={},}@ARTICLE{10190626,
  author={AlZubi, Ahmad Ali and Galyna, Kalda},
  journal={IEEE Access}, 
  title={Artificial Intelligence and Internet of Things for Sustainable Farming and Smart Agriculture}, 
  year={2023},
  volume={11},
  number={},
  pages={78686-78692},
  abstract={Technologies like AI and IoT have been employed in farming for some time now, along with other forms of cutting-edge computer science. There has been a shift in recent years toward thinking about how to put this new technology to use. Agriculture has provided a large portion of humanity’s sustenance for thousands of years, with its most notable contribution being the widespread use of effective agricultural practices for several crop types. The advent of cutting-edge IoT know-how with the ability to monitor agricultural ecosystems and guarantee high-quality production is underway. Smart Sustainable Agriculture continues to face formidable hurdles due to the widespread dispersion of agricultural procedures, such as the deployment and administration of IoT and AI devices, the sharing of data and administration, interoperability, and the analysis and storage of enormous data quantities. This work initially analyses existing Internet-of-Things technologies used in Smart Sustainable Agriculture (SSA) to discover architectural components that might facilitate the development of SSA platforms. This paper examines the state of research and development in SSA, pays attention to the current form of information, and proposes an Internet of Things (IoT) and artificial intelligence (AI) framework as a starting point for SSA.},
  keywords={Smart agriculture;Crops;Artificial intelligence;Internet of Things;Farming;Monitoring;Soil;Internet of Things;Sustainable development;Smart agriculture;Internet of Things (IoT);artificial intelligence (AI);smart sustainable agriculture (SSA);smart farming},
  doi={10.1109/ACCESS.2023.3298215},
  ISSN={2169-3536},
  month={},}@ARTICLE{5733835,
  author={},
  journal={ISO/IEC/IEEE 24765:2010(E)}, 
  title={ISO/IEC/IEEE International Standard - Systems and software engineering -- Vocabulary}, 
  year={2010},
  volume={},
  number={},
  pages={1-418},
  abstract={The systems and software engineering disciplines are continuing to mature while information technology advances. This International Standard was prepared to collect and standardize terminology. Its purpose is to identify terms currently in use in the field and standard definitions for these terms. It is intended to serve as a useful reference for those in the Information Technology field, and to encourage the use of systems and software engineering standards prepared by ISO and liaison organizations IEEE Computer Society and Project Management Institute (PMI). This International Standard replaces IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, which was contributed by the IEEE as a source document. The approach and lexical exactitude of IEEE Std 610.12-1990 served as a model for this International Standard. Nevertheless, approximately two thirds of the definitions in this International Standard are new since IEEE Std 610.12 was last updated in 1990, a reflection of the continued evolution in the field.},
  keywords={IEEE standards;ISO standards;IEC standards;Software engineering;Dictionaries;computer;dictionary;information technology;software engineering;systems engineering;terminology;vocabulary},
  doi={10.1109/IEEESTD.2010.5733835},
  ISSN={},
  month={Dec},}@ARTICLE{9490241,
  author={Tedre, Matti and Toivonen, Tapani and Kahila, Juho and Vartiainen, Henriikka and Valtonen, Teemu and Jormanainen, Ilkka and Pears, Arnold},
  journal={IEEE Access}, 
  title={Teaching Machine Learning in K–12 Classroom: Pedagogical and Technological Trajectories for Artificial Intelligence Education}, 
  year={2021},
  volume={9},
  number={},
  pages={110558-110572},
  abstract={Over the past decades, numerous practical applications of machine learning techniques have shown the potential of AI-driven and data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning and AI in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based “traditional” programming is a central aspect and building block in developing next generation computational thinking.},
  keywords={Education;Machine learning;Programming profession;Automation;Task analysis;Terminology;Technological innovation;Machine learning;artificial intelligence;K-12;school;computing education;computational thinking;pedagogy},
  doi={10.1109/ACCESS.2021.3097962},
  ISSN={2169-3536},
  month={},}@ARTICLE{9405644,
  author={Kashevnik, Alexey and Shchedrin, Roman and Kaiser, Christian and Stocker, Alexander},
  journal={IEEE Access}, 
  title={Driver Distraction Detection Methods: A Literature Review and Framework}, 
  year={2021},
  volume={9},
  number={},
  pages={60063-60076},
  abstract={Driver inattention and distraction are the main causes of road accidents, many of which result in fatalities. To reduce road accidents, the development of information systems to detect driver inattention and distraction is essential. Currently, distraction detection systems for road vehicles are not yet widely available or are limited to specific causes of driver inattention such as driver fatigue. Despite the increasing automation of driving due to the availability of increasingly sophisticated assistance systems, the human driver will continue to play a longer role as supervisor of vehicle automation. With this in mind, we review the published scientific literature on driver distraction detection methods and integrate the identified approaches into a holistic framework that is the main contribution of the paper. Based on published scientific work, our driver distraction detection framework contains a structured summary of reviewed approaches for detecting the three main distraction detection approaches: manual distraction, visual distraction, and cognitive distraction. Our framework visualizes the whole detection information chain from used sensors, measured data, computed data, computed events, inferred behavior, and inferred distraction type. Besides providing a sound summary for researchers interested in distracted driving, we discuss several practical implications for the development of driver distraction detection systems that can also combine different approaches for higher detection quality. We think our research can be useful despite - or even because of - the great developments in automated driving.},
  keywords={Vehicles;Automation;Task analysis;Monitoring;Visualization;Taxonomy;Psychology;Automotive applications;automated vehicles;data systems;distraction detection;driver distraction;driver monitoring;driving distraction;intelligent transportation;vehicle driving},
  doi={10.1109/ACCESS.2021.3073599},
  ISSN={2169-3536},
  month={},}@ARTICLE{9409047,
  author={Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
  journal={IEEE Access}, 
  title={From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction}, 
  year={2021},
  volume={9},
  number={},
  pages={60447-60458},
  abstract={In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
  keywords={Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
  doi={10.1109/ACCESS.2021.3074559},
  ISSN={2169-3536},
  month={},}@ARTICLE{8915693,
  author={Estevez, Julian and Garate, Gorka and Graña, Manuel},
  journal={IEEE Access}, 
  title={Gentle Introduction to Artificial Intelligence for High-School Students Using Scratch}, 
  year={2019},
  volume={7},
  number={},
  pages={179027-179036},
  abstract={The importance of educating the next generations in the understanding of the fundamentals of the upcoming scientific and technological innovations that will force a broad social and economical paradigm change can not be overstressed. One such breakthrough technologies is Artificial Intelligence (AI), specifically machine learning algorithms. Nowadays, the public has little understanding of the workings and implications of AI techniques that are already entering their lives in many ways. We aim to achieve widespread public understanding of these issues in an experiential learning framework. Following a design based research approach, we propose to implement program coding scaffoldings to teach and experiment some basic mechanisms of AI systems. Such experiments would be shedding new light into AI potentials and limitations. In this paper we focus on innovative ways to introduce high school students to the fundamentals and operation of two of the most popular AI algorithms. We describe the elements of a workshop where we provide an academic use-create-modify scaffolding where students work on the Scratch partial coding of the algorithms so they can explore the behavior of the algorithm, gaining understanding of the underlying computational thinking of AI processes. The extent of the impact on the students of this experience is measured through questionnaires filled before and after participation in the workshop. Preliminary experiments offer encouraging results, showing that the workshop has differential impact on the way students understand AI.},
  keywords={Artificial intelligence;Conferences;Tools;Programming profession;Education;Scratch programming;teaching AI fundamentals;public AI awareness},
  doi={10.1109/ACCESS.2019.2956136},
  ISSN={2169-3536},
  month={},}@ARTICLE{954607,
  author={Picard, R.W. and Vyzas, E. and Healey, J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Toward machine emotional intelligence: analysis of affective physiological state}, 
  year={2001},
  volume={23},
  number={10},
  pages={1175-1191},
  abstract={The ability to recognize emotion is one of the hallmarks of emotional intelligence, an aspect of human intelligence that has been argued to be even more important than mathematical and verbal intelligences. This paper proposes that machine intelligence needs to include emotional intelligence and demonstrates results toward this goal: developing a machine's ability to recognize the human affective state given four physiological signals. We describe difficult issues unique to obtaining reliable affective data and collect a large set of data from a subject trying to elicit and experience each of eight emotional states, daily, over multiple weeks. This paper presents and compares multiple algorithms for feature-based recognition of emotional state from this data. We analyze four physiological signals that exhibit problematic day-to-day variations: The features of different emotions on the same day tend to cluster more tightly than do the features of the same emotion on different days. To handle the daily variations, we propose new features and algorithms and compare their performance. We find that the technique of seeding a Fisher Projection with the results of sequential floating forward search improves the performance of the Fisher Projection and provides the highest recognition rates reported to date for classification of affect from physiology: 81 percent recognition accuracy on eight classes of emotion, including neutral.},
  keywords={Machine intelligence;Intelligent agent;Emotion recognition;Humans;Clustering algorithms;Pattern recognition;Neuroscience;Data analysis;Signal analysis;Physiology},
  doi={10.1109/34.954607},
  ISSN={1939-3539},
  month={Oct},}@ARTICLE{10223039,
  author={Guo, Ziyue and Zhu, Zongyang and Li, Yizhi and Cao, Shidong and Chen, Hangyue and Wang, Gaoang},
  journal={IEEE Access}, 
  title={AI Assisted Fashion Design: A Review}, 
  year={2023},
  volume={11},
  number={},
  pages={88403-88415},
  abstract={This review explores the integration of enhanced personalization and seamless multimodal interfaces in the field of fashion design and recommendation. We examine the increasing demand for personalized fashion experiences and the potential of multimodal interfaces in facilitating effective communication between designers and users. By leveraging user preferences, body measurements, and style choices, artificial intelligence (AI) systems can deliver highly personalized fashion recommendations. The integration of various input modalities, including text, images and sketches, enables designers and users to communicate their design ideas with ease. The primary results highlight the transformative potential of enhanced personalization and seamless multimodal interfaces, empowering designers and consumers to co-create unique and personalized designs. This paradigm shift fosters a deeper level of engagement and creativity within the fashion industry. Embracing this advancement unlocks unprecedented opportunities for designers, brands, and consumers, ushering in a new era of innovation and creativity in fashion design.},
  keywords={Artificial intelligence;Task analysis;Feature extraction;Surveys;Context modeling;Adaptation models;Deep learning;Clothing industry;Artificial intelligence;deep learning;fashion design},
  doi={10.1109/ACCESS.2023.3306235},
  ISSN={2169-3536},
  month={},}@ARTICLE{9812604,
  author={Ahmad, Naqash and Ghadi, Yazeed and Adnan, Muhammad and Ali, Mansoor},
  journal={IEEE Access}, 
  title={Load Forecasting Techniques for Power System: Research Challenges and Survey}, 
  year={2022},
  volume={10},
  number={},
  pages={71054-71090},
  abstract={The main and pivot part of electric companies is the load forecasting. Decision-makers and think tank of power sectors should forecast the future need of electricity with large accuracy and small error to give uninterrupted and free of load shedding power to consumers. The demand of electricity can be forecasted amicably by many Machine Learning (ML), Deep Learning (DL) and Artificial Intelligence (AI) techniques among which hybrid methods are most popular. The present technologies of load forecasting and present work regarding combination of various ML, DL and AI algorithms are reviewed in this paper. The comprehensive review of single and hybrid forecasting models with functions; advantages and disadvantages are discussed in this paper. The comparison between the performance of the models in terms of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE) values are compared and discussed with literature of different models to support the researchers to select the best model for load prediction. This comparison validates the fact that the hybrid forecasting models will provide a more optimal solution.},
  keywords={Load forecasting;Load modeling;Forecasting;Predictive models;Biological system modeling;Companies;Meteorology;Load forecasting;machine learning;load shedding;root mean squared error;mean absolute percentage error},
  doi={10.1109/ACCESS.2022.3187839},
  ISSN={2169-3536},
  month={},}@ARTICLE{9319727,
  author={Chen, Xing and Liu, Guizhong},
  journal={IEEE Internet of Things Journal}, 
  title={Energy-Efficient Task Offloading and Resource Allocation via Deep Reinforcement Learning for Augmented Reality in Mobile Edge Networks}, 
  year={2021},
  volume={8},
  number={13},
  pages={10843-10856},
  abstract={The augmented reality (AR) applications have been widely used in the field of Internet of Things (IoT) because of good immersion experience for users, but their ultralow delay demand and high energy consumption bring a huge challenge to the current communication system and terminal power. The emergence of mobile-edge computing (MEC) provides a good thinking to solve this challenge. In this article, we study an energy-efficient task offloading and resource allocation scheme for AR in both the single-MEC and multi-MEC systems. First, a more specific and detailed AR application model is established as a directed acyclic graph according to its internal functionality. Second, based on this AR model, a joint optimization problem of task offloading and resource allocation is formulated to minimize the energy consumption of each user subject to the latency requirement and the limited resources. The problem is a mixed multiuser competition and cooperation problem, which involves the task offloading decision, uplink/downlink transmission resources allocation, and computing resources allocation of users and MEC server. Since it is an NP-hard problem and the communication environment is dynamic, it is difficult for genetic algorithms or heuristic algorithms to solve. Therefore, we propose an intelligent and efficient resource allocation and task offloading algorithm based on the deep reinforcement learning framework of multiagent deep deterministic policy gradient (MADDPG) in a dynamic communication environment. Finally, simulation results show that the proposed algorithm can greatly reduce the energy consumption of each user terminal.},
  keywords={Task analysis;Servers;Optimization;Resource management;Energy consumption;Computational modeling;Heuristic algorithms;Augmented reality (AR);deep reinforcement learning;Internet of Things (IoT);mobile-edge computing (MEC);multiagent deep deterministic policy gradient (MADDPG);resource allocation;task offloading},
  doi={10.1109/JIOT.2021.3050804},
  ISSN={2327-4662},
  month={July},}@INPROCEEDINGS{6168399,
  author={Patidar, Shyam and Rane, Dheeraj and Jain, Pritesh},
  booktitle={2012 Second International Conference on Advanced Computing & Communication Technologies}, 
  title={A Survey Paper on Cloud Computing}, 
  year={2012},
  volume={},
  number={},
  pages={394-398},
  abstract={Cloud computing is the biggest buzz in the computer world these days -- maybe too big of a buzz. Cloud computing means different things to different people. Cloud computing is not a small, undeveloped branch of IT. Research firm IDC thinks that cloud computing will reach $42 billion in 2012. You can do everything on cloud from running applications to storing data off-site. You can run entire operating systems on the cloud. This paper is for anyone who may have recently heard the term "cloud computing" for the first time and needs to know what it is and how it helps them.},
  keywords={Cloud computing;Hardware;Companies;Investments;Google;Computational modeling;Cloud Computing;Utility Computing;Data Center;On-Demand Computing},
  doi={10.1109/ACCT.2012.15},
  ISSN={2327-0659},
  month={Jan},}@ARTICLE{9669005,
  author={Sivapalan, Gawsalyan and Nundy, Koushik Kumar and Dev, Soumyabrata and Cardiff, Barry and John, Deepu},
  journal={IEEE Transactions on Biomedical Circuits and Systems}, 
  title={ANNet: A Lightweight Neural Network for ECG Anomaly Detection in IoT Edge Sensors}, 
  year={2022},
  volume={16},
  number={1},
  pages={24-35},
  abstract={In this paper, we propose a lightweight neural network for real-time electrocardiogram (ECG) anomaly detection and system level power reduction of wearable Internet of Things (IoT) Edge sensors. The proposed network utilizes a novel hybrid architecture consisting of Long Short Term Memory (LSTM) cells and Multi-Layer Perceptrons (MLP). The LSTM block takes a sequence of coefficients representing the morphology of ECG beats while the MLP input layer is fed with features derived from instantaneous heart rate. Simultaneous training of the blocks pushes the overall network to learn distinct features complementing each other for making decisions. The network was evaluated in terms of accuracy, computational complexity, and power consumption using data from the MIT-BIH arrhythmia database. To address the class imbalance in the dataset, we augmented the dataset using SMOTE algorithm for network training. The network achieved an average classification accuracy of 97% across several records in the database. Further, the network was mapped to a fixed point model, retrained in a bit accurate fixed-point environment to compensate for the quantization error, and ported to an ARM Cortex M4 based embedded platform. In laboratory testing, the overall system was successfully demonstrated, and a significant saving of $\simeq \!\! 50\%$ power was achieved by gating the wireless transmission using the classifier. Wireless transmission was enabled only to transmit the beats deemed anomalous by the classifier. The proposed technique compares favourably with current methods in terms of computational complexity and has the advantage of stand-alone operation in the edge node, without the need for always-on wireless connectivity making it ideal for IoT wearable devices.},
  keywords={Electrocardiography;Feature extraction;Wireless sensor networks;Training;Databases;Wireless communication;Power demand;Anomaly detection;edge computing;IoT sensors;LSTM;MLP;neural networks;power reduction},
  doi={10.1109/TBCAS.2021.3137646},
  ISSN={1940-9990},
  month={Feb},}@ARTICLE{9354557,
  author={Abernathey, Ryan P. and Augspurger, Tom and Banihirwe, Anderson and Blackmon-Luca, Charles C. and Crone, Timothy J. and Gentemann, Chelle L. and Hamman, Joseph J. and Henderson, Naomi and Lepore, Chiara and McCaie, Theo A. and Robinson, Niall H. and Signell, Richard P.},
  journal={Computing in Science & Engineering}, 
  title={Cloud-Native Repositories for Big Scientific Data}, 
  year={2021},
  volume={23},
  number={2},
  pages={26-35},
  abstract={Scientific data have traditionally been distributed via downloads from data server to local computer. This way of working suffers from limitations as scientific datasets grow toward the petabyte scale. A “cloud-native data repository,” as defined in this article, offers several advantages over traditional data repositories—performance, reliability, cost-effectiveness, collaboration, reproducibility, creativity, downstream impacts, and access and inclusion. These objectives motivate a set of best practices for cloud-native data repositories: analysis-ready data, cloud-optimized (ARCO) formats, and loose coupling with data-proximate computing. The Pangeo Project has developed a prototype implementation of these principles by using open-source scientific Python tools. By providing an ARCO data catalog together with on-demand, scalable distributed computing, Pangeo enables users to process big data at rates exceeding 10 GB/s. Several challenges must be resolved in order to realize cloud computing’s full potential for scientific research, such as organizing funding, training users, and enforcing data privacy requirements.},
  keywords={Cloud computing;Training data;Computational modeling;Reproducibility of results;Collaboration;Reliability;Distributed databases},
  doi={10.1109/MCSE.2021.3059437},
  ISSN={1558-366X},
  month={March},}@ARTICLE{9387490,
  author={Granger, Brian E. and Pérez, Fernando},
  journal={Computing in Science & Engineering}, 
  title={Jupyter: Thinking and Storytelling With Code and Data}, 
  year={2021},
  volume={23},
  number={2},
  pages={7-14},
  abstract={Project Jupyter is an open-source project for interactive computing widely used in data science, machine learning, and scientific computing. We argue that even though Jupyter helps users perform complex, technical work, Jupyter itself solves problems that are fundamentally human in nature. Namely, Jupyter helps humans to think and tell stories with code and data. We illustrate this by describing three dimensions of Jupyter: 1) interactive computing; 2) computational narratives; and 3) the idea that Jupyter is more than software. We illustrate the impact of these dimensions on a community of practice in earth and climate science.},
  keywords={Open source software;Scientific computing;Machine learning;Data science;Open source software;Meteorology},
  doi={10.1109/MCSE.2021.3059263},
  ISSN={1558-366X},
  month={March},}@ARTICLE{9006805,
  author={Hussein, Mohamed K. and Mousa, Mohamed H.},
  journal={IEEE Access}, 
  title={Efficient Task Offloading for IoT-Based Applications in Fog Computing Using Ant Colony Optimization}, 
  year={2020},
  volume={8},
  number={},
  pages={37191-37201},
  abstract={The current thinking concerning computations required by Internet of Things (IoT) applications is shifting toward fog computing instead of cloud computing, thereby achieving most of the required computations at the network edge of the IoT devices. Fog computing can thus improve the quality of service of delay-sensitive applications by allowing such applications to take advantage of the low latency provided by fog computing rather than the high latency of the cloud. Therefore, tasks in various IoT applications must be effectively distributed over the fog nodes to improve the quality of service, specifically the task response time. In this paper, two nature-inspired meta-heuristic schedulers, namely ant colony optimization (ACO) and particle swarm optimization (PSO), are used to propose two different scheduling algorithms to effectively load balance IoT tasks over the fog nodes under communication cost and response time considerations. The experimental results of the proposed algorithms are compared with those of the round robin (RR) algorithm. The evaluations show that the proposed ACO-based scheduler achieves an improvement in the response times of IoT applications compared to the proposed PSO-based and RR algorithms and effectively load balances the fog nodes.},
  keywords={Task analysis;Cloud computing;Edge computing;Time factors;Quality of service;Delays;Computer architecture;Fog computing;Internet of Things;quality of service;task offloading and scheduling},
  doi={10.1109/ACCESS.2020.2975741},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8253363,
  author={Ahmad, Adang Suwandi},
  booktitle={2017 International Symposium on Electronics and Smart Devices (ISESD)}, 
  title={Brain inspired cognitive artificial intelligence for knowledge extraction and intelligent instrumentation system}, 
  year={2017},
  volume={},
  number={},
  pages={352-356},
  abstract={Artificial intelligence evolves with the development of computers even rely on computational development. The ways and processes of human thinking developed by Psychologists and welcomed by computational experts produce the science of Artificial Intelligence. This continues with the development of cognitive science that encourages the development of Artificial Intelligence to Cognitive Thinking Intelligence, a new pathway to the science of Artificial Intelligence that can emulate human cognitive abilities even if not 100%. Emulation of human cognitive abilities is developed based on the modeling of system interaction with the environment and information fusion, which can be used to conduct Inferencing, so when this occurs repeatedly it will produce knowledge that grows. This process is called Knowledge Growing System which is Brain Inspired Cognitive Artificial Intelligence and can be used for information extraction and when applied to instrumentation system will realize Intelligent Instrumentation System.},
  keywords={Artificial intelligence;Heart;Data mining;Software;Instruments;Electrocardiography;Smart devices;Artificial Intelligence;Cognitive Artificial Intelligece;Knowledge Extraction},
  doi={10.1109/ISESD.2017.8253363},
  ISSN={},
  month={Oct},}@ARTICLE{10050860,
  author={Han, Binghui and Zahraoui, Younes and Mubin, Marizan and Mekhilef, Saad and Seyedmahmoudian, Mehdi and Stojcevski, Alex},
  journal={IEEE Access}, 
  title={Home Energy Management Systems: A Review of the Concept, Architecture, and Scheduling Strategies}, 
  year={2023},
  volume={11},
  number={},
  pages={19999-20025},
  abstract={Growing electricity demand, the deployment of renewable energy sources and the widespread use of smart home appliances provide new opportunities for home energy management systems (HEMSs), which can be defined as systems that improve the overall energy production and consumption of residential buildings by controlling and scheduling the use of household equipment. By saving energy, reducing residential electricity costs, optimizing the utilization rate and reliability of utility companies’ power systems, and reducing air pollution for society, HEMSs lead to an enhancement in the socioeconomic development of low-carbon economies. This review aims to systematically analyze and summarize the development trends and challenges of HEMSs in recent years. This paper reviews the development history of the HEMS architecture and discusses the characteristics of several major communication technologies in the current HEMS infrastructure. In addition, the common objectives and constraints related to scheduling optimization are classified, and several optimization methods in the literature, including various intelligent algorithms, have been introduced, compared, and critically analyzed. Furthermore, experimental studies and challenges in the real world are also summarized and recommendations are given. This paper reveals the trend from simple to complex in the architecture and functionality of HEMSs, discusses the challenges for future improvements in modeling and scheduling, and shows the development of various modeling and scheduling methods. Based on this review, researchers can gain a comprehensive understanding of current research trends in HEMSs and open up ideas for developing new modeling and scheduling approaches by gaining insight into the trade-offs between optimum solutions and computational complexity.},
  keywords={Optimization;Energy management systems;Home appliances;Renewable energy sources;Market research;Reliability;Optimal scheduling;Demand response;home appliances;home energy management system;optimization;renewable energy resources;smart grid},
  doi={10.1109/ACCESS.2023.3248502},
  ISSN={2169-3536},
  month={},}@ARTICLE{6030926,
  author={Rodriguez, Rosa M. and Martinez, Luis and Herrera, Francisco},
  journal={IEEE Transactions on Fuzzy Systems}, 
  title={Hesitant Fuzzy Linguistic Term Sets for Decision Making}, 
  year={2012},
  volume={20},
  number={1},
  pages={109-119},
  abstract={Dealing with uncertainty is always a challenging problem, and different tools have been proposed to deal with it. Recently, a new model that is based on hesitant fuzzy sets has been presented to manage situations in which experts hesitate between several values to assess an indicator, alternative, variable, etc. Hesitant fuzzy sets suit the modeling of quantitative settings; however, similar situations may occur in qualitative settings so that experts think of several possible linguistic values or richer expressions than a single term for an indicator, alternative, variable, etc. In this paper, the concept of a hesitant fuzzy linguistic term set is introduced to provide a linguistic and computational basis to increase the richness of linguistic elicitation based on the fuzzy linguistic approach and the use of context-free grammars by using comparative terms. Then, a multicriteria linguistic decision-making model is presented in which experts provide their assessments by eliciting linguistic expressions. This decision model manages such linguistic expressions by means of its representation using hesitant fuzzy linguistic term sets.},
  keywords={Pragmatics;Fuzzy sets;Grammar;Semantics;Decision making;Uncertainty;Humans;Context-free grammar;fuzzy linguistic approach;hesitant fuzzy sets;linguistic decision making;linguistic information},
  doi={10.1109/TFUZZ.2011.2170076},
  ISSN={1941-0034},
  month={Feb},}@ARTICLE{9613752,
  author={Angara, Prashanti Priya and Stege, Ulrike and MacLean, Andrew and Müller, Hausi A. and Markham, Tom},
  journal={IEEE Transactions on Quantum Engineering}, 
  title={Teaching Quantum Computing to High-School-Aged Youth: A Hands-On Approach}, 
  year={2022},
  volume={3},
  number={},
  pages={1-15},
  abstract={Quantum computing is aninterdisciplinary field that lies at the intersection of mathematics, quantum physics, and computer science, and finds applications in areas including optimization, machine learning, and simulation of chemical, physical, and biological systems. It has the potential to help solve problems that so far have no satisfying method solving them, and to provide significant speedup to solutions when compared with their best classical approaches. In turn, quantum computing may allow us to solve problems for inputs that so far are deemed practically intractable. With the computational power of quantum computers and the proliferation of quantum development kits, quantum computing is anticipated to become mainstream, and the demand for a skilled workforce in quantum computing is expected to increase significantly. Therefore, quantum computing education is ramping up. This article describes our experiences in designing and delivering quantum computing workshops for youth (Grades 9–12). We introduce students to the world of quantum computing in innovative ways, such as newly designed unplugged activities for teaching basic quantum computing concepts. We also take a programmatic approach and introduce students to the IBM Quantum Experience using Qiskit and Jupyter notebooks. Our contributions are as follows. First, we present creative ways to teach quantum computing to youth with little or no experience in science, technology, engineering, and mathematics areas; second, we discuss diversity and highlight various pathways into quantum computing from quantum software to quantum hardware; and third, we discuss the design and delivery of online and in-person motivational, introductory, and advanced workshops for youth.},
  keywords={Quantum computing;Quantum mechanics;Conferences;Qubit;Programming profession;Industries;Software;Computer science (CS) unplugged (CS Unplugged);education;entanglement;high-school-aged youth;measurement;qiskit;quantum computing;quantum computing games;quantum gates;quantum teleportation;qubit systems;superposition;teachers;training;workforce development},
  doi={10.1109/TQE.2021.3127503},
  ISSN={2689-1808},
  month={},}

@inproceedings{10.1145/237814.237866,
author = {Grover, Lov K.},
title = {A fast quantum mechanical algorithm for database search},
year = {1996},
isbn = {0897917855},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/237814.237866},
doi = {10.1145/237814.237866},
booktitle = {Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing},
pages = {212–219},
numpages = {8},
location = {Philadelphia, Pennsylvania, USA},
series = {STOC '96}
}


@inproceedings{10.1145/1390156.1390294,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and composing robust features with denoising autoencoders},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390294},
doi = {10.1145/1390156.1390294},
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1096–1103},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}


@inproceedings{10.1145/512950.512973,
author = {Cousot, Patrick and Cousot, Radhia},
title = {Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints},
year = {1977},
isbn = {9781450373500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512950.512973},
doi = {10.1145/512950.512973},
abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
pages = {238–252},
numpages = {15},
location = {Los Angeles, California},
series = {POPL '77}
}


@proceedings{10.1145/349299,
title = {PLDI '00: Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation},
year = {2000},
isbn = {1581131992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, British Columbia, Canada}
}


@inproceedings{10.1145/52324.52356,
author = {Jacobson, V.},
title = {Congestion avoidance and control},
year = {1988},
isbn = {0897912799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/52324.52356},
doi = {10.1145/52324.52356},
abstract = {In October of '86, the Internet had the first of what became a series of 'congestion collapses'. During this period, the data throughput from LBL to UC Berkeley (sites separated by 400 yards and three IMP hops) dropped from 32 Kbps to 40 bps. Mike Karels1 and I were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad. We wondered, in particular, if the 4.3BSD (Berkeley UNIX) TCP was mis-behaving or if it could be tuned to work better under abysmal network conditions. The answer to both of these questions was “yes”.Since that time, we have put seven new algorithms into the 4BSD TCP:
round-trip-time variance estimationexponential retransmit timer backoffslow-startmore aggressive receiver ack policydynamic window sizing on congestionKarn's clamped retransmit backofffast retransmit Our measurements and the reports of beta testers suggest that the final product is fairly good at dealing with congested conditions on the Internet.This paper is a brief description of (i) - (v) and the rationale behind them. (vi) is an algorithm recently developed by Phil Karn of Bell Communications Research, described in [KP87]. (viii) is described in a soon-to-be-published RFC.Algorithms (i) - (v) spring from one observation: The flow on a TCP connection (or ISO TP-4 or Xerox NS SPP connection) should obey a 'conservation of packets' principle. And, if this principle were obeyed, congestion collapse would become the exception rather than the rule. Thus congestion control involves finding places that violate conservation and fixing them.By 'conservation of packets' I mean that for a connection 'in equilibrium', i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call 'conservative': A new packet isn't put into the network until an old packet leaves. The physics of flow predicts that systems with this property should be robust in the face of congestion. Observation of the Internet suggests that it was not particularly robust. Why the discrepancy?There are only three ways for packet conservation to fail:
The connection doesn't get to equilibrium, orA sender injects a new packet before an old packet has exited, orThe equilibrium can't be reached because of resource limits along the path. In the following sections, we treat each of these in turn.},
booktitle = {Symposium Proceedings on Communications Architectures and Protocols},
pages = {314–329},
numpages = {16},
location = {Stanford, California, USA},
series = {SIGCOMM '88}
}


@article{10.1145/52325.52356,
author = {Jacobson, V.},
title = {Congestion avoidance and control},
year = {1988},
issue_date = {August 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/52325.52356},
doi = {10.1145/52325.52356},
abstract = {In October of '86, the Internet had the first of what became a series of 'congestion collapses'. During this period, the data throughput from LBL to UC Berkeley (sites separated by 400 yards and three IMP hops) dropped from 32 Kbps to 40 bps. Mike Karels1 and I were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad. We wondered, in particular, if the 4.3BSD (Berkeley UNIX) TCP was mis-behaving or if it could be tuned to work better under abysmal network conditions. The answer to both of these questions was “yes”.Since that time, we have put seven new algorithms into the 4BSD TCP:
round-trip-time variance estimationexponential retransmit timer backoffslow-startmore aggressive receiver ack policydynamic window sizing on congestionKarn's clamped retransmit backofffast retransmit Our measurements and the reports of beta testers suggest that the final product is fairly good at dealing with congested conditions on the Internet.This paper is a brief description of (i) - (v) and the rationale behind them. (vi) is an algorithm recently developed by Phil Karn of Bell Communications Research, described in [KP87]. (viii) is described in a soon-to-be-published RFC.Algorithms (i) - (v) spring from one observation: The flow on a TCP connection (or ISO TP-4 or Xerox NS SPP connection) should obey a 'conservation of packets' principle. And, if this principle were obeyed, congestion collapse would become the exception rather than the rule. Thus congestion control involves finding places that violate conservation and fixing them.By 'conservation of packets' I mean that for a connection 'in equilibrium', i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call 'conservative': A new packet isn't put into the network until an old packet leaves. The physics of flow predicts that systems with this property should be robust in the face of congestion. Observation of the Internet suggests that it was not particularly robust. Why the discrepancy?There are only three ways for packet conservation to fail:
The connection doesn't get to equilibrium, orA sender injects a new packet before an old packet has exited, orThe equilibrium can't be reached because of resource limits along the path. In the following sections, we treat each of these in turn.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = aug,
pages = {314–329},
numpages = {16}
}


@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}


@article{10.1145/954339.954342,
author = {Zhao, W. and Chellappa, R. and Phillips, P. J. and Rosenfeld, A.},
title = {Face recognition: A literature survey},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/954339.954342},
doi = {10.1145/954339.954342},
abstract = {As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.},
journal = {ACM Comput. Surv.},
month = dec,
pages = {399–458},
numpages = {60},
keywords = {Face recognition, person identification}
}


@proceedings{10.5555/3041021,
title = {WWW '17 Companion: Proceedings of the 26th International Conference on World Wide Web Companion},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {We welcome you to the WWW2017 conference, the 26th of the series, and only the second one to be held in Australia.The annual World Wide Web Conference is the premier international forum to present and discuss progress in research, development, standards, and applications related to the Web. This conference is organised under the aegis of the International World Wide Web Conference Committee (IW3C2) in collaboration with local conference organisers in the host city, in this case, the four public universities in Western Australia: Curtin University, Murdoch University, the University of Western Australia and Edith Cowan University. This year, WWW 2017 is offered as the centerpiece of the inaugural Festival of the Web in Perth, a week-long celebration.WWW 2017 provides an opportunity to hear from the leaders of the web including three distinguished keynote speeches by world-class experts: Mark Pesce, Yoelle Maarek, and Melanie Johnston-Hollitt. There is a rich environment of technical activities, including 164 high quality papers in the Research Tracks, 54 papers in the four alternate tracks, over 100 papers in 15 workshops, 13 tutorial sessions, a Ph.D. Symposium track comprising presentations by seven doctoral students, an Industry track consisting of 20 papers focused on applied research, 20 demonstrations, a W3C track examining the latest Web standards and emerging technologies and 64 posters with, for the first time, a number of these offered as e-posters to augment the static poster panels. Overall, WWW2017 provides more than 400 high quality presentations on the key research and development issues of the World Wide Web.Co-located events in the Festival of the Web 2017 include the 4th Big Data Innovators Gathering (BIG 2017), the Web for All conference (W4A2017), and the 5th Serious Games and Applications for Health conference (SeGaH'17). In addition, several new events include Collaboration-Innovation, a one day conference focusing on building smart business innovation through collaboration; the Trust Factory, a curated forum exploring issues of trust and privacy on the web; and Bytes and Rights, a conference focused on issues of web governance, copyright, digital rights, privacy and security on the web. Finally, the Big Day In is a one-day IT careers conference designed by students for students, including tips and advice for secondary school students interested in IT and the web.Given Perth's location in one of the world's richest areas of natural resources, DeepSensor, a world class gathering of industry professionals, is being conducted as part of the Festival as an opportunity for professionals to share their real-world insights into the continuing development of the internet of things in the mining, oil and gas industries.},
location = {Perth, Australia}
}


@article{10.1145/272991.272995,
author = {Matsumoto, Makoto and Nishimura, Takuji},
title = {Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator},
year = {1998},
issue_date = {Jan. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {1049-3301},
url = {https://doi.org/10.1145/272991.272995},
doi = {10.1145/272991.272995},
abstract = {A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 219937 −1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 ≤ v ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p2) where  p is the degree of the polynomial.We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.},
journal = {ACM Trans. Model. Comput. Simul.},
month = jan,
pages = {3–30},
numpages = {28},
keywords = {k-distribution, m-sequences, GFSR, MT19937, Mersenne primes, Mersenne twister, TGFSR, finite fields, incomplete array, inversive-decimation method, multiple-recursive matrix method, primitive polynomials, random number generation, tempering}
}


@inproceedings{10.1145/1014052.1014073,
author = {Hu, Minqing and Liu, Bing},
title = {Mining and summarizing customer reviews},
year = {2004},
isbn = {1581138881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1014052.1014073},
doi = {10.1145/1014052.1014073},
abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {168–177},
numpages = {10},
keywords = {reviews, sentiment classification, summarization, text mining},
location = {Seattle, WA, USA},
series = {KDD '04}
}


@article{10.1145/963770.963772,
author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
title = {Evaluating collaborative filtering recommender systems},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/963770.963772},
doi = {10.1145/963770.963772},
abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
pages = {5–53},
numpages = {49},
keywords = {Collaborative filtering, evaluation, metrics, recommender systems}
}


@inproceedings{10.1145/2736277.2741093,
author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
title = {LINE: Large-scale Information Network Embedding},
year = {2015},
isbn = {9781450334693},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2736277.2741093},
doi = {10.1145/2736277.2741093},
abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available onlinefootnote{url{https://github.com/tangjianpku/LINE}}.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1067–1077},
numpages = {11},
keywords = {dimension reduction, feature learning, information network embedding, scalability},
location = {Florence, Italy},
series = {WWW '15}
}


@article{10.5555/1248547.1248548,
author = {Dem\v{s}ar, Janez},
title = {Statistical Comparisons of Classifiers over Multiple Data Sets},
year = {2006},
issue_date = {12/1/2006},
publisher = {JMLR.org},
volume = {7},
issn = {1532-4435},
abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1–30},
numpages = {30}
}


@article{10.1145/235815.235821,
author = {Barber, C. Bradford and Dobkin, David P. and Huhdanpaa, Hannu},
title = {The quickhull algorithm for convex hulls},
year = {1996},
issue_date = {Dec. 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/235815.235821},
doi = {10.1145/235815.235821},
abstract = {The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it used less memory. computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serous errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.},
journal = {ACM Trans. Math. Softw.},
month = dec,
pages = {469–483},
numpages = {15},
keywords = {Delaunay triangulation, Voronoi diagram, convex hull, halfspace intersection}
}


@article{10.1145/363235.363259,
author = {Hoare, C. A. R.},
title = {An axiomatic basis for computer programming},
year = {1969},
issue_date = {Oct. 1969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/363235.363259},
doi = {10.1145/363235.363259},
abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.},
journal = {Commun. ACM},
month = oct,
pages = {576–580},
numpages = {5},
keywords = {axiomatic method, formal language definition, machine-independent programming, program documentation, programming language design, theory of programming' proofs of programs}
}


@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}


@proceedings{10.1145/2807442,
title = {UIST '15: Proceedings of the 28th Annual ACM Symposium on User Interface Software \&amp; Technology},
year = {2015},
isbn = {9781450337793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are very excited to welcome you to the 28th Annual ACM Symposium on User Interface Software and Technology (UIST), held from November 8-11th 2015, in Charlotte, North Carolina, USA.UIST is the premier forum for the presentation of research innovations in the software and technology of human-computer interfaces. Sponsored by ACM's special interest groups on computer-human interaction (SIGCHI) and computer graphics (SIGGRAPH), UIST brings together researchers and practitioners from diverse areas including graphical \&amp; web user interfaces, tangible \&amp; ubiquitous computing, virtual \&amp; augmented reality, multimedia, new input \&amp; output devices, fabrication, wearable computing and CSCW.UIST 2015 received 297 technical paper submissions. After a thorough review process, the 39-member program committee accepted 70 papers (23.6\%). Each anonymous submission that entered the full review process was first reviewed by three external reviewers, and a meta-review was provided by a program committee member. If, after these four reviews, the submission was deemed to pass a rebuttal threshold, we asked the authors to submit a short rebuttal addressing the reviewers' concerns. A second member of the program committee was then asked to examine the paper, rebuttal, and reviews, and to provide their own meta-review. The program committee met in person in Berkeley, California, USA on June 25th and 26th, 2015, to select which papers to invite for the program. Submissions were accepted only after the authors provided a final revision addressing the committee's comments.In addition to papers, our program includes two papers from the ACM Transactions on Computer-Human Interaction journal (TOCHI), as well as 22 posters, 45 demonstrations, and 8 student presentations in the eleventh annual Doctoral Symposium. Our program also features the seventh annual Student Innovation Contest. Teams from all over the world will compete in this year's contest, which focuses on blurring the lines between art and engineering and creating tools for robotic storytelling. UIST 2015 will feature two keynote presentations. The opening keynote will be given by Ramesh Raskar (MIT Media Lab) on extreme computational imaging. Blaise Aguera Y Arcas from Google will deliver the closing keynote on machine intelligence.We welcome you to Charlotte, a city full of southern hospitality. We hope that you will find the technical program interesting and thought-provoking. We also hope that UIST 2015 will provide you with enjoyable opportunities to engage with fellow researchers from both industry and academia, from institutions around the world.},
location = {Charlotte, NC, USA}
}


@inproceedings{10.1145/345910.345920,
author = {Intanagonwiwat, Chalermek and Govindan, Ramesh and Estrin, Deborah},
title = {Directed diffusion: a scalable and robust communication paradigm for sensor networks},
year = {2000},
isbn = {1581131976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/345910.345920},
doi = {10.1145/345910.345920},
abstract = {Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.},
booktitle = {Proceedings of the 6th Annual International Conference on Mobile Computing and Networking},
pages = {56–67},
numpages = {12},
location = {Boston, Massachusetts, USA},
series = {MobiCom '00}
}


@proceedings{10.1145/1094811,
title = {OOPSLA '05: Proceedings of the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications},
year = {2005},
isbn = {1595930310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to OOPSLA 2005 in San Diego, California, USA, October 16-20, 2005. This is the proceedings of the 20th annual ACM SIGPLAN Conference on Object-Oriented Programming, systems, languages, and Applications, which was initiated in 1986. Technical papers present new ideas, new research, or in-depth reflections on languages, systems, and applications, focusing on objects. This year is first time that the technical program has been split into three parts: research papers, which describe substantiated new research or novel technical results, advance the state of the art, or report on significant experience or experimentation; Onward! papers, which describe new paradigms or metaphors in computing, new thinking about objects, new framings of computational problems or systems, and new technologies; and essays, which are explorations of technology and its impacts, presenting in-depth reflections on technology, its relation to human endeavors, and its philosophical, sociological, psychological, historical, r anthropological underpinnings.Each submission was judged on these criteria:Technical contribution--how substantial is the contributionNovelty--how novel or innovative are the ideasSubstantiation--how well proven is the contributionPresentation--how clearly written and presented is the materialArgument--how compelling or well-made are the arguments in the paperArt/Craft--how well does the paper demonstrate, describe, or promote excellence of artistry or craft in architecture, design, implementation, methodology, or documentationEach paper was assigned to at least three reviewers. Moreover, each paper and its reviews were further reviewed by the leaders of 16 focus groups (Analysis and Design Methods; Design Patterns; Distributed Systems; Experience with OO Applications and Systems; Frameworks and Components; Languages/Design; Languages/Implementation; Languages/Aspects; Object Databases and Persistence; Object Testing and Metrics; Parallel Systems; Programming Environments; Real-Time Systems; Reflection and Metaobject Models; Software Engineering Practices; Theoretical Foundations) who could assign further reviews. Therefore, each paper had in effect four reviews, with some having as many as nine.Each submitted paper co-authored by a program committee member was held to a much higher standard of review through a specific voting process designed to be auditable by the conference chair; each such paper was reviewed by at least six other committee members, on the basis of strict anonymity. The author of a program committee paper was required to leave the meeting room while the paper was being discussed. This year, five papers were submitted by program committee members and none were accepted.In all, 74 papers were submitted and 32 were accepted. OOPSLA 2005 continues the tradition of presenting well-written, carefully selected papers that should be of lasting value to the programming and object communities. And I hope that the new traditions started this year in pursuit of the themes of Explore / Discover / Understand serve well the community of researchers, practitioners, educations, and software thinkers.},
location = {San Diego, CA, USA}
}


@article{10.1145/359576.359585,
author = {Hoare, C. A. R.},
title = {Communicating sequential processes},
year = {1978},
issue_date = {Aug. 1978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/359576.359585},
doi = {10.1145/359576.359585},
abstract = {This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises.},
journal = {Commun. ACM},
month = aug,
pages = {666–677},
numpages = {12},
keywords = {classes, concurrency, conditional critical regions, coroutines, data representations, guarded commands, input, iterative arrays, monitors, multiple entries, multiple exits, nondeterminacy, output, parallel programming, procedures, program structures, programming, programming languages, programming primitives, recursion}
}


@proceedings{10.1145/237721,
title = {POPL '96: Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
year = {1996},
isbn = {0897917693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {St. Petersburg Beach, Florida, USA}
}


@proceedings{10.1145/178243,
title = {PLDI '94: Proceedings of the ACM SIGPLAN 1994 conference on Programming language design and implementation},
year = {1994},
isbn = {089791662X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, Florida, USA}
}


@article{10.1145/3298981,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3298981},
doi = {10.1145/3298981},
abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {12},
numpages = {19},
keywords = {Federated learning, GDPR, transfer learning}
}


@inproceedings{10.1145/312624.312649,
author = {Hofmann, Thomas},
title = {Probabilistic latent semantic indexing},
year = {1999},
isbn = {1581130961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/312624.312649},
doi = {10.1145/312624.312649},
booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {50–57},
numpages = {8},
location = {Berkeley, California, USA},
series = {SIGIR '99}
}


@article{10.1145/1177352.1177355,
author = {Yilmaz, Alper and Javed, Omar and Shah, Mubarak},
title = {Object tracking: A survey},
year = {2006},
issue_date = {2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1177352.1177355},
doi = {10.1145/1177352.1177355},
abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
journal = {ACM Comput. Surv.},
month = dec,
pages = {13–es},
numpages = {45},
keywords = {Appearance models, contour evolution, feature selection, object detection, object representation, point tracking, shape tracking}
}


@inproceedings{10.1145/279943.279962,
author = {Blum, Avrim and Mitchell, Tom},
title = {Combining labeled and unlabeled data with co-training},
year = {1998},
isbn = {1581130570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/279943.279962},
doi = {10.1145/279943.279962},
booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
pages = {92–100},
numpages = {9},
location = {Madison, Wisconsin, USA},
series = {COLT' 98}
}


@article{10.1145/2133806.2133826,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = apr,
pages = {77–84},
numpages = {8}
}


@proceedings{10.1145/113445,
title = {PLDI '91: Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation},
year = {1991},
isbn = {0897914287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Toronto, Ontario, Canada}
}


@article{10.1145/1968.1972,
author = {Valiant, L. G.},
title = {A theory of the learnable},
year = {1984},
issue_date = {Nov. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/1968.1972},
doi = {10.1145/1968.1972},
journal = {Commun. ACM},
month = nov,
pages = {1134–1142},
numpages = {9},
keywords = {inductive inference, probabilistic models of learning, propositional expressions}
}


@article{10.1145/3326362,
author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
title = {Dynamic Graph CNN for Learning on Point Clouds},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3326362},
doi = {10.1145/3326362},
abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
journal = {ACM Trans. Graph.},
month = oct,
articleno = {146},
numpages = {12},
keywords = {Point cloud, classification, segmentation}
}


@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}


@inproceedings{10.1145/1065010.1065034,
author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
title = {Pin: building customized program analysis tools with dynamic instrumentation},
year = {2005},
isbn = {1595930566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065010.1065034},
doi = {10.1145/1065010.1065034},
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {190–200},
numpages = {11},
keywords = {dynamic compilation, instrumentation, program analysis tools},
location = {Chicago, IL, USA},
series = {PLDI '05}
}


@article{10.1145/1064978.1065034,
author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
title = {Pin: building customized program analysis tools with dynamic instrumentation},
year = {2005},
issue_date = {June 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1064978.1065034},
doi = {10.1145/1064978.1065034},
abstract = {Robust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easy-to-use, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecture-specific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register re-allocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basic-block counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.},
journal = {SIGPLAN Not.},
month = jun,
pages = {190–200},
numpages = {11},
keywords = {dynamic compilation, instrumentation, program analysis tools}
}


@inproceedings{10.1145/2976749.2978318,
author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
title = {Deep Learning with Differential Privacy},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978318},
doi = {10.1145/2976749.2978318},
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {308–318},
numpages = {11},
keywords = {deep learning, differential privacy},
location = {Vienna, Austria},
series = {CCS '16}
}


@inproceedings{10.1145/93597.98741,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-tree: an efficient and robust access method for points and rectangles},
year = {1990},
isbn = {0897913655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/93597.98741},
doi = {10.1145/93597.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
booktitle = {Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data},
pages = {322–331},
numpages = {10},
location = {Atlantic City, New Jersey, USA},
series = {SIGMOD '90}
}


@article{10.1145/93605.98741,
author = {Beckmann, Norbert and Kriegel, Hans-Peter and Schneider, Ralf and Seeger, Bernhard},
title = {The R*-tree: an efficient and robust access method for points and rectangles},
year = {1990},
issue_date = {Jun. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/93605.98741},
doi = {10.1145/93605.98741},
abstract = {The R-tree, one of the most popular access methods for rectangles, is based on the heuristic optimization of the area of the enclosing rectangle in each inner node. By running numerous experiments in a standardized testbed under highly varying data, queries and operations, we were able to design the R*-tree which incorporates a combined optimization of area, margin and overlap of each enclosing rectangle in the directory. Using our standardized testbed in an exhaustive performance comparison, it turned out that the R*-tree clearly outperforms the existing R-tree variants. Guttman's linear and quadratic R-tree and Greene's variant of the R-tree. This superiority of the R*-tree holds for different types of queries and operations, such as map overlay, for both rectangles and multidimensional points in all experiments. From a practical point of view the R*-tree is very attractive because of the following two reasons 1 it efficiently supports point and spatial data at the same time and 2 its implementation cost is only slightly higher than that of other R-trees.},
journal = {SIGMOD Rec.},
month = may,
pages = {322–331},
numpages = {10}
}


@inproceedings{10.1145/945445.945462,
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
title = {Xen and the art of virtualization},
year = {2003},
isbn = {1581137575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/945445.945462},
doi = {10.1145/945445.945462},
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
pages = {164–177},
numpages = {14},
keywords = {hypervisors, paravirtualization, virtual machine monitors},
location = {Bolton Landing, NY, USA},
series = {SOSP '03}
}


@article{10.1145/1165389.945462,
author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
title = {Xen and the art of virtualization},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1165389.945462},
doi = {10.1145/1165389.945462},
abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
journal = {SIGOPS Oper. Syst. Rev.},
month = oct,
pages = {164–177},
numpages = {14},
keywords = {hypervisors, paravirtualization, virtual machine monitors}
}


@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {Bayesian optimization, black-box optimization, hyperparameter optimization, machine learning system},
location = {Anchorage, AK, USA},
series = {KDD '19}
}


@proceedings{10.1145/268946,
title = {POPL '98: Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
year = {1998},
isbn = {0897919793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Diego, California, USA}
}


@inproceedings{10.1145/945445.945450,
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
title = {The Google file system},
year = {2003},
isbn = {1581137575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/945445.945450},
doi = {10.1145/945445.945450},
booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
pages = {29–43},
numpages = {15},
keywords = {clustered storage, data storage, fault tolerance, scalability},
location = {Bolton Landing, NY, USA},
series = {SOSP '03}
}


@article{10.1145/1165389.945450,
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
title = {The Google file system},
year = {2003},
issue_date = {December 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/1165389.945450},
doi = {10.1145/1165389.945450},
journal = {SIGOPS Oper. Syst. Rev.},
month = oct,
pages = {29–43},
numpages = {15},
keywords = {clustered storage, data storage, fault tolerance, scalability}
}


@inproceedings{10.1145/168588.168596,
author = {Bellare, Mihir and Rogaway, Phillip},
title = {Random oracles are practical: a paradigm for designing efficient protocols},
year = {1993},
isbn = {0897916298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168588.168596},
doi = {10.1145/168588.168596},
abstract = {We argue that the random oracle model—where all parties have access to a public random oracle—provides a bridge between cryptographic theory and cryptographic practice. In the paradigm we suggest, a practical protocol P is produced by first devising and proving correct a protocol PR for the random oracle model, and then replacing oracle accesses by the computation of an “appropriately chosen” function h. This paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable security. We illustrate these gains for problems including encryption, signatures, and zero-knowledge proofs.},
booktitle = {Proceedings of the 1st ACM Conference on Computer and Communications Security},
pages = {62–73},
numpages = {12},
location = {Fairfax, Virginia, USA},
series = {CCS '93}
}


@inproceedings{10.1145/233269.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: an efficient data clustering method for very large databases},
year = {1996},
isbn = {0897917944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/233269.233324},
doi = {10.1145/233269.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
booktitle = {Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data},
pages = {103–114},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {SIGMOD '96}
}


@article{10.1145/235968.233324,
author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
title = {BIRCH: an efficient data clustering method for very large databases},
year = {1996},
issue_date = {June 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/235968.233324},
doi = {10.1145/235968.233324},
abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
journal = {SIGMOD Rec.},
month = jun,
pages = {103–114},
numpages = {12}
}


@article{10.1145/355984.355989,
author = {Paige, Christopher C. and Saunders, Michael A.},
title = {LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares},
year = {1982},
issue_date = {March 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/355984.355989},
doi = {10.1145/355984.355989},
journal = {ACM Trans. Math. Softw.},
month = mar,
pages = {43–71},
numpages = {29}
}


@proceedings{10.1145/1957656,
title = {HRI '11: Proceedings of the 6th international conference on Human-robot interaction},
year = {2011},
isbn = {9781450305617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI 2011). HRI is a single-track, highly selective annual conference that showcases the very best research and thinking in human-robot interaction. HRI is inherently interdisciplinary and multidisciplinary, reflecting work from researchers in robotics, psychology, cognitive science, HCI, human factors, linguistics, artificial intelligence, organizational behavior, and anthropology.The theme of HRI 2011 is "Real World HRI." The theme is intended to highlight HRI in which basic scientific research is further tested in real world settings or applied to questions that arise in real world settings. One central aspect of this type of research, in contrast to other realms of applied research, is that it is theoretically driven and feeds back to our theoretical understandings. As such, real world research fortifies our understanding of people, robots, and interaction between the two. This year's conference seeks to take up grand challenges of deploying real world human-robot systems.This year we have three keynote speakers. They will discuss their work on gesture (Sotaro Kita), biologically inspired computational vision (Randy O'Reilly), and cognitive robotics (Angelo Cangelosi). We also have a panel to highlight the conference theme: HRI in the real world. This panel brings together leaders from business and industrial robotics that are relying on current robotic technology to accomplish work in the world today.The call for papers attracted 149 full paper submissions (eight page papers) from Asia, Europe, the Middle East, and North America. The program committee conducted a rigorous review process for full papers, accepting 33 full papers for oral presentation and publication in the proceedings. This year, taking advantage of having both ACM and IEEE as the sponsor, all papers are archived in both the ACM Digital Library and IEEE Xplore.Furthermore, 123 late-breaking reports (two page brief papers) were screened for relevance and lightly reviewed; 99 were accepted for presentation at the HRI conference as posters, exposing a broader perspective of solutions, challenges and issues in HRI. They will be made available in the IEEE Xplore as well as the ACM Digital Library. Finally, a total of 18 videos (out of 36 submissions) were accepted based on importance, novelty and entertainment value. Four videos will be shown throughout the conference and the remaining videos will be shown in a special video session.Presented papers describe novel interaction techniques, the design of new robots, experimental evaluations of people and robots, and robots in real-world settings.This year the local hosts will provide three research laboratory tours during the lunch breaks. We hope that visitors enjoy the opportunity to experience the research ideas of the local hosts.},
location = {Lausanne, Switzerland}
}


@inproceedings{10.1145/1658939.1658941,
author = {Jacobson, Van and Smetters, Diana K. and Thornton, James D. and Plass, Michael F. and Briggs, Nicholas H. and Braynard, Rebecca L.},
title = {Networking named content},
year = {2009},
isbn = {9781605586366},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1658939.1658941},
doi = {10.1145/1658939.1658941},
abstract = {Network use has evolved to be dominated by content distribution and retrieval, while networking technology still speaks only of connections between hosts. Accessing content and services requires mapping from the what that users care about to the network's where. We present Content-Centric Networking (CCN) which treats content as a primitive - decoupling location from identity, security and access, and retrieving content by name. Using new approaches to routing named content, derived heavily from IP, we can simultaneously achieve scalability, security and performance. We implemented our architecture's basic features and demonstrate resilience and performance with secure file downloads and VoIP calls.},
booktitle = {Proceedings of the 5th International Conference on Emerging Networking Experiments and Technologies},
pages = {1–12},
numpages = {12},
keywords = {content routing, content-based security, content-centric networking},
location = {Rome, Italy},
series = {CoNEXT '09}
}


@inproceedings{10.1145/800157.805047,
author = {Cook, Stephen A.},
title = {The complexity of theorem-proving procedures},
year = {1971},
isbn = {9781450374644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800157.805047},
doi = {10.1145/800157.805047},
abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
booktitle = {Proceedings of the Third Annual ACM Symposium on Theory of Computing},
pages = {151–158},
numpages = {8},
location = {Shaker Heights, Ohio, USA},
series = {STOC '71}
}


@proceedings{10.1145/199448,
title = {POPL '95: Proceedings of the 22nd ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
year = {1995},
isbn = {0897916921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, California, USA}
}


@article{10.1145/3149.214121,
author = {Fischer, Michael J. and Lynch, Nancy A. and Paterson, Michael S.},
title = {Impossibility of distributed consensus with one faulty process},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/3149.214121},
doi = {10.1145/3149.214121},
abstract = {The consensus problem involves an asynchronous system of processes, some of which may be unreliable. The problem is for the reliable processes to agree on a binary value. In this paper, it is shown that every protocol for this problem has the possibility of nontermination, even with only one faulty process. By way of contrast, solutions are known for the synchronous case, the “Byzantine Generals” problem.},
journal = {J. ACM},
month = apr,
pages = {374–382},
numpages = {9}
}


@article{10.1145/116873.116880,
author = {Aurenhammer, Franz},
title = {Voronoi diagrams—a survey of a fundamental geometric data structure},
year = {1991},
issue_date = {Sept. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/116873.116880},
doi = {10.1145/116873.116880},
journal = {ACM Comput. Surv.},
month = sep,
pages = {345–405},
numpages = {61},
keywords = {cell complex, clustering, combinatorial complexity, convex hull, crystal structure, divide-and-conquer, geometric data structure, growth model, higher dimensional embedding, hyperplane arrangement, k-set, motion planning, neighbor searching, object modeling, plane-sweep, proximity, randomized insertion, spanning tree, triangulation}
}


@article{10.1145/321062.321069,
author = {Hooke, Robert and Jeeves, T. A.},
title = {`` Direct Search'' Solution of Numerical and Statistical Problems},
year = {1961},
issue_date = {April 1961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/321062.321069},
doi = {10.1145/321062.321069},
journal = {J. ACM},
month = apr,
pages = {212–229},
numpages = {18}
}


@inproceedings{10.1145/3079856.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1–12},
numpages = {12},
keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}


@article{10.1145/3140659.3080246,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
issue_date = {May 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/3140659.3080246},
doi = {10.1145/3140659.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
journal = {SIGARCH Comput. Archit. News},
month = jun,
pages = {1–12},
numpages = {12},
keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network}
}


@proceedings{10.1145/93542,
title = {PLDI '90: Proceedings of the ACM SIGPLAN 1990 conference on Programming language design and implementation},
year = {1990},
isbn = {0897913647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {White Plains, New York, USA}
}


@ARTICLE{9020189,
  author={Lin, Pei-Hsuan and Chen, Shih-Yeh},
  journal={IEEE Access}, 
  title={Design and Evaluation of a Deep Learning Recommendation Based Augmented Reality System for Teaching Programming and Computational Thinking}, 
  year={2020},
  volume={8},
  number={},
  pages={45689-45699},
  abstract={Programming is considered a skill to arouse and inspire learner's potential. Learning to program is a complex process that requires students to write grammar and instructions. The structure of a programming language does not cause impose problems to students, the real obstacle is how to apply these learned grammars and present them in a complete and correct program code for problem solving. In this study, a deep learning recommendation system was developed, which includes augmented reality (AR) technology, and learning theory, and is provided for study by students in non-major and also from different learning backgrounds. Those students divided into two groups, the students participating in the experimental group were using the AR system with deep learning recommendation and the students participating in the control group were using the AR system without deep learning recommendation. The results show that students in experimental group perform better than the control group with regards to learning achievement. On the other hand, in the part of computational thinking ability, students using a deep learning recommendation based AR system is significantly better than those using non-deep learning recommendation based AR system. Among the various dimensions of computational thinking, creativity, logical computing, critical thinking, and problem-solving skills are significantly different among the two groups of students. The students in experimental group perform better than the control group with regards to the dimensions of computational thinking, creativity, logical computing, critical thinking, and problem-solving skills.},
  keywords={Machine learning;Education;Programming profession;Problem-solving;Grammar;Augmented reality;Deep learning recommendation;computational thinking;AR technology},
  doi={10.1109/ACCESS.2020.2977679},
  ISSN={2169-3536},
  month={},}@ARTICLE{9172001,
  author={Demartini, Claudio Giovanni and Benussi, Lorenzo and Gatteschi, Valentina and Renga, Flavio},
  journal={IEEE Access}, 
  title={Education and Digital Transformation: The “Riconnessioni” Project}, 
  year={2020},
  volume={8},
  number={},
  pages={186233-186256},
  abstract={Schools, universities, and other educational entities are increasingly aware of the untapped potential of digital transformation, an essential process for increasing efficiency and collaboration, and reducing costs and errors in the management of at-scale training systems. In this context, the “Riconnessioni” project was promoted by the Compagnia di San Paolo in agreement with the Ministry of Education but planned, started and developed by the Foundation for the School. The digital transformation started with a defined strategy that leveraged opportunities presented by new technology while meeting the objectives of system stakeholders. Through several steps, that strategy was developed for education connecting everything to support tomorrow’s digital world and creating strong strategic partnerships able to build an ecosystem connecting people, processes, and things into a powerful, secure, and smart communications network. This paper reports on the three-year Riconnessioni project, which is combining the energies of teachers, managers, administrative staff, students, among others, and experimenting with new learning models, taking advantage of opportunities that emerged from perceptions stemming from concerns and systemic issues. To date, more than 150 schools in Italy have been included in the project, together with 550 teachers selected to scale up the instructional process. Using a methodology called “cascade training”, the 550 selected teachers were able to spread the knowledge to more than 2,600 colleagues. The monitoring and evaluation activity performed in Riconnessioni aims at processing information on implementation and results, following three lines. First, it regularly evaluates project activities from a reporting standpoint. Second, it verifies the plan consistency against implementation achievements. Third, it identifies changes produced and focuses on teachers’ and students’ skills to evaluate the effects of the project. The assessment framework is also discussed in this work, reporting on results regarding feedback, follow-up, and effects gathered from the field. The evaluation highlighted that labs were indeed able to improve teachers’ competence and underlined the added value of cascade training which spread digital domain knowledge and awareness into the group of involved schools.},
  keywords={Digital transformation;Training;Biological system modeling;Business;Cloud computing;Collaboration;Digital transformation;adaptive learning;computational thinking;computer science;education;information technologies;primary school;secondary school},
  doi={10.1109/ACCESS.2020.3018189},
  ISSN={2169-3536},
  month={},}@ARTICLE{5613437,
  author={Liu, Zhicheng and Stasko, John},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Mental Models, Visual Reasoning and Interaction in Information Visualization: A Top-down Perspective}, 
  year={2010},
  volume={16},
  number={6},
  pages={999-1008},
  abstract={Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development},
  keywords={Cognitive science;Cognition;Visualization;Data visualization;Computational modeling;Brain modeling;Humans;mental model;model-based reasoning;distributed cognition;interaction;theory;information visualization},
  doi={10.1109/TVCG.2010.177},
  ISSN={1941-0506},
  month={Nov},}@ARTICLE{10681094,
  author={Ahmed, Zishan and Shanto, Shakib Sadat and Rime, Most. Humayra Khanom and Morol, Md. Kishor and Fahad, Nafiz and Hossen, Md. Jakir and Abdullah-Al-Jubair, Md.},
  journal={IEEE Access}, 
  title={The Generative AI Landscape in Education: Mapping the Terrain of Opportunities, Challenges, and Student Perception}, 
  year={2024},
  volume={12},
  number={},
  pages={147023-147050},
  abstract={Generative AI (GAI) technologies like ChatGPT are permanently changing academic education. Their integration opens up vast opportunities for bespoke learning and better student interaction but also brings about academic honesty issues and the application of real-life educators. This study aims to fill the literature gap regarding the use of multiple GAI tools and their effect on academic outcomes via a comprehensive review. A systematic literature review was performed following PRISMA guidelines to synthesize results on the potential and drawbacks of GAI in educational domains. We included theoretical and empirical papers that used qualitative, quantitative, or mixed-methods study designs. We have also explored conceptual frameworks and the most creative AI applications with a special emphasis on uniqueness and practicability. Experiences, and Perceptions Concerning To compile the information needed we gathered insights into what students were going through by conducting the survey which contains 200 respondents of undergraduate university students gathering insights into the college students’ experiences and perceptions related to GAI used for educational purposes. At the basic level, GAI comprises areas like personalization, task automation, teacher assistance, and efficiency among others, and respective solutions for the immersion of a learner in learning processes to reform directions. However, it generates plenty of challenges such as the question of assessment integrity, the risk that too much automated grading could overwhelm educational value, and relevantly the veracity of AI-generated content as well as the potential disruption to skills like critical thinking, in addition to data privacy and ethical issues. Student Perception Survey the text also indicates that most students, as per the student perception survey found AI systems useful in academic support. However, they also know the other side of the coin and are very familiar with the technology constraints and challenges.},
  keywords={Education;Generative AI;Artificial intelligence;Surveys;Chatbots;Ethics;Market research;Chatbots;education;generative AI;opportunities and challenges;student perception},
  doi={10.1109/ACCESS.2024.3461874},
  ISSN={2169-3536},
  month={},}@ARTICLE{6389686,
  author={Schreck, Tobias and Keim, Daniel},
  journal={Computer}, 
  title={Visual Analysis of Social Media Data}, 
  year={2013},
  volume={46},
  number={5},
  pages={68-75},
  abstract={The application of visual analytics, which combines the advantages of computational knowledge discovery and interactive visualization, to social media data highlights the many benefits of this integrated approach. The Web extra at http://youtu.be/nhoq71gqyXE is a video demonstrating a prototype system for visual-interactive analysis of large georeferenced microblog datasets, describing the design of the system, and detailing its application to the VAST 2011 Challenge dataset. The dataset models an epidemic outbreak in a fictitious metropolitan area. The video shows how the system can detect the epidemic and analyze its development over time. The system was implemented by Juri Buchmueller, Fabian Maass, Stephan Sellien, Florian Stoffel, and Matthias Zieker at the University of Konstanz (they also produced this video). Further information on the system and the VAST challenge dataset can be found in E. Bertini et al., "Visual Analytics of Terrorist Activities Related to Epidemics," Proc. IEEE Conf. Visual Analytics Science and Technology (VAST 11), IEEE CS, pp. 329-330, 2011.},
  keywords={Media;Data visualization;Visual analytics;Data analysis;Data mining;Social network services;visual analytics;knowledge discovery;interactive data visualization;social media;text visualization;complex data},
  doi={10.1109/MC.2012.430},
  ISSN={1558-0814},
  month={May},}@ARTICLE{6843352,
  author={Fatemi, Mehdi and Haykin, Simon},
  journal={IEEE Access}, 
  title={Cognitive Control: Theory and Application}, 
  year={2014},
  volume={2},
  number={},
  pages={698-710},
  abstract={From an engineering point-of-view, cognitive control is inspired by the prefrontal cortex of the human brain; cognitive control may therefore be viewed as the overarching function of a cognitive dynamic system. In this paper, we describe a new way of thinking about cognitive control that embodies two basic components: learning and planning, both of which are based on two notions: 1) two-state model of the environment and the perceptor and 2) perception-action cycle, which is a distinctive characteristic of the cognitive dynamic system. Most importantly, it is shown that the cognitive control learning algorithm is a special form of Bellman's dynamic programming. Distinctive properties of the new algorithm include the following: 1) optimality of performance; 2) algorithmic convergence to optimal policy; and 3) linear law of complexity measured in terms of the number of actions taken by the cognitive controller on the environment. To validate these intrinsic properties of the algorithm, a computational experiment is presented, which involves a cognitive tracking radar that is known to closely mimic the visual brain. The experiment illustrates two different scenarios: 1) the impact of planning on learning curves of the new cognitive controller and 2) comparison of the learning curves of three different controllers, based on dynamic optimization, traditional  \(Q\) -learning, and the new algorithm. The latter two algorithms are based on the two-state model, and they both involve the use of planning.},
  keywords={Cognition;Heuristic algorithms;Brain modeling;Radar tracking;Dynamic programming;Complexity theory;Perception;Control systems;Cognitive dyanamic systems;cognitive control;dynamic programming;two-state model;entropic state;Shannon's entropy;explore/exploit tradeoff;learning;planning;Bayesian filtering},
  doi={10.1109/ACCESS.2014.2332333},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{6113213,
  author={Yuen, Man-Ching and King, Irwin and Leung, Kwong-Sak},
  booktitle={2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing}, 
  title={A Survey of Crowdsourcing Systems}, 
  year={2011},
  volume={},
  number={},
  pages={766-773},
  abstract={Crowd sourcing is evolving as a distributed problem-solving and business production model in recent years. In crowd sourcing paradigm, tasks are distributed to networked people to complete such that a company's production cost can be greatly reduced. In 2003, Luis von Ahn and his colleagues pioneered the concept of "human computation", which utilizes human abilities to perform computation tasks that are difficult for computers to process. Later, the term "crowdsourcing" was coined by Jeff Howe in 2006. Since then, a lot of work in crowd sourcing has focused on different aspects of crowd sourcing, such as computational techniques and performance analysis. In this paper, we give a survey on the literature on crowd sourcing which are categorized according to their applications, algorithms, performances and datasets. This paper provides a structured view of the research on crowd sourcing to date.},
  keywords={Humans;Games;Production;Internet;Algorithm design and analysis;Computers;Encyclopedias;crowdsourcing;survey},
  doi={10.1109/PASSAT/SocialCom.2011.203},
  ISSN={},
  month={Oct},}@ARTICLE{8839118,
  author={Jacob, Sunil and Menon, Varun G. and Al-Turjman, Fadi and P. G., Vinoj and Mostarda, Leonardo},
  journal={IEEE Access}, 
  title={Artificial Muscle Intelligence System With Deep Learning for Post-Stroke Assistance and Rehabilitation}, 
  year={2019},
  volume={7},
  number={},
  pages={133463-133473},
  abstract={Stroke is one of the prime reasons for paralysis throughout the world caused due to impaired nervous system and resulting in disability to move the affected body parts. Rehabilitation is the natural remedy for recovering from paralysis and enhancing the quality of life. Brain Computer Interface (BCI) controlled assistive technology is the new paradigm, providing assistance and rehabilitation for the paralysed. But, most of these devices are error prone and also hard to get continuous control because of the dynamic nature of the brain signals. Moreover, existing devices like exoskeletons brings additional burden on the patient and the caregivers and also results in mental fatigue and frustration. To solve these issues Artificial Muscle Intelligence with Deep Learning (AMIDL) system is proposed in this paper. AMIDL integrates user intentions with artificial muscle movements in an efficient way to improve the performance. Human thoughts captured using Electroencephalogram (EEG) sensors are transformed into body movements, by utilising microcontroller and Transcutaneous Electrical Nerve Stimulation (TENS) device. EEG signals are subjected to pre-processing, feature extraction and classification, before being passed on to the affected body part. The received EEG signal is correlated with the recorded artificial muscle movements. If the captured EEG signal falls below the desired level, the affected body part will be stimulated by the recorded artificial muscle movements. The system also provides a feature for communicating human intentions as alert message to caregivers, in case of emergency situations. This is achieved by offline training of specific gesture and online gesture recognition algorithm. The recognised gesture is transformed into speech, thus enabling the paralysed to express their feelings to the relatives or friends. Experiments were carried out with the aid of healthy and paralysed subjects. The AMIDL system helped to reduce mental fatigue, miss-operation, frustration and provided continuous control. The thrust of lifting the exoskeleton is also reduced by using light weight wireless electrodes. The proposed system will be a great communication aid for paralysed to express their thoughts and feelings with dear and near ones, thereby enhancing the quality of life.},
  keywords={Electroencephalography;Exoskeletons;Muscles;Robot kinematics;Robot sensing systems;Task analysis;Artificial muscle intelligence;assistivetechnologies;BCI;EEG;exoskeleton;healthcare;intelligent solutions;deep learning system;paralyzed;stroke},
  doi={10.1109/ACCESS.2019.2941491},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{6014716,
  author={Sabahi, Farzad},
  booktitle={2011 IEEE 3rd International Conference on Communication Software and Networks}, 
  title={Virtualization-level security in cloud computing}, 
  year={2011},
  volume={},
  number={},
  pages={250-254},
  abstract={Cloud computing is one of today's most exciting technology because of its cost-reducing, flexibility, and scalability. With the fast growing of cloud computing technology, Data security becomes more and more important in it. In evaluating whether to move to cloud computing, it is important to compare benefits and also risks of it. Thus, security and other existed issues in the cloud cause cloud clients need more time to think about moving to cloud environments. But Security-related topics is one of the most arguable issues in the cloud computing which caused several enterprises looks to this technology uncertainly and move toward it warily. In this paper I try to summarize cloud computing RAS (Reliability, Availability, and Security) issues and also clarify available solution for some of them. In this paper I try to summarize virtualization level of cloud computing security in detailed view.},
  keywords={Privacy;Computational modeling;Business;Reliability;Virtual machine monitors;Security;Virtualization;Cloud computing;Security},
  doi={10.1109/ICCSN.2011.6014716},
  ISSN={},
  month={May},}@ARTICLE{9756272,
  author={Cao, Longbing},
  journal={IEEE Intelligent Systems}, 
  title={A New Age of AI: Features and Futures}, 
  year={2022},
  volume={37},
  number={1},
  pages={25-37},
  abstract={By reviewing the 70 years of AI, this article summarizes and discusses the paradigm transformations from the age of AI before the year 2000 to the new age of AI from the year 2000 onward. It reviews the AI thinking and features of various AI generations and paradigms during these two ages of AI and their transformations. The paper further summarizes several AI Formulas from the AI vision, system, goal, task, and process perspectives. Several important areas are highlighted in developing AI Futures: shrinking the gaps between human, natural and social AI, and developing human-like/level AI, meta AI, reflective AI, metasynthetic AI, data-driven AI, beyond ‘IID AI,’ actionable AI, and sustainable AI. In the new age of AI, we encourage your deep thinking of AI futures.},
  keywords={Machine vision;Artificial intelligence;Technology forecasting;Intelligent systems},
  doi={10.1109/MIS.2022.3150944},
  ISSN={1941-1294},
  month={Jan},}@ARTICLE{10602503,
  author={Kalateh, Sepideh and Estrada-Jimenez, Luis A. and Nikghadam-Hojjati, Sanaz and Barata, Jose},
  journal={IEEE Access}, 
  title={A Systematic Review on Multimodal Emotion Recognition: Building Blocks, Current State, Applications, and Challenges}, 
  year={2024},
  volume={12},
  number={},
  pages={103976-104019},
  abstract={Emotion recognition involves accurately interpreting human emotions from various sources and modalities, including questionnaires, verbal, and physiological signals. With its broad applications in affective computing, computational creativity, human-robot interactions, and market research, the field has seen a surge in interest in recent years. This paper presents a systematic review of multimodal emotion recognition (MER) techniques developed from 2014 to 2024, encompassing verbal, physiological signals, facial, body gesture, and speech as well as emerging methods like sketches emotion recognition. The review explores various emotion models, distinguishing between emotions, feelings, sentiments, and moods, along with human emotional expression, categorized in both artistic and non-verbal ways. It also discusses the background of automated emotion recognition systems and introduces seven criteria for evaluating modalities alongside a current state analysis of MER, drawn from the human-centric perspective of this field. By selecting the PRISMA guidelines and carefully analyzing 45 selected articles, this review provides comprehensive perspectives into existing studies, datasets, technical approaches, identified gaps, and future directions in MER. It also highlights existing challenges and current applications of the MER.},
  keywords={Emotion recognition;Physiology;Mood;Feature extraction;Cultural differences;Guidelines;Multimodal sensors;Artificial intelligence;Affective computing;Deep learning;Machine learning;Multimodal emotion recognition;artificial intelligence;affective computing;emotion recognition;deep learning;machine learning;emotion expression},
  doi={10.1109/ACCESS.2024.3430850},
  ISSN={2169-3536},
  month={},}@ARTICLE{8951014,
  author={Khan, Irfan and Zhang, Xianchao and Rehman, Mobashar and Ali, Rahman},
  journal={IEEE Access}, 
  title={A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection}, 
  year={2020},
  volume={8},
  number={},
  pages={10262-10281},
  abstract={Classification is the key and most widely studied paradigm in machine learning community. The selection of appropriate classification algorithm for a particular problem is a challenging task, formally known as algorithm selection problem (ASP) in literature. It is increasingly becoming focus of research in machine learning community. Meta-learning has demonstrated substantial success in solving ASP, especially in the domain of classification. Considerable progress has been made in classification algorithm recommendation and researchers have proposed various methods in literature that tackles ASP in many different ways in meta-learning setup. Yet there is a lack of survey and comparative study that critically analyze, summarize and assess the performance of existing methods. To fill these gaps, in this paper we first present a literature survey of classification algorithm recommendation methods. The survey shed light on the motivational reasons for pursuing classifier selection through meta-learning and comprehensively discusses the different phases of classifier selection based on a generic framework that is formed as an outcome of reviewing prior works. Subsequently, we critically analyzed and summarized the existing studies from the literature in three important dimensions i.e., meta-features, meta-learner and meta-target. In the second part of this paper, we present extensive comparative evaluation of all the prominent methods for classifier selection based on 17 classification algorithms and 84 benchmark datasets. The comparative study quantitatively assesses the performance of classifier selection methods and highlight the limitations and strengths of meta-features, meta-learners and meta-target in classification algorithm recommendation system. Finally, we conclude this paper by identifying current challenges and suggesting future work directions. We expect that this work will provide baseline and a solid overview of state of the art works in this domain to new researchers, and will steer future research in this direction.},
  keywords={Machine learning algorithms;Task analysis;Classification algorithms;Machine learning;Heuristic algorithms;Clustering algorithms;Space exploration;Meta-learning;algorithm selection;classification;machine learning},
  doi={10.1109/ACCESS.2020.2964726},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9138986,
  author={Abts, Dennis and Ross, Jonathan and Sparling, Jonathan and Wong-VanHaren, Mark and Baker, Max and Hawkins, Tom and Bell, Andrew and Thompson, John and Kahsai, Temesghen and Kimmell, Garrin and Hwang, Jennifer and Leslie-Hurd, Rebekah and Bye, Michael and Creswick, E.R. and Boyd, Matthew and Venigalla, Mahitha and Laforge, Evan and Purdy, Jon and Kamath, Purushotham and Maheshwari, Dinesh and Beidler, Michael and Rosseel, Geert and Ahmad, Omar and Gagarin, Gleb and Czekalski, Richard and Rane, Ashay and Parmar, Sahil and Werner, Jeff and Sproch, Jim and Macias, Adrian and Kurtz, Brian},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning Workloads}, 
  year={2020},
  volume={},
  number={},
  pages={145-158},
  abstract={In this paper, we introduce the Tensor Streaming Processor (TSP) architecture, a functionally-sliced microarchitecture with memory units interleaved with vector and matrix deep learning functional units in order to take advantage of dataflow locality of deep learning operations. The TSP is built based on two key observations: (1) machine learning workloads exhibit abundant data parallelism, which can be readily mapped to tensors in hardware, and (2) a simple and deterministic processor with producer-consumer stream programming model enables precise reasoning and control of hardware components, achieving good performance and power efficiency. The TSP is designed to exploit parallelism inherent in machine-learning workloads including instruction-level, memory concurrency, data and model parallelism, while guaranteeing determinism by eliminating all reactive elements in the hardware (e.g. arbiters, and caches). Early ResNet50 image classification results demonstrate 20.4K processed images per second (IPS) with a batch-size of one— a $4 \times$ improvement compared to other modern GPUs and accelerators [44]. Our first ASIC implementation of the TSP architecture yields a computational density of more than 1 TeraOp/s per square mm of silicon for its $25 \times 29$ mm 14nm chip operating at a nominal clock frequency of 900 MHz. The TSP demonstrates a novel hardware-software approach to achieve fast, yet predictable, performance on machine-learning workloads within a desired power envelope.},
  keywords={},
  doi={10.1109/ISCA45697.2020.00023},
  ISSN={},
  month={May},}@ARTICLE{8951256,
  author={Chakraborty, Koyel and Bhattacharyya, Siddhartha and Bag, Rajib},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={A Survey of Sentiment Analysis from Social Media Data}, 
  year={2020},
  volume={7},
  number={2},
  pages={450-464},
  abstract={In the current era of automation, machines are constantly being channelized to provide accurate interpretations of what people express on social media. The human race nowadays is submerged in the idea of what and how people think and the decisions taken thereafter are mostly based on the drift of the masses on social platforms. This article provides a multifaceted insight into the evolution of sentiment analysis into the limelight through the sudden explosion of plethora of data on the internet. This article also addresses the process of capturing data from social media over the years along with the similarity detection based on similar choices of the users in social networks. The techniques of communalizing user data have also been surveyed in this article. Data, in its different forms, have also been analyzed and presented as a part of survey in this article. Other than this, the methods of evaluating sentiments have been studied, categorized, and compared, and the limitations exposed in the hope that this shall provide scope for better research in the future.},
  keywords={Social networking (online);Sentiment analysis;Clustering algorithms;Indexes;Computer science;Tools;Business;Clustering;community;sentiment analysis;social media;social networks},
  doi={10.1109/TCSS.2019.2956957},
  ISSN={2329-924X},
  month={April},}@ARTICLE{8449329,
  author={Wang, Shuai and Wang, Jing and Wang, Xiao and Qiu, Tianyu and Yuan, Yong and Ouyang, Liwei and Guo, Yuanyuan and Wang, Fei-Yue},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Blockchain-Powered Parallel Healthcare Systems Based on the ACP Approach}, 
  year={2018},
  volume={5},
  number={4},
  pages={942-950},
  abstract={To improve the accuracy of diagnosis and the effectiveness of treatment, a framework of parallel healthcare systems (PHSs) based on the artificial systems + computational experiments + parallel execution (ACP) approach is proposed in this paper. PHS uses artificial healthcare systems to model and represent patients’ conditions, diagnosis, and treatment process, then applies computational experiments to analyze and evaluate various therapeutic regimens, and implements parallel execution for decision-making support and real-time optimization in both actual and artificial healthcare processes. In addition, we combine the emerging blockchain technology with PHS, via constructing a consortium blockchain linking patients, hospitals, health bureaus, and healthcare communities for comprehensive healthcare data sharing, medical records review, and care auditability. Finally, a prototype named parallel gout diagnosis and treatment system is built and deployed to verify and demonstrate the effectiveness and efficiency of the blockchain-powered PHS framework.},
  keywords={Medical diagnosis;Blockchain;Parallel processing;Smart healthcare;Medical treatment;Artificial systems + computational experiments + parallel execution (ACP);blockchain;parallel healthcare systems (PHSs);parallel intelligence;smart medicine},
  doi={10.1109/TCSS.2018.2865526},
  ISSN={2329-924X},
  month={Dec},}@INPROCEEDINGS{8252094,
  author={Ahmad, Adang Suwandi and Sumari, Arwin Datumaya Wahyudi},
  booktitle={2017 Computing Conference}, 
  title={Cognitive artificial intelligence: Brain-inspired intelligent computation in artificial intelligence}, 
  year={2017},
  volume={},
  number={},
  pages={135-141},
  abstract={Computation occurred within human brain is very much awesome and is not possible to be emulated 100% exactly in Artificial Intelligence (AI) method-based machines. What scientists did and have been done so far up to now are to try to model it as close as to what exactly occurs within the brain. Human brain has an awesome mechanism in performing computation with the end result is new knowledge and human uses the knowledge to actuate his organs. In this paper we will show a new approach for emulating the computation occured within human brain to obtain new knowledge based on the inputs sensed by the system's sensory system taken from the environment. When this process is carried out recursively, the system's knowledge becomes newer and newer, and it is called as knowledge growing. This approach is designed for an agent that has ability to think and act rationally like human. Our cognitive modelling approach is resulted in a model of human information processing and a technique to obtain the most maximum performance should be taken by the cognitive agent. This method is called as A3S (Arwin-Adang-Aciek-Sembiring), the agent is called as Knowledge-Growing System (KGS) and this brain-inspired method opens a new perspective in AI that we call as Cognitive Artificial Intelligence (CAI).},
  keywords={Brain modeling;Information processing;Artificial intelligence;Mathematical model;Psychology;Computational modeling;A3S;Cognitive Artificial Intelligence;intelligent computation;knowledge extraction;Knowledge-Growing System},
  doi={10.1109/SAI.2017.8252094},
  ISSN={},
  month={July},}@INPROCEEDINGS{990497,
  author={Bertalmio, M. and Bertozzi, A.L. and Sapiro, G.},
  booktitle={Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}, 
  title={Navier-stokes, fluid dynamics, and image and video inpainting}, 
  year={2001},
  volume={1},
  number={},
  pages={I-I},
  abstract={Image inpainting involves filling in part of an image or video using information from the surrounding area. Applications include the restoration of damaged photographs and movies and the removal of selected objects. We introduce a class of automated methods for digital inpainting. The approach uses ideas from classical fluid dynamics to propagate isophote lines continuously from the exterior into the region to be inpainted. The main idea is to think of the image intensity as a 'stream function for a two-dimensional incompressible flow. The Laplacian of the image intensity plays the role of the vorticity of the fluid; it is transported into the region to be inpainted by a vector field defined by the stream function. The resulting algorithm is designed to continue isophotes while matching gradient vectors at the boundary of the inpainting region. The method is directly based on the Navier-Stokes equations for fluid dynamics, which has the immediate advantage of well-developed theoretical and numerical results. This is a new approach for introducing ideas from computational fluid dynamics into problems in computer vision and image analysis.},
  keywords={Fluid dynamics;Streaming media;Filling;Image restoration;Motion pictures;Laplace equations;Algorithm design and analysis;Navier-Stokes equations;Computational fluid dynamics;Computer vision},
  doi={10.1109/CVPR.2001.990497},
  ISSN={1063-6919},
  month={Dec},}@ARTICLE{9199553,
  author={Ji, Shaoxiong and Pan, Shirui and Li, Xue and Cambria, Erik and Long, Guodong and Huang, Zi},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={Suicidal Ideation Detection: A Review of Machine Learning Methods and Applications}, 
  year={2021},
  volume={8},
  number={1},
  pages={214-226},
  abstract={Suicide is a critical issue in modern society. Early detection and prevention of suicide attempts should be addressed to save people's life. Current suicidal ideation detection (SID) methods include clinical methods based on the interaction between social workers or experts and the targeted individuals and machine learning techniques with feature engineering or deep learning for automatic detection based on online social contents. This article is the first survey that comprehensively introduces and discusses the methods from these categories. Domain-specific applications of SID are reviewed according to their data sources, i.e., questionnaires, electronic health records, suicide notes, and online user content. Several specific tasks and data sets are introduced and summarized to facilitate further research. Finally, we summarize the limitations of current work and provide an outlook of further research directions.},
  keywords={Feature extraction;Machine learning;Psychology;Twitter;Task analysis;Deep learning;feature engineering;social content;suicidal ideation detection (SID)},
  doi={10.1109/TCSS.2020.3021467},
  ISSN={2329-924X},
  month={Feb},}@ARTICLE{10506064,
  author={Li, Xiang and Wen, Congcong and Hu, Yuan and Yuan, Zhenghang and Zhu, Xiao Xiang},
  journal={IEEE Geoscience and Remote Sensing Magazine}, 
  title={Vision-Language Models in Remote Sensing: Current progress and future trends}, 
  year={2024},
  volume={12},
  number={2},
  pages={32-66},
  abstract={The remarkable achievements of ChatGPT and Generative Pre-trained Transformer 4 (GPT-4) have sparked a wave of interest and research in the field of large language models (LLMs) for artificial general intelligence (AGI). These models provide intelligent solutions that are closer to human thinking, enabling us to use general artificial intelligence (AI) to solve problems in various applications. However, in the field of remote sensing (RS), the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research in RS focuses primarily on visual-understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-LMs (VLMs) excel as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. VLMs can go beyond visual recognition of RS images and can model semantic relationships as well as generate natural language descriptions of the image. This makes them better suited for tasks that require both visual and textual understanding, such as image captioning and visual question answering (VQA). This article provides a comprehensive review of the research on VLMs in RS, summarizing the latest progress, highlighting current challenges, and identifying potential research opportunities. Specifically, we review the application of VLMs in mainstream RS tasks, including image captioning, text-based image generation, text-based image retrieval (TBIR), VQA, scene classification, semantic segmentation, and object detection. For each task, we analyze representative works and discuss research progress. Finally, we summarize the limitations of existing works and provide possible directions for future development. This review aims to provide a comprehensive overview of the current research progress of VLMs in RS (see Figure 1), and to inspire further research in this exciting and promising field.},
  keywords={Task analysis;Visualization;Transformers;Computational modeling;Feature extraction;Chatbots;Semantics;Visual analytics;Communication systems},
  doi={10.1109/MGRS.2024.3383473},
  ISSN={2168-6831},
  month={June},}@ARTICLE{4503259,
  author={Licklider, J. C. R.},
  journal={IRE Transactions on Human Factors in Electronics}, 
  title={Man-Computer Symbiosis}, 
  year={1960},
  volume={HFE-1},
  number={1},
  pages={4-11},
  abstract={Man-computer symbiosis is an expected development in cooperative interaction between men and electronic computers. It will involve very close coupling between the human and the electronic members of the partnership. The main aims are 1) to let computers facilitate formulative thinking as they now facilitate the solution of formulated problems, and 2) to enable men and computers to cooperate in making decisions and controlling complex situations without inflexible dependence on predetermined programs. In the anticipated symbiotic partnership, men will set the goals, formulate the hypotheses, determine the criteria, and perform the evaluations. Computing machines will do the routinizable work that must be done to prepare the way for insights and decisions in technical and scientific thinking. Preliminary analyses indicate that the symbiotic partnership will perform intellectual operations much more effectively than man alone can perform them. Prerequisites for the achievement of the effective, cooperative association include developments in computer time sharing, in memory components, in memory organization, in programming languages, and in input and output equipment.},
  keywords={Symbiosis;Insects;Time sharing computer systems;Performance evaluation;Performance analysis;Computer languages},
  doi={10.1109/THFE2.1960.4503259},
  ISSN={2168-2836},
  month={March},}@ARTICLE{10577164,
  author={Hang, Ching Nam and Wei Tan, Chee and Yu, Pei-Duo},
  journal={IEEE Access}, 
  title={MCQGen: A Large Language Model-Driven MCQ Generator for Personalized Learning}, 
  year={2024},
  volume={12},
  number={},
  pages={102261-102273},
  abstract={In the dynamic landscape of contemporary education, the evolution of teaching strategies such as blended learning and flipped classrooms has highlighted the need for efficient and effective generation of multiple-choice questions (MCQs). To address this, we introduce MCQGen, a novel generative artificial intelligence framework designed for the automated creation of MCQs. MCQGen uniquely integrates a large language model (LLM) with retrieval-augmented generation and advanced prompt engineering techniques, drawing from an extensive external knowledge base. This integration significantly enhances the ability of the LLM to produce educationally relevant questions that align with both the goals of educators and the diverse learning needs of students. The framework employs innovative prompt engineering, combining chain-of-thought and self-refine prompting techniques, to enhance the performance of the LLM. This process leads to the generation of questions that are not only contextually relevant and challenging but also reflective of common student misconceptions, contributing effectively to personalized learning experiences and enhancing student engagement and understanding. Our extensive evaluations showcase the effectiveness of MCQGen in producing high-quality MCQs for various educational needs and learning styles. The framework demonstrates its potential to significantly reduce the time and expertise required for MCQ creation, marking its practical utility in modern education. In essence, MCQGen offers an innovative and robust solution for the automated generation of MCQs, enhancing personalized learning in the digital era.},
  keywords={Education;Knowledge engineering;Testing;Knowledge based systems;Task analysis;Semantics;Problem-solving;Large language models;Information retrieval;Data augmentation;Large language models;multiple-choice questions;personalized learning;prompt engineering;retrieval-augmented generation},
  doi={10.1109/ACCESS.2024.3420709},
  ISSN={2169-3536},
  month={},}@ARTICLE{933500,
  author={Buttazzo, G.},
  journal={Computer}, 
  title={Artificial consciousness: Utopia or real possibility?}, 
  year={2001},
  volume={34},
  number={7},
  pages={24-30},
  abstract={Since the beginnings of computer technology, researchers have speculated about the possibility of building smart machines that could compete with human intelligence. Given the current pace of advances in artificial intelligence and neural computing, such an evolution seems to be a more concrete possibility. Many people now believe that artificial consciousness is possible and that, in the future, it will emerge in complex computing machines. However, a discussion of artificial consciousness gives rise to several philosophical issues: can computers think or do they just calculate? Is consciousness a human prerogative? Does consciousness depend on the material that comprises the human brain, or can computer hardware replicate consciousness? Answering these questions is difficult because it requires combining information from many disciplines including computer science, neurophysiology, philosophy, and religion. Further, we must consider the influence of science fiction, especially science fiction films, when addressing artificial consciousness. As a product of the human imagination, such works express human desires and fears about future technologies and may influence the course of progress. At a societal level, science fiction simulates future scenarios that can help prepare us for crucial transitions by predicting the consequences of significant technological advances. The paper considers robots in science fiction, the Turing test, computer chess and artificial consciousness.},
  keywords={Humans;Artificial intelligence;Intelligent structures;Machine intelligence;Concrete;Hardware;Computer science;Neurophysiology;Computational modeling;Predictive models},
  doi={10.1109/2.933500},
  ISSN={1558-0814},
  month={July},}@ARTICLE{9749967,
  author={Herrero-Álvarez, Rafael and Miranda, Gara and León, Coromoto and Segredo, Eduardo},
  journal={IEEE Transactions on Emerging Topics in Computing}, 
  title={Engaging Primary and Secondary School Students in Computer Science Through Computational Thinking Training}, 
  year={2023},
  volume={11},
  number={1},
  pages={56-69},
  abstract={Although Computer Science has grown to become one of the most highly demanded professional careers, every year, only a small percentage of students choose a degree directly related to Computer Science. Perhaps the problem lies in the lack of information that society has about Computer Science itself, and particularly about the work computer scientists do. No one doubts the role of Mathematics or Languages as core subjects in every primary and secondary education syllabus; however, Computer Science plays a negligible role in most current syllabuses. Only in a few countries have governments paid special attention to content related to Computer Science and to learning to analyze and solve problems the way computer scientists do (Computational Thinking). In this article, we present Piens@ Computacion@ULLmente, a project that provides a methodology to promote Computer Science through Computational Thinking activities among primary and secondary education students. The results obtained from an exhaustive statistical analysis of the data we collected demonstrate that the perception of Computer Science that pre-university students have can be improved through specific training. Moreover, we can also confirm that the performance of pre-university students involving Computational Thinking skills is independent of gender, particularly at the primary education level.},
  keywords={Computer science;Training;Problem-solving;Programming profession;Engineering profession;Licenses;Europe;Computer science;computational thinking;primary education;secondary education;syllabus},
  doi={10.1109/TETC.2022.3163650},
  ISSN={2168-6750},
  month={Jan},}@INPROCEEDINGS{7738674,
  author={Agrawal, Ankur and Choi, Jungwook and Gopalakrishnan, Kailash and Gupta, Suyog and Nair, Ravi and Oh, Jinwook and Prener, Daniel A. and Shukla, Sunil and Srinivasan, Vijayalakshmi and Sura, Zehra},
  booktitle={2016 IEEE International Conference on Rebooting Computing (ICRC)}, 
  title={Approximate computing: Challenges and opportunities}, 
  year={2016},
  volume={},
  number={},
  pages={1-8},
  abstract={Approximate computing is gaining traction as a computing paradigm for data analytics and cognitive applications that aim to extract deep insight from vast quantities of data. In this paper, we demonstrate that multiple approximation techniques can be applied to applications in these domains and can be further combined together to compound their benefits. In assessing the potential of approximation in these applications, we took the liberty of changing multiple layers of the system stack: architecture, programming model, and algorithms. Across a set of applications spanning the domains of DSP, robotics, and machine learning, we show that hot loops in the applications can be perforated by an average of 50% with proportional reduction in execution time, while still producing acceptable quality of results. In addition, the width of the data used in the computation can be reduced to 10-16 bits from the currently common 32/64 bits with potential for significant performance and energy benefits. For parallel applications we reduced execution time by 50% using relaxed synchronization mechanisms. Finally, our results also demonstrate that benefits compounded when these techniques are applied concurrently. Our results across different applications demonstrate that approximate computing is a widely applicable paradigm with potential for compounded benefits from applying multiple techniques across the system stack. In order to exploit these benefits it is essential to re-think multiple layers of the system stack to embrace approximations ground-up and to design tightly integrated approximate accelerators. Doing so will enable moving the applications into a world in which the architecture, programming model, and even the algorithms used to implement the application are all fundamentally designed for approximate computing.},
  keywords={Approximate computing;Synchronization;Hardware;Computer architecture;Synthetic aperture radar;Training;Approximation algorithms;Approximate computing;perforation;reduced precision;relaxed synchronization},
  doi={10.1109/ICRC.2016.7738674},
  ISSN={},
  month={Oct},}@ARTICLE{9741843,
  author={Rijo-García, Sara and Segredo, Eduardo and León, Coromoto},
  journal={IEEE Transactions on Education}, 
  title={Computational Thinking and User Interfaces: A Systematic Review}, 
  year={2022},
  volume={65},
  number={4},
  pages={647-656},
  abstract={Contribution: This document presents a systematic bibliographic review that demonstrates the need to conduct research on how the user experience impacts the development of computational thinking. Background: In the field of computer science, computational thinking is defined as a method that enhances problem-solving skills, system design, and human behavior understanding. Over the last few decades, several tools have been proposed for the development of computational thinking skills; however, there is no area of study that evaluates the implications or the impact that these types of platforms have on users belonging to any knowledge area. Research Question: Do user interfaces influence the development of computational thinking skills? Methodology: To address this issue, a systematic review of the literature was conducted using the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology for analyzing and evaluating scientific publications. Findings: The results show that despite the dearth of literature on the subject, the specific design of a user interface has a significant impact on the development of computational thinking. Bearing the above in mind, it is necessary to conduct research that delves more deeply into the effects caused by the technologies that are used to develop computational thinking, this being a line of research that is worthy of consideration.},
  keywords={User interfaces;Systematics;User experience;Usability;Programming profession;Computational modeling;Libraries;Computational thinking;human–computer interaction;preferred reporting items for systematic reviews and meta-analyses (PRISMA);survey;systematic review;usability;user experience;user interface;visual programming},
  doi={10.1109/TE.2022.3159765},
  ISSN={1557-9638},
  month={Nov},}

